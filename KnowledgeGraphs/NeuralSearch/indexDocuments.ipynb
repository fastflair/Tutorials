{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8bbdb8f",
   "metadata": {},
   "source": [
    "# Tutorial for building a graph from a document for Q/A and Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c346e7c0",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "585b74cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-25 23:58:08.152795: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-25 23:58:09.136710: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-25 23:58:09.136849: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-25 23:58:09.136861: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "[nltk_data] Downloading package punkt to /home/titan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for parsing documents with structure\n",
    "# sudo apt install pandoc\n",
    "# pip install pandoc\n",
    "import pandoc \n",
    "# for embeddings DB with meta data\n",
    "import pinecone\n",
    "# for large language model use (davinci-003)\n",
    "# this is a paid API\n",
    "# pip install openai\n",
    "import openai\n",
    "# hugging face transformers for efficient access to trained embeddings models\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "\n",
    "# for parsing web-pages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "import re\n",
    "import tqdm\n",
    "\n",
    "import PyPDF2\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989e89a3",
   "metadata": {},
   "source": [
    "## Define Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e555039c",
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTOR_INDEX_NAME = 'audio'\n",
    "# we encode and insert in batches of 64\n",
    "batch_size = 64\n",
    "\n",
    "OPENAI_COMPLETION_ENGINE = 'text-davinci-003'\n",
    "OPENAI_CONTEXT_LENGTH_LIMIT = 4000\n",
    "EMBED_MODEL = 'multi-qa-mpnet-base-dot-v1'\n",
    "EMBED_CROSS_MODEL = 'cross-encoder/ms-marco-MiniLM-L-6-v2'\n",
    "READER_MODEL = 'deepset/electra-base-squad2'\n",
    "\n",
    "openai_api_key = \"sk-KrOiHnlQW3dDLnU7aE8PooCP5MGhvYzP9XX2uSEG\"\n",
    "pinecone_api_key = 'c65fa925-08e1-4af0-b08b-1104c6ffba25' # https://app.pinecone.io/projects\n",
    "google_api_key = 'AIzaSyAIIY6OsTws8dTfoyxNmJLmnfmH2f859Fw' # https://console.cloud.google.com/apis/dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7add27fa",
   "metadata": {},
   "source": [
    "## Open AI functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10f37c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def openai_complete(context, prompt, max_answer_len=256) -> str:\n",
    "        text_context = (context[:OPENAI_CONTEXT_LENGTH_LIMIT] + '...') \\\n",
    "            if len(context) > OPENAI_CONTEXT_LENGTH_LIMIT else context\n",
    "\n",
    "        openai_prompt = f\"{text_context}\\n\\n{prompt}\"\n",
    "\n",
    "        response = openai.Completion.create(\n",
    "            engine=OPENAI_COMPLETION_ENGINE,\n",
    "            prompt=openai_prompt,\n",
    "            temperature=0.7,\n",
    "            max_tokens=max_answer_len,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0\n",
    "        )\n",
    "        return response.choices[0].text\n",
    "\n",
    "def openai_summary(text, max_answer_len=255) -> str:\n",
    "        if len(text) < max_answer_len:\n",
    "            return text\n",
    "        return openai_complete(text, \"TLDR:\", max_answer_len)[0]\n",
    "    \n",
    "def openai_question(question, context, max_answer_len=512) -> str:\n",
    "        context = context[0:9000]\n",
    "        openai_prompt = f\"Answer the question: {question}\\n\\nContext: {context}\\n\\nAnswer:\"\n",
    "\n",
    "        response = openai.Completion.create(\n",
    "            engine=OPENAI_COMPLETION_ENGINE,\n",
    "            prompt=openai_prompt,\n",
    "            temperature=0.7,\n",
    "            max_tokens=max_answer_len,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0\n",
    "        )\n",
    "        return response.choices[0].text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a81c51",
   "metadata": {},
   "source": [
    "## Embedding definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37473784",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = SentenceTransformer(EMBED_MODEL)\n",
    "EMBED_DIMENSIONS = retriever.get_sentence_embedding_dimension()\n",
    "\n",
    "cross_encoder = CrossEncoder(EMBED_CROSS_MODEL)\n",
    "reader = pipeline(tokenizer=READER_MODEL, model=READER_MODEL, task='question-answering')\n",
    "\n",
    "openai.api_key = openai_api_key\n",
    "\n",
    "pinecone.init(\n",
    "    api_key=pinecone_api_key\n",
    ")\n",
    "\n",
    "if VECTOR_INDEX_NAME not in pinecone.list_indexes():\n",
    "    pinecone.create_index(\n",
    "        name=VECTOR_INDEX_NAME,\n",
    "        dimension=EMBED_DIMENSIONS,\n",
    "        metric='dotproduct',\n",
    "    )\n",
    "vector_index = pinecone.Index(VECTOR_INDEX_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f402343",
   "metadata": {},
   "source": [
    "## Webpage parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4cfe00e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(raw_html):\n",
    "    # Remove extra white space\n",
    "    clean_text = re.sub('\\s+', ' ', raw_html).strip()\n",
    "    \n",
    "    # Remove special characters and symbols except for punctuation\n",
    "    clean_text = re.sub('[^A-Za-z0-9\\s.,?!]+', '', clean_text)\n",
    "    \n",
    "    return clean_text\n",
    "\n",
    "def crawl_domain(url):\n",
    "    # Parse the base URL\n",
    "    base_url = urlparse(url).scheme + '://' + urlparse(url).hostname\n",
    "    \n",
    "    # Initialize a set to store visited URLs\n",
    "    visited_urls = set()\n",
    "    \n",
    "    # Initialize a list to store text information\n",
    "    text_info = []\n",
    "    \n",
    "    # Define a recursive function to crawl links within the domain\n",
    "    def crawl(url):\n",
    "        # Check if the URL has already been visited\n",
    "        if url in visited_urls:\n",
    "            return\n",
    "        \n",
    "        # Add the URL to the visited set\n",
    "        visited_urls.add(url)\n",
    "        \n",
    "        # Define headers to make the request look more like a legitimate request from a web browser\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "        # Make a request to the URL with headers\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        # Parse the HTML content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Extract the text information from the page\n",
    "        text = soup.get_text()\n",
    "        \n",
    "        # Clean the raw html text\n",
    "        text = clean_html(text)\n",
    "        \n",
    "        # Append the text information to the list\n",
    "        text_info.append((url, text))\n",
    "        \n",
    "        # Find all links on the page\n",
    "        links = soup.find_all('a')\n",
    "        \n",
    "        # Recursively crawl links within the domain\n",
    "        for link in links:\n",
    "            href = link.get('href')\n",
    "            if href and base_url in href:\n",
    "                crawl(href)\n",
    "    \n",
    "    # Start crawling from the base URL\n",
    "    crawl(base_url)\n",
    "    \n",
    "    # Return the text information\n",
    "    return text_info\n",
    "\n",
    "def tokenize_sentences(text):\n",
    "    \"\"\"\n",
    "    Tokenize sentences in the given text using the nltk library.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of sentence strings.\n",
    "    \"\"\"\n",
    "    # Use the nltk library to tokenize sentences\n",
    "    return nltk.sent_tokenize(text)\n",
    "\n",
    "def crawl_website_pdfs(url):\n",
    "    # Get HTML content of website\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    # Find all links in the HTML content\n",
    "    links = soup.find_all('a')\n",
    "    \n",
    "    # Filter out only the links that end with \".pdf\"\n",
    "    pdf_links = [link.get('href') for link in links if link.get('href') and link.get('href').endswith('.pdf')]\n",
    "    \n",
    "    # Download each PDF file to local system\n",
    "    for pdf_link in pdf_links:\n",
    "        file_name = pdf_link.split(\"/\")[-1]\n",
    "        try:\n",
    "            urllib.request.urlretrieve(pdf_link, file_name)\n",
    "            print(f\"Downloaded {file_name}\")\n",
    "        except:\n",
    "            print(f\"Failed to download {file_name}\")\n",
    "            \n",
    "def extract_text(pdf_file):\n",
    "    # Open the PDF file in read-binary mode\n",
    "    with open(pdf_file, 'rb') as f:\n",
    "        # Create a PDF reader object\n",
    "        pdf_reader = PyPDF2.PdfFileReader(f)\n",
    "        \n",
    "        # Initialize an empty string to store the extracted text\n",
    "        text = \"\"\n",
    "        \n",
    "        # Iterate through each page of the PDF file\n",
    "        for page_num in range(pdf_reader.numPages):\n",
    "            # Extract the text from each page\n",
    "            page = pdf_reader.getPage(page_num)\n",
    "            page_text = page.extractText()\n",
    "            \n",
    "            # Concatenate the extracted text to the final string\n",
    "            text += page_text\n",
    "        \n",
    "        # Close the PDF file\n",
    "        f.close()\n",
    "        \n",
    "        # Return the extracted text\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c249e5a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "skipping PDF\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "skipping PDF\n",
      "skipping PDF\n",
      "0 1 1\n",
      "skipping PDF\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "skipping PDF\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "skipping PDF\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "skipping PDF\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "skipping PDF\n",
      "skipping PDF\n",
      "skipping PDF\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "skipping PDF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "skipping PDF\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "skipping PDF\n",
      "skipping PDF\n",
      "0 1 1\n",
      "skipping PDF\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "skipping PDF\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "skipping PDF\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "skipping PDF\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "skipping PDF\n",
      "skipping PDF\n",
      "skipping PDF\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "0 1 1\n",
      "skipping PDF\n"
     ]
    }
   ],
   "source": [
    "domains = [('https://www.lawyersweekly.com.au/sme-law/36853-why-your-firm-needs-an-ai-use-policy', 'Lawyers Weekly'),\n",
    "                ('https://stirlingandrose.com/insights/', 'Sterling and Rose'),\n",
    "                ('https://stirlingandrose.com/', 'Sterling and Rose')]\n",
    "\n",
    "for domain in domains:\n",
    "    channel_name = domain[1]\n",
    "    text_info = crawl_domain(domain[0])\n",
    "    for scrubed_data in text_info:\n",
    "        # set window (length of text chunk) and stride\n",
    "        window = 1\n",
    "        stride = 1  # smaller stride creates overlap\n",
    "        \n",
    "        data = []\n",
    "        results = []\n",
    "        \n",
    "        new_data = []\n",
    "        \n",
    "        window = 6  # number of sentences to combine\n",
    "        stride = 3  # number of sentences to 'stride' over, used to create overlap\n",
    "        \n",
    "        text = tokenize_sentences(scrubed_data[1])\n",
    "        if text[0].startswith(\"PDF\"):\n",
    "            print('skipping PDF')\n",
    "        else:\n",
    "            new_data.append({\n",
    "                'text': text,\n",
    "                'id': scrubed_data[0],\n",
    "                'url': scrubed_data[0],\n",
    "                \"name\":channel_name,\n",
    "                \"title\":\"channel_name\",\n",
    "            })\n",
    "            \n",
    "        # loop through in batches of 64\n",
    "        index = pinecone.Index(VECTOR_INDEX_NAME)\n",
    "        \n",
    "        for j in range(0, len(new_data), batch_size):\n",
    "            if len(new_data) > 0:\n",
    "                # find end position of batch (for when we hit end of data)\n",
    "                j_end = min(len(new_data), j+batch_size)\n",
    "                print(j, j_end, len(new_data))\n",
    "                try:\n",
    "                    # extract the metadata like text, start/end positions, etc\n",
    "                    batch_meta = [{\n",
    "                        \"text\": new_data[x][\"text\"],\n",
    "                        \"url\": new_data[x][\"url\"],\n",
    "                        \"name\": new_data[x][\"name\"],\n",
    "                        \"title\": new_data[x][\"title\"]\n",
    "                    } for x in range(j, j_end)]\n",
    "                    # extract only text to be encoded by embedding model\n",
    "                    batch_text = [row['text'] for row in new_data[j:j_end]]\n",
    "                    # create the embedding vectors\n",
    "                    batch_embeds = retriever.encode(batch_text).tolist()\n",
    "                    # extract IDs to be attached to each embedding and metadata\n",
    "                    batch_ids = [row['id'] for row in new_data[j:j_end]]\n",
    "                    # 'upsert' (eg insert) IDs, embeddings, and metadata to index\n",
    "                    to_upsert = list(zip(batch_ids, batch_embeds, batch_meta))\n",
    "                    index.upsert(to_upsert)\n",
    "                except:\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8e81cc",
   "metadata": {},
   "source": [
    "## Query pinecone index for answer to question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7eee742f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'matches': [{'id': '8LK4wSGC5bQ-t46.44',\n",
      "              'metadata': {'end': 76.72,\n",
      "                           'name': '8LK4wSGC5bQ',\n",
      "                           'start': 46.44,\n",
      "                           'text': 'done by other means than expensive '\n",
      "                                   'lawyers. Alright, today I would like to '\n",
      "                                   'welcome Shelly Jane Price who is a partner '\n",
      "                                   'with the Sterling and Rose Law Firm here '\n",
      "                                   'in Western Australia where she leads the '\n",
      "                                   'artificial intelligence practice. SJ has '\n",
      "                                   'also started lecturing in technology and '\n",
      "                                   'the law at Murdoch University also here in '\n",
      "                                   'Western Australia. ',\n",
      "                           'title': 'Alex Jenkins and SJ Price talk all things '\n",
      "                                    'Artificial Intelligence',\n",
      "                           'url': 'https://youtu.be/8LK4wSGC5bQ?t=46'},\n",
      "              'score': 25.7483673,\n",
      "              'values': []},\n",
      "             {'id': '8LK4wSGC5bQ-t65.24',\n",
      "              'metadata': {'end': 86.03999999999999,\n",
      "                           'name': '8LK4wSGC5bQ',\n",
      "                           'start': 65.24,\n",
      "                           'text': 'practice. SJ has also started lecturing in '\n",
      "                                   'technology and the law at Murdoch '\n",
      "                                   'University also here in Western Australia. '\n",
      "                                   \"So welcome SJ. Thanks Alex. I'm really \"\n",
      "                                   'delighted to be here and geek out about my '\n",
      "                                   'favourite topic. ',\n",
      "                           'title': 'Alex Jenkins and SJ Price talk all things '\n",
      "                                    'Artificial Intelligence',\n",
      "                           'url': 'https://youtu.be/8LK4wSGC5bQ?t=65'},\n",
      "              'score': 20.9402676,\n",
      "              'values': []},\n",
      "             {'id': 'FTX the legend of Sam Bankman-Fried  FT Film-t1084.0',\n",
      "              'metadata': {'end': 1113.0,\n",
      "                           'name': 'FTX the legend of Sam Bankman-Fried  FT '\n",
      "                                   'Film',\n",
      "                           'start': 1084.0,\n",
      "                           'text': \"He's pleaded not guilty. They raise a $250 \"\n",
      "                                   'million bail for him to effectively go and '\n",
      "                                   \"sit in his parents' house in California \"\n",
      "                                   'for as long as it takes until this goes to '\n",
      "                                   'trial. He is charged with eight counts by '\n",
      "                                   'the Southern District of New York, which '\n",
      "                                   'includes wire fraud, conspiracy to commit '\n",
      "                                   'wire fraud, campaign finance violations. '\n",
      "                                   'The SEC and the CFTC, who are two major '\n",
      "                                   'regulators in the states, ',\n",
      "                           'title': 'FTX the legend of Sam Bankman-Fried  FT '\n",
      "                                    'Film',\n",
      "                           'url': 'https://youtu.be/yGGzimG8VMQ?t=1084'},\n",
      "              'score': 19.0318642,\n",
      "              'values': []},\n",
      "             {'id': 'FTX the legend of Sam Bankman-Fried  FT Film-t1073.0',\n",
      "              'metadata': {'end': 1104.0,\n",
      "                           'name': 'FTX the legend of Sam Bankman-Fried  FT '\n",
      "                                   'Film',\n",
      "                           'start': 1073.0,\n",
      "                           'text': 'that idea became a little bit more '\n",
      "                                   'appealing to him. He was flown to the US. '\n",
      "                                   \"He appeared before a judge. He's pleaded \"\n",
      "                                   'not guilty. They raise a $250 million bail '\n",
      "                                   'for him to effectively go and sit in his '\n",
      "                                   \"parents' house in California for as long \"\n",
      "                                   'as it takes until this goes to trial. ',\n",
      "                           'title': 'FTX the legend of Sam Bankman-Fried  FT '\n",
      "                                    'Film',\n",
      "                           'url': 'https://youtu.be/yGGzimG8VMQ?t=1073'},\n",
      "              'score': 18.7093887,\n",
      "              'values': []},\n",
      "             {'id': '8LK4wSGC5bQ-t282.92',\n",
      "              'metadata': {'end': 325.32,\n",
      "                           'name': '8LK4wSGC5bQ',\n",
      "                           'start': 282.92,\n",
      "                           'text': 'justice for our society. Okay, so you see '\n",
      "                                   'a future where when you talk about the '\n",
      "                                   'splitting up of certain parts of legal '\n",
      "                                   'services into areas where you might be '\n",
      "                                   'able to take advice from a chatbot almost '\n",
      "                                   'as being powered by something like '\n",
      "                                   'chat.jbt, obviously specialised on the '\n",
      "                                   'law, you think that there will be much '\n",
      "                                   \"more streamlined services where we won't \"\n",
      "                                   'need to use expensive human lawyers for '\n",
      "                                   'everything, is that sort of where you see '\n",
      "                                   'it? ',\n",
      "                           'title': 'Alex Jenkins and SJ Price talk all things '\n",
      "                                    'Artificial Intelligence',\n",
      "                           'url': 'https://youtu.be/8LK4wSGC5bQ?t=282'},\n",
      "              'score': 18.6703033,\n",
      "              'values': []}],\n",
      " 'namespace': ''}\n"
     ]
    }
   ],
   "source": [
    "# Define the query or question to ask\n",
    "query = \"Who the hell is the lawyer SJ Price?\"\n",
    "# Create and embedding representing the question\n",
    "xq = retriever.encode(query).tolist()\n",
    "# Search the index for the top (k) answers \n",
    "results = vector_index.query(xq, top_k=5, include_metadata=True)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a7b1b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Shelley Jane Price is a partner with the Sterling and Rose Law Firm in Western Australia, where she leads the artificial intelligence practice. She has also started lecturing in technology and the law at Murdoch University in Western Australia. She is a lawyer and is involved in cases such as wire fraud, conspiracy to commit wire fraud, and campaign finance violations.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = \"\"\n",
    "for result in results['matches']:\n",
    "    for line in result['metadata']['text']:\n",
    "        context += line+\" \"\n",
    "openai_answser = openai_question(query, context)\n",
    "openai_answser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4055b659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sterling and Rose is an incorporated legal practice in Western Australia that specializes in emerging technology, including crypto and digital assets, web 3.0, metaverse, smart legal contracts, and data rights. They serve investors, platform providers, entrepreneurs, financial institutions, and governments and are uniquely positioned to advise on FinTech, RegTech and LegalTech holistically.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the query or question to ask\n",
    "query = \"What is Sterling and Rose?\"\n",
    "# Create and embedding representing the question\n",
    "xq = retriever.encode(query).tolist()\n",
    "# Search the index for the top (k) answers \n",
    "results = vector_index.query(xq, top_k=5, include_metadata=True)\n",
    "context = \"\"\n",
    "for result in results['matches']:\n",
    "    for line in result['metadata']['text']:\n",
    "        context += line+\" \"\n",
    "openai_answser = openai_question(query, context)\n",
    "openai_answser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2f8899",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
