{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "07e3e05f-6f92-44d8-8577-6694340aa335",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from itertools import combinations, product\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LassoCV, ElasticNetCV\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.signal import savgol_filter\n",
    "import scipy.special as sp_special\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b57f7299-3f2d-4145-ae32-c8eb5e8d34d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_polynomial_feature_descriptions(candidate_function_descriptions, degree):\n",
    "    poly = PolynomialFeatures(degree, include_bias=False)\n",
    "    feature_combinations = poly.fit_transform(np.zeros((1, len(candidate_function_descriptions))))\n",
    "    feature_names = poly.get_feature_names_out(candidate_function_descriptions)\n",
    "    return feature_names\n",
    "    \n",
    "def robust_differential_equation_discovery(\n",
    "    data,\n",
    "    candidate_functions,\n",
    "    candidate_function_descriptions,\n",
    "    derivative_order=3,\n",
    "    alpha_range=(1e-7, 1e-4),\n",
    "    l1_ratio_range=(0.1, 0.9),\n",
    "    cv_folds=10,\n",
    "    n_bootstrap=100\n",
    "):\n",
    "    # Step 1: Denoising\n",
    "    denoised_data = denoise_data(data)\n",
    "   \n",
    "    # Step 2: Interpolation\n",
    "    interpolated_data = interpolate_data(denoised_data)\n",
    "   \n",
    "    # Step 3: Derivative Estimation\n",
    "    derivatives = estimate_derivatives(interpolated_data, order=derivative_order)\n",
    "   \n",
    "    # Step 4: Feature Engineering\n",
    "    X = create_feature_matrix(interpolated_data, derivatives, candidate_functions)\n",
    "   \n",
    "    # Polynomial Features Expansion\n",
    "    degree = 3\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "   \n",
    "    # Generate Polynomial Feature Descriptions\n",
    "    poly_feature_descriptions = create_polynomial_feature_descriptions(candidate_function_descriptions, degree)\n",
    "   \n",
    "    # Feature Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_poly)\n",
    "   \n",
    "    # Step 5: Sparse Identification using ElasticNet\n",
    "    # Ensure no excessive regularization\n",
    "    elastic_net = ElasticNetCV(\n",
    "        alphas=np.logspace(-7, -4, 100),  # Smaller alpha range\n",
    "        l1_ratio=np.linspace(0.1, 0.9, 10),\n",
    "        cv=cv_folds,\n",
    "        max_iter=10000,\n",
    "        random_state=0\n",
    "    )\n",
    "    \n",
    "    # Fit the model with the adjusted regularization\n",
    "    elastic_net.fit(X_scaled, interpolated_data['y'])\n",
    "   \n",
    "    # Step 6: Model Validation\n",
    "    scores = cross_val_score(\n",
    "        elastic_net,\n",
    "        X_scaled,\n",
    "        interpolated_data['y'],\n",
    "        cv=cv_folds,\n",
    "        scoring='neg_mean_squared_error'\n",
    "    )\n",
    "    print(\"Cross-validation MSE:\", -np.mean(scores))\n",
    "   \n",
    "    # Step 7: Uncertainty Quantification using Bootstrapping\n",
    "    bootstrapped_models = bootstrap_model_fitting(\n",
    "        X_scaled,\n",
    "        interpolated_data['y'],\n",
    "        elastic_net,\n",
    "        n_bootstrap=n_bootstrap\n",
    "    )\n",
    "   \n",
    "    # Step 8: Final Model Selection\n",
    "    final_model = select_best_model(bootstrapped_models, X_scaled, interpolated_data['y'])\n",
    "   \n",
    "    return final_model, elastic_net.coef_, poly_feature_descriptions\n",
    "\n",
    "def denoise_data(data):\n",
    "    # Implement a denoising technique like Savitzky-Golay filtering\n",
    "    denoised_y = savgol_filter(data['y'], window_length=7, polyorder=3)\n",
    "    return {'x': data['x'], 'y': denoised_y}\n",
    "\n",
    "def interpolate_data(denoised_data):\n",
    "    # Implement an interpolation technique like Spline Interpolation\n",
    "    spline = UnivariateSpline(denoised_data['x'], denoised_data['y'], s=0)\n",
    "    interpolated_y = spline(denoised_data['x'])\n",
    "    return {'x': denoised_data['x'], 'y': interpolated_y}\n",
    "\n",
    "def estimate_derivatives(data, order=3):\n",
    "    # Use finite differences to estimate derivatives up to the specified order\n",
    "    derivatives = {'0th': data['y']}\n",
    "    for i in range(1, order + 1):\n",
    "        derivatives[f'{i}th'] = np.gradient(derivatives[f'{i-1}th'], data['x'])\n",
    "    return derivatives\n",
    "\n",
    "def create_feature_matrix(data, derivatives, candidate_functions):\n",
    "    # Construct a matrix with candidate functions applied to the data and its derivatives\n",
    "    X = []\n",
    "    feature_names = []\n",
    "    for order_label, derivative in derivatives.items():\n",
    "        for func_index, func in enumerate(candidate_functions):\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                try:\n",
    "                    feature = func(derivative)\n",
    "                    # Check for invalid values\n",
    "                    if np.any(np.isnan(feature)) or np.any(np.isinf(feature)) or np.iscomplexobj(feature):\n",
    "                        continue\n",
    "                    # Clip extreme values to prevent overflow\n",
    "                    feature = np.clip(feature, -1e6, 1e6)\n",
    "                    X.append(feature.real)  # Ensure real values\n",
    "                    feature_names.append(f'{order_label}_func_{func_index}')\n",
    "                except Exception:\n",
    "                    continue\n",
    "    if not X:\n",
    "        raise ValueError(\"No valid features generated. Check candidate functions and data.\")\n",
    "    X = np.column_stack(X)\n",
    "    return X\n",
    "\n",
    "def bootstrap_model_fitting(X, y, base_model, n_bootstrap=100):\n",
    "    # Apply bootstrapping to fit the model multiple times and quantify uncertainty\n",
    "    bootstrapped_models = []\n",
    "    n_samples = X.shape[0]\n",
    "    for i in range(n_bootstrap):\n",
    "        sample_indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "        X_sample, y_sample = X[sample_indices], y[sample_indices]\n",
    "        model = LassoCV(\n",
    "            alphas=base_model.alphas_,\n",
    "            cv=base_model.cv,            # Corrected attribute for cross-validation folds\n",
    "            max_iter=base_model.max_iter,\n",
    "            random_state=i\n",
    "        )\n",
    "        model.fit(X_sample, y_sample)\n",
    "        bootstrapped_models.append(model)\n",
    "    return bootstrapped_models# Output the discovered model coefficients\n",
    "\n",
    "def select_best_model(models, X, y):\n",
    "    # Select the model with the best performance according to mean squared error\n",
    "    mse_scores = [mean_squared_error(m.predict(X), y) for m in models]\n",
    "    best_model_index = np.argmin(mse_scores)\n",
    "    best_model = models[best_model_index]\n",
    "    print(f\"Best model MSE: {mse_scores[best_model_index]}\")\n",
    "    return best_model\n",
    "\n",
    "# Updated candidate functions with safer implementations\n",
    "candidate_functions = [\n",
    "    # Basic functions\n",
    "    lambda x: x,                                # Linear term\n",
    "    lambda x: x**2,                             # Quadratic term\n",
    " #   lambda x: x**3,                             # Cubic term\n",
    " #   lambda x: x**4,                             # Fourth power term\n",
    " #   lambda x: x**5,                             # Fifth power term\n",
    "\n",
    "    # Trigonometric functions\n",
    "    lambda x: np.sin(x),                        # Sine function\n",
    " #   lambda x: np.cos(x),                        # Cosine function\n",
    " #   lambda x: np.tan(x),                        # Tangent function with domain restriction\n",
    " #   lambda x: np.sin(2 * x),                    # Harmonic sine term\n",
    " #   lambda x: np.cos(2 * x),                    # Harmonic cosine term\n",
    "\n",
    "    # Exponential and logarithmic functions\n",
    " #   lambda x: np.exp(np.clip(x, -100, 100)),    # Exponential function with clipping\n",
    " #   lambda x: np.exp(-np.clip(x, -100, 100)),   # Decaying exponential function with clipping\n",
    " #   lambda x: np.log(np.abs(x) + 1e-6),         # Logarithmic function\n",
    " #   lambda x: np.exp(np.clip(x**2, -100, 100)), # Exponential of a quadratic term with clipping\n",
    " #   lambda x: np.log(x**2 + 1e-6),              # Logarithm of a quadratic term\n",
    "\n",
    "    # Hyperbolic functions\n",
    " #   lambda x: np.tanh(x),                       # Hyperbolic tangent function\n",
    " #   lambda x: np.sinh(x),                       # Hyperbolic sine function\n",
    " #   lambda x: np.cosh(x),                       # Hyperbolic cosine function with clipping\n",
    "     \n",
    "    # Special functions with safe evaluations\n",
    " #   lambda x: sp_special.gamma(np.clip(x, 1e-6, 100)),          # Gamma function\n",
    " #   lambda x: sp_special.psi(np.clip(x, 1e-6, 100)),            # Digamma function\n",
    " #   lambda x: sp_special.erf(x),                                 # Error function\n",
    " #   lambda x: sp_special.erfc(x),                                # Complementary error function\n",
    " #   lambda x: sp_special.jv(0, x),                               # Bessel function of the first kind (order 0)\n",
    " #   lambda x: sp_special.yv(0, np.clip(x, 1e-6, 100)),          # Bessel function of the second kind (order 0)\n",
    " #   lambda x: sp_special.beta(np.clip(x, 1e-6, 100), np.clip(x+1, 1e-6, 100)), # Beta function\n",
    " #   lambda x: sp_special.lambertw(x).real,                       # Lambert W function (principal branch)\n",
    " #   lambda x: sp_special.zeta(np.clip(x, 1.1, 100)),             # Riemann zeta function\n",
    "\n",
    "    # Inverse and root functions\n",
    " #   lambda x: 1 / (np.abs(x) + 1e-6),                            # Inverse function\n",
    " #   lambda x: np.sqrt(np.abs(x) + 1e-6),                         # Square root function\n",
    "]\n",
    "\n",
    "# List of descriptions for the candidate functions\n",
    "candidate_function_descriptions = [\n",
    "    \"x\",                                        # Linear term\n",
    "    \"x^2\",                                      # Quadratic term\n",
    "#    \"x^3\",                                      # Cubic term\n",
    "#    \"x^4\",                                      # Fourth power term\n",
    "#    \"x^5\",                                      # Fifth power term\n",
    "\n",
    "    # Trigonometric functions\n",
    "    \"sin(x)\",                                   # Sine function\n",
    " #   \"cos(x)\",                                   # Cosine function\n",
    " #   \"tan(x)\",                                   # Tangent function with domain restriction\n",
    " #   \"sin(2x)\",                                  # Harmonic sine term\n",
    " #   \"cos(2x)\",                                  # Harmonic cosine term\n",
    "\n",
    "    # Exponential and logarithmic functions\n",
    " #   \"exp(clip(x, -100, 100))\",                  # Exponential function with clipping\n",
    " #   \"exp(-clip(x, -100, 100))\",                 # Decaying exponential function with clipping\n",
    " #   \"log(abs(x) + 1e-6)\",                       # Logarithmic function\n",
    " #   \"exp(clip(x^2, -100, 100))\",                # Exponential of a quadratic term with clipping\n",
    " #   \"log(x^2 + 1e-6)\",                          # Logarithm of a quadratic term\n",
    "\n",
    "    # Hyperbolic functions\n",
    " #   \"tanh(x)\",                                  # Hyperbolic tangent function\n",
    " #   \"sinh(x)\",                                  # Hyperbolic sine function\n",
    " #   \"cosh(x)\",                                  # Hyperbolic cosine function with clipping\n",
    "     \n",
    "    # Special functions with safe evaluations\n",
    " #   \"gamma(clip(x, 1e-6, 100))\",                # Gamma function\n",
    " #   \"psi(clip(x, 1e-6, 100))\",                  # Digamma function\n",
    " #   \"erf(x)\",                                   # Error function\n",
    " #   \"erfc(x)\",                                  # Complementary error function\n",
    " #   \"jv(0, x)\",                                 # Bessel function of the first kind (order 0)\n",
    " #   \"yv(0, clip(x, 1e-6, 100))\",                # Bessel function of the second kind (order 0)\n",
    " #   \"beta(clip(x, 1e-6, 100), clip(x+1, 1e-6, 100))\", # Beta function with arbitrary parameters\n",
    " #   \"lambertw(x).real\",                         # Lambert W function (principal branch)\n",
    " #   \"zeta(clip(x, 1.1, 100))\",                  # Riemann zeta function\n",
    "\n",
    "    # Inverse and root functions\n",
    " #   \"1 / (abs(x) + 1e-6)\",                      # Inverse function\n",
    " #   \"sqrt(abs(x) + 1e-6)\",                      # Square root function\n",
    "]\n",
    "\n",
    "def create_feature_matrix_simple(data, candidate_functions):\n",
    "    # Construct a matrix with candidate functions applied to the original data\n",
    "    X = []\n",
    "    feature_names = []\n",
    "    for func_index, func in enumerate(candidate_functions):\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            try:\n",
    "                feature = func(data['x'])\n",
    "                # Check for invalid values\n",
    "                if np.any(np.isnan(feature)) or np.any(np.isinf(feature)) or np.iscomplexobj(feature):\n",
    "                    continue\n",
    "                # Clip extreme values to prevent overflow\n",
    "                feature = np.clip(feature, -1e6, 1e6)\n",
    "                X.append(feature.real)  # Ensure real values\n",
    "                feature_names.append(candidate_function_descriptions[func_index])\n",
    "            except Exception:\n",
    "                continue\n",
    "    if not X:\n",
    "        raise ValueError(\"No valid features generated. Check candidate functions and data.\")\n",
    "    X = np.column_stack(X)\n",
    "    return X, feature_names\n",
    "\n",
    "def robust_differential_equation_discovery_simple(\n",
    "    data,\n",
    "    candidate_functions,\n",
    "    candidate_function_descriptions,\n",
    "    alpha_range=(1e-8, 1e-5),\n",
    "    l1_ratio_range=(0.1, 0.9),\n",
    "    cv_folds=10,\n",
    "    n_bootstrap=100\n",
    "):\n",
    "    # Skip Denoising and Interpolation\n",
    "    interpolated_data = data  # Use raw data\n",
    "\n",
    "    # Step 1: Feature Engineering\n",
    "    X, feature_descriptions = create_feature_matrix_simple(interpolated_data, candidate_functions)\n",
    "    \n",
    "    # Separate scaling for polynomial and trigonometric terms\n",
    "    X_poly = X[:, :2]  # Assuming first two are x and x^2\n",
    "    X_trig = X[:, 2:]  # Assuming the last one is sin(x)\n",
    "    \n",
    "    scaler_poly = StandardScaler()\n",
    "    scaler_trig = StandardScaler()\n",
    "    \n",
    "    X_poly_scaled = scaler_poly.fit_transform(X_poly)\n",
    "    X_trig_scaled = scaler_trig.fit_transform(X_trig)\n",
    "    \n",
    "    # Combine scaled features\n",
    "    X_scaled = np.hstack([X_poly_scaled, X_trig_scaled])\n",
    "   \n",
    "    # Step 2: Sparse Identification using ElasticNet\n",
    "    elastic_net = ElasticNetCV(\n",
    "        alphas=np.logspace(np.log10(alpha_range[0]), np.log10(alpha_range[1]), 100),\n",
    "        l1_ratio=np.linspace(l1_ratio_range[0], l1_ratio_range[1], 10),\n",
    "        cv=cv_folds,\n",
    "        max_iter=10000,\n",
    "        random_state=0\n",
    "    )\n",
    "    \n",
    "    # Fit the model\n",
    "    elastic_net.fit(X_scaled, interpolated_data['y'])\n",
    "   \n",
    "    # Step 3: Model Validation\n",
    "    scores = cross_val_score(\n",
    "        elastic_net,\n",
    "        X_scaled,\n",
    "        interpolated_data['y'],\n",
    "        cv=cv_folds,\n",
    "        scoring='neg_mean_squared_error'\n",
    "    )\n",
    "    print(\"Cross-validation MSE:\", -np.mean(scores))\n",
    "   \n",
    "    # Step 4: Uncertainty Quantification using Bootstrapping\n",
    "    bootstrapped_models = bootstrap_model_fitting(\n",
    "        X_scaled,\n",
    "        interpolated_data['y'],\n",
    "        elastic_net,\n",
    "        n_bootstrap=n_bootstrap\n",
    "    )\n",
    "   \n",
    "    # Step 5: Final Model Selection\n",
    "    final_model = select_best_model(bootstrapped_models, X_scaled, interpolated_data['y'])\n",
    "   \n",
    "    return final_model, elastic_net.coef_, feature_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7a06bd8a-0412-4788-9bf3-25ea825ccb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Simulate some noisy data for testing\n",
    "np.random.seed(0)\n",
    "x = np.linspace(0, 10, 100)\n",
    "y = 2*x - 0.5*x**2 + np.sin(x)\n",
    "data = {\"x\": x, \"y\": y}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ff4a61-f3f0-4e4e-9f0e-3761e36e5c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation MSE: 1.199804084936819e-08\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Discover the differential equation\n",
    "model, coefficients, poly_feature_descriptions = robust_differential_equation_discovery(\n",
    "    data,\n",
    "    candidate_functions,\n",
    "    candidate_function_descriptions,\n",
    "    derivative_order=3\n",
    ")\n",
    "\n",
    "# Identify the non-zero coefficients\n",
    "non_zero_indices = np.nonzero(coefficients)[0]\n",
    "\n",
    "# Ensure that the index does not exceed the length of feature descriptions\n",
    "valid_non_zero_indices = [idx for idx in non_zero_indices if idx < len(poly_feature_descriptions)]\n",
    "\n",
    "# Print the non-zero coefficients and corresponding functions\n",
    "print(\"Discovered model:\")\n",
    "terms = []\n",
    "for idx in valid_non_zero_indices:\n",
    "    description = poly_feature_descriptions[idx]  # Get the description of the function\n",
    "    coefficient = coefficients[idx]\n",
    "    terms.append(f\"{coefficient:.6f} * {description}\")\n",
    "    print(f\"Coefficient: {coefficient:.6f}, Function: {description}\")\n",
    "\n",
    "# Print the final equation\n",
    "equation = \" + \".join(terms)\n",
    "print(f\"Final Model: y = {equation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6b579e-1489-429e-bdc2-b52122828b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold for filtering small coefficients\n",
    "threshold = 1e-5\n",
    "\n",
    "# Identify the non-zero coefficients with magnitude above the threshold\n",
    "significant_indices = [idx for idx in np.nonzero(coefficients)[0] if abs(coefficients[idx]) > threshold]\n",
    "\n",
    "# Print the significant coefficients and corresponding functions\n",
    "print(\"Discovered model:\")\n",
    "terms = []\n",
    "for idx in significant_indices:\n",
    "    description = poly_feature_descriptions[idx]  # Get the description of the function\n",
    "    coefficient = coefficients[idx]\n",
    "    terms.append(f\"{coefficient:.6f} * {description}\")\n",
    "    print(f\"Coefficient: {coefficient:.6f}, Function: {description}\")\n",
    "\n",
    "# Print the final equation\n",
    "equation = \" + \".join(terms)\n",
    "print(f\"Final Model: y = {equation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fe04e6-f273-4ac7-bdd2-6d074ccd3344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with raw data (no denoising/interpolation)\n",
    "model, coefficients, feature_descriptions = robust_differential_equation_discovery_simple(\n",
    "    data,\n",
    "    candidate_functions,\n",
    "    candidate_function_descriptions\n",
    ")\n",
    "\n",
    "# Threshold for filtering small coefficients\n",
    "threshold = 1e-5\n",
    "\n",
    "# Identify the non-zero coefficients with magnitude above the threshold\n",
    "significant_indices = [idx for idx in np.nonzero(coefficients)[0] if abs(coefficients[idx]) > threshold]\n",
    "\n",
    "# Print the significant coefficients and corresponding functions\n",
    "print(\"Discovered model:\")\n",
    "terms = []\n",
    "for idx in significant_indices:\n",
    "    description = feature_descriptions[idx]  # Get the description of the function\n",
    "    coefficient = coefficients[idx]\n",
    "    terms.append(f\"{coefficient:.6f} * {description}\")\n",
    "    print(f\"Coefficient: {coefficient:.6f}, Function: {description}\")\n",
    "\n",
    "# Print the final equation\n",
    "equation = \" + \".join(terms)\n",
    "print(f\"Final Model: y = {equation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4aff70-a003-4167-97bd-164213374358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_function(x, a, b, c):\n",
    "    return a * x + b * x**2 + c * np.sin(x)\n",
    "\n",
    "params, _ = curve_fit(true_function, x, y)\n",
    "print(f\"Fitted parameters: a = {params[0]}, b = {params[1]}, c = {params[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66637a34-3ba2-4cb6-b98e-0f2fd75e91a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define Candidate Functions with Descriptive Names\n",
    "def generate_candidate_functions():\n",
    "    return [\n",
    "        (lambda x: x, \"x\"),                     # Linear term\n",
    "        (lambda x: x**2, \"x^2\"),                # Quadratic term\n",
    "        (lambda x: x**3, \"x^3\"),                # Cubic term\n",
    "        (lambda x: np.sin(x), \"sin(x)\"),        # Sine function\n",
    "        (lambda x: np.cos(x), \"cos(x)\"),        # Cosine function\n",
    "        (lambda x: np.exp(x), \"exp(x)\"),        # Exponential function\n",
    "        (lambda x: np.log(np.abs(x) + 1e-6), \"log(|x| + 1e-6)\")  # Logarithmic function\n",
    "    ]\n",
    "\n",
    "# Step 2: Generate Features with Descriptive Names\n",
    "def create_feature_matrix(x, candidate_functions):\n",
    "    X = []\n",
    "    feature_names = []\n",
    "    for func, name in candidate_functions:\n",
    "        try:\n",
    "            X.append(func(x))\n",
    "            feature_names.append(name)\n",
    "        except:\n",
    "            continue\n",
    "    return np.column_stack(X), feature_names\n",
    "\n",
    "# Step 3: Use Regularized Regression\n",
    "def discover_equation(x, y):\n",
    "    candidate_functions = generate_candidate_functions()\n",
    "    X, feature_names = create_feature_matrix(x, candidate_functions)\n",
    "\n",
    "    # Feature Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Regularized Regression with LassoCV to select the best features\n",
    "    lasso = LassoCV(cv=10, random_state=0).fit(X_scaled, y)\n",
    "\n",
    "    # Identify significant features (non-zero coefficients)\n",
    "    significant_indices = np.where(np.abs(lasso.coef_) > 1e-5)[0]\n",
    "    significant_features = [(lasso.coef_[i], feature_names[i]) for i in significant_indices]\n",
    "\n",
    "    # Step 4: Evaluate and Simplify the Model\n",
    "    equation = \"y = \" + \" + \".join(f\"{coef:.6f}*{name}\" for coef, name in significant_features)\n",
    "    print(\"Discovered model:\")\n",
    "    print(equation)\n",
    "    \n",
    "    # Calculate and print model performance\n",
    "    y_pred = lasso.predict(X_scaled)\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    print(f\"Mean Squared Error: {mse:.6f}\")\n",
    "    \n",
    "    return equation, lasso\n",
    "\n",
    "# Example usage\n",
    "np.random.seed(0)\n",
    "x = np.linspace(0, 10, 100)\n",
    "y = 2*x - 0.5*x**2 + np.sin(x)  # Generate example data\n",
    "y += np.random.normal(scale=0.1, size=y.shape)  # Add some noise\n",
    "\n",
    "# Discover the equation from data\n",
    "equation, model = discover_equation(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec97fb7-e87f-47a1-abe5-7e863438d4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define Candidate Functions\n",
    "def generate_candidate_functions():\n",
    "    return [\n",
    "        (lambda x: x, \"x\"),                     # Linear term\n",
    "        (lambda x: x**2, \"x^2\"),                # Quadratic term\n",
    "        (lambda x: np.sin(x), \"sin(x)\"),        # Sine function\n",
    "        (lambda x: np.cos(x), \"cos(x)\"),        # Cosine function\n",
    "        (lambda x: np.exp(x), \"exp(x)\"),        # Exponential function\n",
    "        (lambda x: np.log(np.abs(x) + 1e-6), \"log(|x| + 1e-6)\")  # Logarithmic function\n",
    "    ]\n",
    "\n",
    "# Step 2: Build Candidate Equations\n",
    "def build_candidate_equations(candidate_functions, max_terms=3):\n",
    "    candidate_equations = []\n",
    "    for num_terms in range(1, max_terms + 1):\n",
    "        for combination in combinations(candidate_functions, num_terms):\n",
    "            functions, names = zip(*combination)\n",
    "            candidate_equations.append((functions, names))\n",
    "    return candidate_equations\n",
    "\n",
    "# Step 3: Evaluate Each Equation\n",
    "def evaluate_equations(x, y, candidate_equations):\n",
    "    best_mse = float('inf')\n",
    "    best_equation = None\n",
    "    best_params = None\n",
    "    best_description = \"\"\n",
    "\n",
    "    for functions, names in candidate_equations:\n",
    "        # Define a composite function\n",
    "        def composite_function(x, *params):\n",
    "            result = np.zeros_like(x)\n",
    "            for i, func in enumerate(functions):\n",
    "                result += params[i] * func(x)\n",
    "            return result\n",
    "        \n",
    "        # Initial guess for parameters\n",
    "        initial_guess = np.ones(len(functions))\n",
    "        \n",
    "        try:\n",
    "            # Fit the model using curve_fit\n",
    "            params, _ = curve_fit(composite_function, x, y, p0=initial_guess)\n",
    "            \n",
    "            # Calculate MSE\n",
    "            y_pred = composite_function(x, *params)\n",
    "            mse = mean_squared_error(y, y_pred)\n",
    "            \n",
    "            # Simple description of the equation\n",
    "            equation_description = \" + \".join(f\"{param:.6f}*{name}\" for param, name in zip(params, names))\n",
    "            \n",
    "            # Check if this is the best equation (balance between simplicity and accuracy)\n",
    "            if mse < best_mse:\n",
    "                best_mse = mse\n",
    "                best_equation = composite_function\n",
    "                best_params = params\n",
    "                best_description = equation_description\n",
    "                \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    return best_description, best_mse\n",
    "\n",
    "# Step 4: Implement the Process\n",
    "def discover_best_equation(x, y):\n",
    "    candidate_functions = generate_candidate_functions()\n",
    "    candidate_equations = build_candidate_equations(candidate_functions, max_terms=3)\n",
    "    best_description, best_mse = evaluate_equations(x, y, candidate_equations)\n",
    "    \n",
    "    print(\"Best Discovered Model:\")\n",
    "    print(f\"y = {best_description}\")\n",
    "    print(f\"Mean Squared Error: {best_mse:.6f}\")\n",
    "    \n",
    "    return best_description, best_mse\n",
    "\n",
    "# Example usage\n",
    "np.random.seed(0)\n",
    "x = np.linspace(0, 10, 100)\n",
    "y = 2*x - 0.5*x**2 + np.sin(x)  # Generate example data\n",
    "y += np.random.normal(scale=0.1, size=y.shape)  # Add some noise\n",
    "\n",
    "# Discover the best equation from data\n",
    "best_description, best_mse = discover_best_equation(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acf3601-6fe4-44c4-a911-b993cd0a91d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
