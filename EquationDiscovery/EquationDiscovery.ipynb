{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07e3e05f-6f92-44d8-8577-6694340aa335",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LassoCV, ElasticNetCV\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from scipy.signal import savgol_filter\n",
    "import scipy.special as sp_special\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b57f7299-3f2d-4145-ae32-c8eb5e8d34d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_polynomial_feature_descriptions(candidate_function_descriptions, degree):\n",
    "    poly = PolynomialFeatures(degree, include_bias=False)\n",
    "    feature_combinations = poly.fit_transform(np.zeros((1, len(candidate_function_descriptions))))\n",
    "    feature_names = poly.get_feature_names_out(candidate_function_descriptions)\n",
    "    return feature_names\n",
    "    \n",
    "def robust_differential_equation_discovery(\n",
    "    data,\n",
    "    candidate_functions,\n",
    "    candidate_function_descriptions,\n",
    "    derivative_order=3,\n",
    "    alpha_range=(1e-7, 1e-4),\n",
    "    l1_ratio_range=(0.1, 0.9),\n",
    "    cv_folds=10,\n",
    "    n_bootstrap=100\n",
    "):\n",
    "    # Step 1: Denoising\n",
    "    denoised_data = denoise_data(data)\n",
    "   \n",
    "    # Step 2: Interpolation\n",
    "    interpolated_data = interpolate_data(denoised_data)\n",
    "   \n",
    "    # Step 3: Derivative Estimation\n",
    "    derivatives = estimate_derivatives(interpolated_data, order=derivative_order)\n",
    "   \n",
    "    # Step 4: Feature Engineering\n",
    "    X = create_feature_matrix(interpolated_data, derivatives, candidate_functions)\n",
    "   \n",
    "    # Polynomial Features Expansion\n",
    "    degree = 3\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "   \n",
    "    # Generate Polynomial Feature Descriptions\n",
    "    poly_feature_descriptions = create_polynomial_feature_descriptions(candidate_function_descriptions, degree)\n",
    "   \n",
    "    # Feature Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_poly)\n",
    "   \n",
    "    # Step 5: Sparse Identification using ElasticNet\n",
    "    elastic_net = ElasticNetCV(\n",
    "        alphas=np.logspace(np.log10(alpha_range[0]), np.log10(alpha_range[1]), 100),\n",
    "        l1_ratio=np.linspace(l1_ratio_range[0], l1_ratio_range[1], 10),\n",
    "        cv=cv_folds,\n",
    "        max_iter=10000,\n",
    "        random_state=0\n",
    "    )\n",
    "    elastic_net.fit(X_scaled, interpolated_data['y'])\n",
    "   \n",
    "    # Step 6: Model Validation\n",
    "    scores = cross_val_score(\n",
    "        elastic_net,\n",
    "        X_scaled,\n",
    "        interpolated_data['y'],\n",
    "        cv=cv_folds,\n",
    "        scoring='neg_mean_squared_error'\n",
    "    )\n",
    "    print(\"Cross-validation MSE:\", -np.mean(scores))\n",
    "   \n",
    "    # Step 7: Uncertainty Quantification using Bootstrapping\n",
    "    bootstrapped_models = bootstrap_model_fitting(\n",
    "        X_scaled,\n",
    "        interpolated_data['y'],\n",
    "        elastic_net,\n",
    "        n_bootstrap=n_bootstrap\n",
    "    )\n",
    "   \n",
    "    # Step 8: Final Model Selection\n",
    "    final_model = select_best_model(bootstrapped_models, X_scaled, interpolated_data['y'])\n",
    "   \n",
    "    return final_model, elastic_net.coef_, poly_feature_descriptions\n",
    "\n",
    "def denoise_data(data):\n",
    "    # Implement a denoising technique like Savitzky-Golay filtering\n",
    "    denoised_y = savgol_filter(data['y'], window_length=7, polyorder=3)\n",
    "    return {'x': data['x'], 'y': denoised_y}\n",
    "\n",
    "def interpolate_data(denoised_data):\n",
    "    # Implement an interpolation technique like Spline Interpolation\n",
    "    spline = UnivariateSpline(denoised_data['x'], denoised_data['y'], s=0)\n",
    "    interpolated_y = spline(denoised_data['x'])\n",
    "    return {'x': denoised_data['x'], 'y': interpolated_y}\n",
    "\n",
    "def estimate_derivatives(data, order=3):\n",
    "    # Use finite differences to estimate derivatives up to the specified order\n",
    "    derivatives = {'0th': data['y']}\n",
    "    for i in range(1, order + 1):\n",
    "        derivatives[f'{i}th'] = np.gradient(derivatives[f'{i-1}th'], data['x'])\n",
    "    return derivatives\n",
    "\n",
    "def create_feature_matrix(data, derivatives, candidate_functions):\n",
    "    # Construct a matrix with candidate functions applied to the data and its derivatives\n",
    "    X = []\n",
    "    feature_names = []\n",
    "    for order_label, derivative in derivatives.items():\n",
    "        for func_index, func in enumerate(candidate_functions):\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                try:\n",
    "                    feature = func(derivative)\n",
    "                    # Check for invalid values\n",
    "                    if np.any(np.isnan(feature)) or np.any(np.isinf(feature)) or np.iscomplexobj(feature):\n",
    "                        continue\n",
    "                    # Clip extreme values to prevent overflow\n",
    "                    feature = np.clip(feature, -1e6, 1e6)\n",
    "                    X.append(feature.real)  # Ensure real values\n",
    "                    feature_names.append(f'{order_label}_func_{func_index}')\n",
    "                except Exception:\n",
    "                    continue\n",
    "    if not X:\n",
    "        raise ValueError(\"No valid features generated. Check candidate functions and data.\")\n",
    "    X = np.column_stack(X)\n",
    "    return X\n",
    "\n",
    "def bootstrap_model_fitting(X, y, base_model, n_bootstrap=100):\n",
    "    # Apply bootstrapping to fit the model multiple times and quantify uncertainty\n",
    "    bootstrapped_models = []\n",
    "    n_samples = X.shape[0]\n",
    "    for i in range(n_bootstrap):\n",
    "        sample_indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "        X_sample, y_sample = X[sample_indices], y[sample_indices]\n",
    "        model = LassoCV(\n",
    "            alphas=base_model.alphas_,\n",
    "            cv=base_model.cv,            # Corrected attribute for cross-validation folds\n",
    "            max_iter=base_model.max_iter,\n",
    "            random_state=i\n",
    "        )\n",
    "        model.fit(X_sample, y_sample)\n",
    "        bootstrapped_models.append(model)\n",
    "    return bootstrapped_models# Output the discovered model coefficients\n",
    "\n",
    "def select_best_model(models, X, y):\n",
    "    # Select the model with the best performance according to mean squared error\n",
    "    mse_scores = [mean_squared_error(m.predict(X), y) for m in models]\n",
    "    best_model_index = np.argmin(mse_scores)\n",
    "    best_model = models[best_model_index]\n",
    "    print(f\"Best model MSE: {mse_scores[best_model_index]}\")\n",
    "    return best_model\n",
    "\n",
    "# Updated candidate functions with safer implementations\n",
    "candidate_functions = [\n",
    "    # Basic functions\n",
    "    lambda x: x,                                # Linear term\n",
    "    lambda x: x**2,                             # Quadratic term\n",
    "    lambda x: x**3,                             # Cubic term\n",
    "    lambda x: x**4,                             # Fourth power term\n",
    "    lambda x: x**5,                             # Fifth power term\n",
    "\n",
    "    # Trigonometric functions\n",
    "    lambda x: np.sin(x),                        # Sine function\n",
    "    lambda x: np.cos(x),                        # Cosine function\n",
    "    lambda x: np.tan(x),                        # Tangent function with domain restriction\n",
    "    lambda x: np.sin(2 * x),                    # Harmonic sine term\n",
    "    lambda x: np.cos(2 * x),                    # Harmonic cosine term\n",
    "\n",
    "    # Exponential and logarithmic functions\n",
    "    lambda x: np.exp(np.clip(x, -100, 100)),    # Exponential function with clipping\n",
    "    lambda x: np.exp(-np.clip(x, -100, 100)),   # Decaying exponential function with clipping\n",
    "    lambda x: np.log(np.abs(x) + 1e-6),         # Logarithmic function\n",
    "    lambda x: np.exp(np.clip(x**2, -100, 100)), # Exponential of a quadratic term with clipping\n",
    "    lambda x: np.log(x**2 + 1e-6),              # Logarithm of a quadratic term\n",
    "\n",
    "    # Hyperbolic functions\n",
    "    lambda x: np.tanh(x),                       # Hyperbolic tangent function\n",
    "    lambda x: np.sinh(x),                       # Hyperbolic sine function\n",
    "    lambda x: np.cosh(x),                       # Hyperbolic cosine function with clipping\n",
    "     \n",
    "    # Special functions with safe evaluations\n",
    "    lambda x: sp_special.gamma(np.clip(x, 1e-6, 100)),          # Gamma function\n",
    "    lambda x: sp_special.psi(np.clip(x, 1e-6, 100)),            # Digamma function\n",
    "    lambda x: sp_special.erf(x),                                 # Error function\n",
    "    lambda x: sp_special.erfc(x),                                # Complementary error function\n",
    "    lambda x: sp_special.jv(0, x),                               # Bessel function of the first kind (order 0)\n",
    "    lambda x: sp_special.yv(0, np.clip(x, 1e-6, 100)),          # Bessel function of the second kind (order 0)\n",
    "    lambda x: sp_special.beta(np.clip(x, 1e-6, 100), np.clip(x+1, 1e-6, 100)), # Beta function\n",
    "    lambda x: sp_special.lambertw(x).real,                       # Lambert W function (principal branch)\n",
    "    lambda x: sp_special.zeta(np.clip(x, 1.1, 100)),             # Riemann zeta function\n",
    "\n",
    "    # Inverse and root functions\n",
    "    lambda x: 1 / (np.abs(x) + 1e-6),                            # Inverse function\n",
    "    lambda x: np.sqrt(np.abs(x) + 1e-6),                         # Square root function\n",
    "]\n",
    "\n",
    "# List of descriptions for the candidate functions\n",
    "candidate_function_descriptions = [\n",
    "    \"x\",                                        # Linear term\n",
    "    \"x^2\",                                      # Quadratic term\n",
    "    \"x^3\",                                      # Cubic term\n",
    "    \"x^4\",                                      # Fourth power term\n",
    "    \"x^5\",                                      # Fifth power term\n",
    "\n",
    "    # Trigonometric functions\n",
    "    \"sin(x)\",                                   # Sine function\n",
    "    \"cos(x)\",                                   # Cosine function\n",
    "    \"tan(x)\",                                   # Tangent function with domain restriction\n",
    "    \"sin(2x)\",                                  # Harmonic sine term\n",
    "    \"cos(2x)\",                                  # Harmonic cosine term\n",
    "\n",
    "    # Exponential and logarithmic functions\n",
    "    \"exp(clip(x, -100, 100))\",                  # Exponential function with clipping\n",
    "    \"exp(-clip(x, -100, 100))\",                 # Decaying exponential function with clipping\n",
    "    \"log(abs(x) + 1e-6)\",                       # Logarithmic function\n",
    "    \"exp(clip(x^2, -100, 100))\",                # Exponential of a quadratic term with clipping\n",
    "    \"log(x^2 + 1e-6)\",                          # Logarithm of a quadratic term\n",
    "\n",
    "    # Hyperbolic functions\n",
    "    \"tanh(x)\",                                  # Hyperbolic tangent function\n",
    "    \"sinh(x)\",                                  # Hyperbolic sine function\n",
    "    \"cosh(x)\",                                  # Hyperbolic cosine function with clipping\n",
    "     \n",
    "    # Special functions with safe evaluations\n",
    "    \"gamma(clip(x, 1e-6, 100))\",                # Gamma function\n",
    "    \"psi(clip(x, 1e-6, 100))\",                  # Digamma function\n",
    "    \"erf(x)\",                                   # Error function\n",
    "    \"erfc(x)\",                                  # Complementary error function\n",
    "    \"jv(0, x)\",                                 # Bessel function of the first kind (order 0)\n",
    "    \"yv(0, clip(x, 1e-6, 100))\",                # Bessel function of the second kind (order 0)\n",
    "    \"beta(clip(x, 1e-6, 100), clip(x+1, 1e-6, 100))\", # Beta function with arbitrary parameters\n",
    "    \"lambertw(x).real\",                         # Lambert W function (principal branch)\n",
    "    \"zeta(clip(x, 1.1, 100))\",                  # Riemann zeta function\n",
    "\n",
    "    # Inverse and root functions\n",
    "    \"1 / (abs(x) + 1e-6)\",                      # Inverse function\n",
    "    \"sqrt(abs(x) + 1e-6)\",                      # Square root function\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a06bd8a-0412-4788-9bf3-25ea825ccb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Simulate some noisy data for testing\n",
    "np.random.seed(0)\n",
    "x = np.linspace(0, 10, 100)\n",
    "y = 2*x - 0.5*x**2 + np.sin(x)\n",
    "data = {\"x\": x, \"y\": y}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ff4a61-f3f0-4e4e-9f0e-3761e36e5c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Discover the differential equation\n",
    "model, coefficients, poly_feature_descriptions = robust_differential_equation_discovery(\n",
    "    data,\n",
    "    candidate_functions,\n",
    "    candidate_function_descriptions,\n",
    "    derivative_order=3\n",
    ")\n",
    "\n",
    "# Identify the non-zero coefficients\n",
    "non_zero_indices = np.nonzero(coefficients)[0]\n",
    "\n",
    "# Ensure that the index does not exceed the length of feature descriptions\n",
    "valid_non_zero_indices = [idx for idx in non_zero_indices if idx < len(poly_feature_descriptions)]\n",
    "\n",
    "# Print the non-zero coefficients and corresponding functions\n",
    "print(\"Discovered model:\")\n",
    "terms = []\n",
    "for idx in valid_non_zero_indices:\n",
    "    description = poly_feature_descriptions[idx]  # Get the description of the function\n",
    "    coefficient = coefficients[idx]\n",
    "    terms.append(f\"{coefficient:.6f} * {description}\")\n",
    "    print(f\"Coefficient: {coefficient:.6f}, Function: {description}\")\n",
    "\n",
    "# Print the final equation\n",
    "equation = \" + \".join(terms)\n",
    "print(f\"Final Model: y = {equation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6b579e-1489-429e-bdc2-b52122828b63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c5162e-cca0-41eb-901a-7151703e8379",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fe04e6-f273-4ac7-bdd2-6d074ccd3344",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
