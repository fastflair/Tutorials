{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77e63123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path\n",
    "from parameters import *\n",
    "import sys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c454aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stock Ticker to get\n",
    "TICKER = 'EPAM'\n",
    "# Number of days ahead to forecast\n",
    "LOOKUP_STEP = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a9b8a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# naming convention for model name\n",
    "model_name = f\"{date_now}_{TICKER}-{shuffle_str}-{scale_str}-{split_by_date_str}-{LOSS}-{OPTIMIZER}-{CELL.__name__}-seq-{N_STEPS}-step-{LOOKUP_STEP}-layers-{N_LAYERS}-units-{UNITS}-activation-{ACTIVATION}\"\n",
    "if BIDIRECTIONAL:\n",
    "    model_name += \"-b\"\n",
    "# file to save csv data to\n",
    "ticker_data_filename = os.path.join(\"data\", f\"{TICKER}_{date_now}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35464b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create these folders if they does not exist\n",
    "if not os.path.isdir(\"results\"):\n",
    "    os.mkdir(\"results\")\n",
    "\n",
    "if not os.path.isdir(\"logs\"):\n",
    "    os.mkdir(\"logs\")\n",
    "\n",
    "if not os.path.isdir(\"data\"):\n",
    "    os.mkdir(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "accfb6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stock_prediction import create_model, load_data\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da62f40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\repos\\Tutorials\\TimeSeriesML\\StockForecasts\\stock_prediction.py:97: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['in_uptrend'][current] = df['in_uptrend'][previous]\n",
      "G:\\repos\\Tutorials\\TimeSeriesML\\StockForecasts\\stock_prediction.py:100: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['lowerband'][current] = df['lowerband'][previous]\n",
      "G:\\repos\\Tutorials\\TimeSeriesML\\StockForecasts\\stock_prediction.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['in_uptrend'][current] = 0\n",
      "G:\\repos\\Tutorials\\TimeSeriesML\\StockForecasts\\stock_prediction.py:103: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['upperband'][current] = df['upperband'][previous]\n",
      "G:\\repos\\Tutorials\\TimeSeriesML\\StockForecasts\\stock_prediction.py:93: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['in_uptrend'][current] = 1\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "data = load_data(TICKER, n_steps=N_STEPS, scale=SCALE, split_by_date=SPLIT_BY_DATE, shuffle=SHUFFLE, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE, feature_columns=FEATURE_COLUMNS, ma_periods=MA_PERIODS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddd92027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'df':                   open        high         low       close    adjclose  \\\n",
       " 2019-01-02  113.790001  115.889999  112.300003  114.320000  114.320000   \n",
       " 2019-01-03  112.559998  113.980003  111.440002  112.290001  112.290001   \n",
       " 2019-01-04  114.000000  118.480003  113.339996  117.339996  117.339996   \n",
       " 2019-01-07  120.000000  124.589996  119.269997  123.339996  123.339996   \n",
       " 2019-01-08  124.599998  126.110001  122.120003  125.370003  125.370003   \n",
       " ...                ...         ...         ...         ...         ...   \n",
       " 2022-03-15  216.929993  228.610001  210.919998  225.039993  225.039993   \n",
       " 2022-03-16  233.509995  284.500000  232.300003  281.720001  281.720001   \n",
       " 2022-03-17  270.470001  275.709991  261.970001  273.190002  273.190002   \n",
       " 2022-03-18  273.230011  300.220001  271.839996  294.470001  294.470001   \n",
       " 2022-03-18  273.230011  300.220001  271.839996  294.470001  294.470001   \n",
       " \n",
       "              volume ticker        sma7        ema7       sma20  ...    percc1  \\\n",
       " 2019-01-02   283500   EPAM  114.320000  114.320000  114.320000  ...  0.000000   \n",
       " 2019-01-03   329200   EPAM  113.305000  113.160000  113.305000  ...  0.982243   \n",
       " 2019-01-04   279600   EPAM  114.649999  114.967566  114.649999  ...  1.044973   \n",
       " 2019-01-07   479900   EPAM  116.822498  118.029484  116.822498  ...  1.051133   \n",
       " 2019-01-08   263600   EPAM  118.531999  120.435595  118.531999  ...  1.016459   \n",
       " ...             ...    ...         ...         ...         ...  ...       ...   \n",
       " 2022-03-15  2451300   EPAM  199.128571  214.914434  293.868001  ...  1.022909   \n",
       " 2022-03-16  3724600   EPAM  214.402856  231.615826  284.827000  ...  1.251866   \n",
       " 2022-03-17  1484900   EPAM  226.751428  242.009370  275.967000  ...  0.969722   \n",
       " 2022-03-18  2816600   EPAM  240.474285  255.124528  268.641000  ...  1.077895   \n",
       " 2022-03-18  2726927   EPAM  255.575714  264.960896  261.203000  ...  1.000000   \n",
       " \n",
       "               YTDPer      ROC5     ROC10     ROC25       MDT         NC  \\\n",
       " 2019-01-02  0.000000  0.000000  0.000000  0.000000  1.000000   0.000000   \n",
       " 2019-01-03  0.000000  0.000000  0.000000  0.000000  0.991042   2.029999   \n",
       " 2019-01-04  0.000000  0.000000  0.000000  0.000000  1.023463   5.049995   \n",
       " 2019-01-07  0.000000  0.000000  0.000000  0.000000  1.055790   6.000000   \n",
       " 2019-01-08  0.000000  0.000000  0.000000  0.000000  1.057689   2.030006   \n",
       " ...              ...       ...       ...       ...       ...        ...   \n",
       " 2022-03-15  0.605467  1.205033  1.065480  0.502221  0.409101   5.039993   \n",
       " 2022-03-16  0.753504  1.419888  1.149080  0.617753  0.514680  56.680008   \n",
       " 2022-03-17  0.740533  1.447288  1.282703  0.570918  0.501672   8.529999   \n",
       " 2022-03-18  0.775738  1.471320  1.483327  0.625973  0.543280  21.279999   \n",
       " 2022-03-18  0.774004  1.338500  1.684611  0.691017  0.545763   0.000000   \n",
       " \n",
       "                  LIN     LIN25       date  \n",
       " 2019-01-02  0.000000  0.000000 2019-01-02  \n",
       " 2019-01-03  0.000000  0.000000 2019-01-03  \n",
       " 2019-01-04  0.299010  0.299010 2019-01-04  \n",
       " 2019-01-07  0.920833  0.609921 2019-01-07  \n",
       " 2019-01-08  1.977828  1.065890 2019-01-08  \n",
       " ...              ...       ...        ...  \n",
       " 2022-03-15  2.470241  6.924985 2022-03-15  \n",
       " 2022-03-16  0.544460  6.940826 2022-03-16  \n",
       " 2022-03-17  2.822392  7.026646 2022-03-17  \n",
       " 2022-03-18  0.299577  7.003079 2022-03-18  \n",
       " 2022-03-18  0.000000  7.270234 2022-03-18  \n",
       " \n",
       " [811 rows x 58 columns],\n",
       " 'column_scaler': {'adjclose': MinMaxScaler(),\n",
       "  'volume': MinMaxScaler(),\n",
       "  'open': MinMaxScaler(),\n",
       "  'high': MinMaxScaler(),\n",
       "  'low': MinMaxScaler(),\n",
       "  'close': MinMaxScaler(),\n",
       "  'OBV': MinMaxScaler(),\n",
       "  'SMAFast': MinMaxScaler(),\n",
       "  'SMASlow': MinMaxScaler(),\n",
       "  'ROC10': MinMaxScaler(),\n",
       "  'ROC25': MinMaxScaler(),\n",
       "  'perc1c2': MinMaxScaler()},\n",
       " 'last_sequence': array([[0.971117  , 0.14469226, 0.97160757, ..., 0.6699214 , 0.7294508 ,\n",
       "         0.93996125],\n",
       "        [0.96634173, 0.09544433, 0.9567097 , ..., 0.67950183, 0.6896132 ,\n",
       "         0.79865897],\n",
       "        [0.9463153 , 1.        , 0.9552901 , ..., 0.64118785, 0.68838793,\n",
       "         0.7955094 ],\n",
       "        ...,\n",
       "        [0.26586252, 0.11997753, 0.2576692 , ..., 0.76142365, 0.40452236,\n",
       "         1.        ],\n",
       "        [0.30102447, 0.23509681, 0.26217285, ..., 0.8805158 , 0.44353104,\n",
       "         0.7746208 ],\n",
       "        [0.30102447, 0.227345  , 0.26217285, ..., 1.        , 0.48961815,\n",
       "         0.86103   ]], dtype=float32),\n",
       " 'X_train': array([[[0.15277594, 0.01608748, 0.14777102, ..., 0.6469787 ,\n",
       "          0.86472565, 0.82693315],\n",
       "         [0.1499339 , 0.02738589, 0.14276157, ..., 0.6267638 ,\n",
       "          0.8704849 , 0.81448036],\n",
       "         [0.15447786, 0.02091113, 0.14922328, ..., 0.6196248 ,\n",
       "          0.8675244 , 0.7920969 ],\n",
       "         ...,\n",
       "         [0.23086582, 0.02034924, 0.21752821, ..., 0.6343356 ,\n",
       "          0.79739636, 0.80439675],\n",
       "         [0.23243557, 0.01545643, 0.22875464, ..., 0.619955  ,\n",
       "          0.80040234, 0.8180236 ],\n",
       "         [0.227809  , 0.0139177 , 0.23147966, ..., 0.6131687 ,\n",
       "          0.77717435, 0.8018186 ]],\n",
       " \n",
       "        [[0.17187706, 0.01146266, 0.16572024, ..., 0.61372495,\n",
       "          0.7204373 , 0.8050685 ],\n",
       "         [0.17367812, 0.01033022, 0.1650349 , ..., 0.61652285,\n",
       "          0.7255683 , 0.8144325 ],\n",
       "         [0.16839062, 0.03460408, 0.16616082, ..., 0.60212636,\n",
       "          0.71243864, 0.80283254],\n",
       "         ...,\n",
       "         [0.14940517, 0.03486342, 0.16444749, ..., 0.5422978 ,\n",
       "          0.6019681 , 0.84465355],\n",
       "         [0.08451752, 0.0638399 , 0.12432282, ..., 0.43932003,\n",
       "          0.4989442 , 0.7416921 ],\n",
       "         [0.13170853, 0.07664246, 0.11257425, ..., 0.51063126,\n",
       "          0.57944053, 0.64405835]],\n",
       " \n",
       "        [[0.12481824, 0.03426694, 0.1291528 , ..., 0.5780969 ,\n",
       "          0.76959705, 0.79860127],\n",
       "         [0.10589889, 0.03658368, 0.11187258, ..., 0.53997284,\n",
       "          0.7219731 , 0.7744398 ],\n",
       "         [0.11173166, 0.02980636, 0.10937601, ..., 0.54638374,\n",
       "          0.7213781 , 0.75011253],\n",
       "         ...,\n",
       "         [0.12151355, 0.00989799, 0.12001501, ..., 0.6000671 ,\n",
       "          0.6877837 , 0.7756035 ],\n",
       "         [0.11921679, 0.00732192, 0.11867697, ..., 0.59619486,\n",
       "          0.6707958 , 0.80452204],\n",
       "         [0.12678455, 0.00828147, 0.12021081, ..., 0.6221111 ,\n",
       "          0.6943314 , 0.79283226]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0.9268837 , 0.01233575, 0.89812994, ..., 0.6456344 ,\n",
       "          0.7578158 , 0.8210563 ],\n",
       "         [0.90442836, 0.01023513, 0.917972  , ..., 0.61880577,\n",
       "          0.7820778 , 0.8020357 ],\n",
       "         [0.9286848 , 0.00849758, 0.891195  , ..., 0.63121027,\n",
       "          0.840374  , 0.78268266],\n",
       "         ...,\n",
       "         [0.7712492 , 0.04488244, 0.7618465 , ..., 0.5203918 ,\n",
       "          0.6895417 , 0.7588998 ],\n",
       "         [0.7342862 , 0.03233921, 0.76099795, ..., 0.4983564 ,\n",
       "          0.6363481 , 0.78691626],\n",
       "         [0.71639127, 0.07517289, 0.6978656 , ..., 0.46488005,\n",
       "          0.6481765 , 0.7679476 ]],\n",
       " \n",
       "        [[0.67126566, 0.00477178, 0.64950067, ..., 0.6311849 ,\n",
       "          0.8136461 , 0.792474  ],\n",
       "         [0.67229015, 0.01093534, 0.6603191 , ..., 0.6300017 ,\n",
       "          0.8215691 , 0.8113561 ],\n",
       "         [0.6668705 , 0.01633818, 0.66423535, ..., 0.6211833 ,\n",
       "          0.8186824 , 0.79976237],\n",
       "         ...,\n",
       "         [0.8427793 , 0.02893326, 0.8280138 , ..., 0.6136654 ,\n",
       "          0.7984017 , 0.78200674],\n",
       "         [0.85394907, 0.01335581, 0.8403661 , ..., 0.614361  ,\n",
       "          0.79974705, 0.8018222 ],\n",
       "         [0.865846  , 0.01316563, 0.8495692 , ..., 0.61022884,\n",
       "          0.8142617 , 0.8074841 ]],\n",
       " \n",
       "        [[0.3940185 , 0.01148859, 0.39409956, ..., 0.60730463,\n",
       "          0.7220305 , 0.7865039 ],\n",
       "         [0.37562788, 0.01823997, 0.38091508, ..., 0.60772747,\n",
       "          0.6944183 , 0.7911384 ],\n",
       "         [0.39107734, 0.005861  , 0.37389857, ..., 0.62106997,\n",
       "          0.7258952 , 0.77345955],\n",
       "         ...,\n",
       "         [0.4907799 , 0.00516079, 0.4703348 , ..., 0.6400686 ,\n",
       "          0.77625185, 0.8054524 ],\n",
       "         [0.49358892, 0.00753804, 0.48185492, ..., 0.641291  ,\n",
       "          0.7571038 , 0.81742   ],\n",
       "         [0.49251485, 0.00364799, 0.48534688, ..., 0.65161526,\n",
       "          0.7507537 , 0.80212504]]], dtype=float32),\n",
       " 'X_test': array([[[0.9113517 , 0.00899032, 0.9055055 , ..., 0.55890465,\n",
       "          0.7598862 , 0.7966595 ],\n",
       "         [0.82926303, 0.02202628, 0.9017035 , ..., 0.5081197 ,\n",
       "          0.6876868 , 0.79868704],\n",
       "         [0.8107237 , 0.03555498, 0.83653146, ..., 0.49912217,\n",
       "          0.67366505, 0.73902667],\n",
       "         ...,\n",
       "         [0.5598315 , 0.03914246, 0.52610797, ..., 0.49531457,\n",
       "          0.48389998, 0.79488254],\n",
       "         [0.60120624, 0.03564142, 0.55694795, ..., 0.5204599 ,\n",
       "          0.50878876, 0.8314113 ],\n",
       "         [0.6147885 , 0.02337483, 0.60120094, ..., 0.56827825,\n",
       "          0.49238434, 0.8431481 ]],\n",
       " \n",
       "        [[0.41574687, 0.01858575, 0.39256576, ..., 0.62486047,\n",
       "          0.80274063, 0.8066115 ],\n",
       "         [0.4146894 , 0.01812759, 0.41191828, ..., 0.6541114 ,\n",
       "          0.7914238 , 0.8271989 ],\n",
       "         [0.4087905 , 0.00781466, 0.40666404, ..., 0.63080585,\n",
       "          0.78193885, 0.79740244],\n",
       "         ...,\n",
       "         [0.43917713, 0.01766079, 0.4504928 , ..., 0.6023878 ,\n",
       "          0.7341596 , 0.84163344],\n",
       "         [0.44814938, 0.00895574, 0.42906794, ..., 0.6125025 ,\n",
       "          0.7311975 , 0.7789061 ],\n",
       "         [0.4699273 , 0.03023859, 0.44804516, ..., 0.6298243 ,\n",
       "          0.76118064, 0.81027985]],\n",
       " \n",
       "        [[0.30042958, 0.02606328, 0.2948404 , ..., 0.6277147 ,\n",
       "          0.8269148 , 0.7980407 ],\n",
       "         [0.30599803, 0.02281293, 0.29989883, ..., 0.6347019 ,\n",
       "          0.8332488 , 0.8060425 ],\n",
       "         [0.30685723, 0.02200899, 0.30293387, ..., 0.6453683 ,\n",
       "          0.844048  , 0.80796033],\n",
       "         ...,\n",
       "         [0.3846497 , 0.00517808, 0.37601984, ..., 0.6226974 ,\n",
       "          0.7730176 , 0.8050375 ],\n",
       "         [0.39073035, 0.01172199, 0.38091508, ..., 0.6359105 ,\n",
       "          0.79881334, 0.8173533 ],\n",
       "         [0.38764048, 0.00959544, 0.38744205, ..., 0.63696176,\n",
       "          0.77068835, 0.80732596]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0.80917054, 0.0399464 , 0.7921154 , ..., 0.6425586 ,\n",
       "          0.8279219 , 0.7932283 ],\n",
       "         [0.81883675, 0.01656293, 0.79864234, ..., 0.6445608 ,\n",
       "          0.8159796 , 0.80147   ],\n",
       "         [0.83721083, 0.01215422, 0.80844915, ..., 0.6579979 ,\n",
       "          0.82038456, 0.80656976],\n",
       "         ...,\n",
       "         [0.86230993, 0.00779737, 0.8550682 , ..., 0.65168005,\n",
       "          0.72157544, 0.8166162 ],\n",
       "         [0.8519663 , 0.00883472, 0.8582338 , ..., 0.64023745,\n",
       "          0.7131813 , 0.80053675],\n",
       "         [0.87637144, 0.00547199, 0.8451798 , ..., 0.64128596,\n",
       "          0.72582686, 0.7909221 ]],\n",
       " \n",
       "        [[0.27466953, 0.04397476, 0.27150643, ..., 0.6554289 ,\n",
       "          0.8413752 , 0.84157085],\n",
       "         [0.2740251 , 0.01250864, 0.27126163, ..., 0.6457812 ,\n",
       "          0.8356368 , 0.7978334 ],\n",
       "         [0.27417383, 0.00896438, 0.27183276, ..., 0.6301177 ,\n",
       "          0.8138887 , 0.7976888 ],\n",
       "         ...,\n",
       "         [0.34808332, 0.01014869, 0.327818  , ..., 0.60868704,\n",
       "          0.7557228 , 0.79939103],\n",
       "         [0.35801387, 0.01000173, 0.34752953, ..., 0.6015451 ,\n",
       "          0.7553917 , 0.8190197 ],\n",
       "         [0.3523959 , 0.01200726, 0.3551661 , ..., 0.5770193 ,\n",
       "          0.73596853, 0.8136728 ]],\n",
       " \n",
       "        [[0.12658624, 0.02366874, 0.1269173 , ..., 0.5913861 ,\n",
       "          0.68476695, 0.8019051 ],\n",
       "         [0.12799075, 0.01061549, 0.12339272, ..., 0.61773217,\n",
       "          0.6827831 , 0.7985114 ],\n",
       "         [0.13167547, 0.00947441, 0.13088246, ..., 0.61774194,\n",
       "          0.6888832 , 0.8024017 ],\n",
       "         ...,\n",
       "         [0.10761731, 0.00646611, 0.10518244, ..., 0.5633188 ,\n",
       "          0.6969973 , 0.7929937 ],\n",
       "         [0.10738599, 0.01333852, 0.10797271, ..., 0.5658403 ,\n",
       "          0.68896025, 0.80543524],\n",
       "         [0.10826173, 0.00769364, 0.10619411, ..., 0.62087995,\n",
       "          0.6943715 , 0.79817694]]], dtype=float32),\n",
       " 'y_train': array([0.29377064, 0.14020159, 0.12713152, 0.760922  , 0.67229012,\n",
       "        0.38764048, 0.16508592, 0.3523959 , 0.91017848, 0.2368969 ,\n",
       "        0.81883672, 0.09583608, 0.72961002, 0.12832121, 0.45513879,\n",
       "        0.79762065, 0.17113351, 0.41574686, 0.13440186, 0.81460672,\n",
       "        0.58843359, 0.14436551, 0.2006279 , 0.13484797, 0.11974553,\n",
       "        0.39119301, 0.19692664, 0.54730667, 0.85465968, 0.12670192,\n",
       "        0.61138466, 0.09605089, 0.10766688, 0.56059154, 0.1321877 ,\n",
       "        0.35925313, 0.3240086 , 0.16525116, 0.26586253, 0.38560807,\n",
       "        0.17171184, 0.20201587, 0.35304034, 0.19236617, 0.37374424,\n",
       "        0.12010905, 0.227809  , 0.19748843, 0.09963648, 0.10860873,\n",
       "        0.79760411, 0.09986781, 0.20768341, 0.16359882, 0.09869465,\n",
       "        0.57873433, 0.55219765, 0.59923995, 0.86059155, 0.12584269,\n",
       "        0.39567086, 0.41924982, 0.86584599, 0.3451917 , 0.16450761,\n",
       "        0.3349141 , 0.80259423, 0.19851288, 0.80039658, 0.1828817 ,\n",
       "        0.13835097, 0.57455389, 0.1729511 , 0.5290648 , 0.11482155,\n",
       "        0.19796763, 0.87637143, 0.15566755, 0.82073696, 0.84986783,\n",
       "        0.4304197 , 0.55292468, 0.22341375, 0.39073036, 0.10644416,\n",
       "        0.18927627, 0.6040152 , 0.07904826, 0.37480172, 0.3937211 ,\n",
       "        0.68641776, 0.21069069, 0.59930601, 0.15948447, 0.17191012,\n",
       "        0.11138466, 0.13415399, 0.14246529, 0.4381031 , 0.3420522 ,\n",
       "        0.18793787, 0.43917712, 0.15343687, 0.90613021, 0.12660278,\n",
       "        0.43215464, 0.43007272, 0.11804362, 0.38357567, 0.39510907,\n",
       "        0.09254792, 0.19130867, 0.73712827, 0.08539326, 0.42860211,\n",
       "        0.81999338, 0.20152016, 0.41620951, 0.36531726, 0.3773298 ,\n",
       "        0.34818244, 0.11726701, 0.36653998, 0.10723728, 0.42253803,\n",
       "        0.81072374, 0.97133183, 0.12262062, 0.14940517, 0.3894911 ,\n",
       "        0.1252809 , 0.13317911, 0.41738271, 0.85561797, 0.67126567,\n",
       "        0.32561138, 0.2023959 , 0.83678128, 0.09846333, 0.12549571,\n",
       "        0.44168871, 0.10826173, 0.75708855, 0.86230995, 0.12837078,\n",
       "        0.3480833 , 0.11862195, 0.18435228, 0.13547587, 0.4233642 ,\n",
       "        0.163384  , 0.8128387 , 0.19767019, 0.15788169, 0.19142432,\n",
       "        0.33098151, 0.59408459, 0.10642762, 0.59479513, 0.64195306,\n",
       "        0.2033047 , 0.09699274, 0.16501984, 0.13815267, 0.97946135,\n",
       "        0.36786189, 0.13458362, 0.10713814, 0.30599804, 0.38704561,\n",
       "        0.11574686, 0.62273632, 0.18367481, 0.23174156, 0.16148382,\n",
       "        0.15403172, 0.33691343, 0.34776935, 0.44035032, 0.85456044,\n",
       "        0.5548579 , 0.60299076, 0.12799075, 0.40384999, 0.37562789,\n",
       "        0.13742566, 0.16645737, 0.36855585, 0.1904825 , 0.11920025,\n",
       "        0.14631526, 0.37171184, 0.46627564, 0.11345009, 0.11877065,\n",
       "        0.57260408, 0.10019827, 0.71639125, 0.34606744, 0.20275942,\n",
       "        0.78392263, 0.09616657, 0.23086582, 0.37172838, 0.92921354,\n",
       "        0.09834765, 0.84221746, 0.76528422, 0.35300726, 0.38339394,\n",
       "        0.36597819, 0.11247522, 0.15561798, 0.52562789, 0.44636483,\n",
       "        0.18955716, 0.20606411, 0.10553537, 0.18341043, 0.27523131,\n",
       "        0.45003304, 0.12303371, 0.20016522, 0.19124256, 0.41468938,\n",
       "        0.60525446, 0.10398216, 0.57409124, 0.26736617, 0.33570719,\n",
       "        0.5555684 , 0.86007932, 0.14073035, 0.72034038, 0.20275942,\n",
       "        0.75829483, 0.57262062, 0.14076338, 0.36240912, 0.10123926,\n",
       "        0.14628223, 0.30345341, 0.44611699, 0.89238267, 0.67328159,\n",
       "        0.09456379, 0.1052049 , 0.09738929, 0.39517517, 0.55147061,\n",
       "        0.24595178, 0.18446793, 0.12176139, 0.40325512, 0.36913418,\n",
       "        0.20413087, 0.14993391, 0.14595174, 0.24418376, 0.89221748,\n",
       "        0.61488767, 0.15844348, 0.35183411, 0.30042959, 0.18342697,\n",
       "        0.15447786, 0.69778591, 0.15327165, 0.83945807, 0.17367811,\n",
       "        0.88585589, 0.65874091, 0.34068077, 0.70991408, 0.11191342,\n",
       "        0.18005617, 0.3327991 , 0.11941507, 0.42736285, 0.83935894,\n",
       "        0.33032058, 0.19805022, 0.92688367, 0.43574026, 0.12481825,\n",
       "        0.10738599, 0.14626569, 0.83724388, 0.68532715, 0.10001654,\n",
       "        0.08942499, 0.17936219, 0.45439526, 0.10589888, 0.54340715,\n",
       "        0.8372108 , 0.09234962, 0.42460346, 0.34786849, 0.99932258,\n",
       "        0.12946133, 0.92868478, 0.11820886, 0.72934569, 0.91847329,\n",
       "        0.08451752, 0.61761403, 0.12635492, 0.16032718, 0.12301719,\n",
       "        0.08142764, 0.57081957, 0.46992731, 0.16095505, 0.6051223 ,\n",
       "        0.09950429, 0.09770324, 0.74649705, 0.35801387, 0.20087575,\n",
       "        0.12678454, 0.11921679, 0.09216789, 0.13749174, 0.40879049,\n",
       "        0.91009588, 0.09707534, 0.77640448, 1.        , 0.13078322,\n",
       "        0.7270489 , 0.38567418, 0.75812953, 0.37994054, 0.0950099 ,\n",
       "        0.19864507, 0.91447454, 0.73878062, 0.36640782, 0.39107734,\n",
       "        0.38017184, 0.67542962, 0.11898546, 0.20779908, 0.3895737 ,\n",
       "        0.13060146, 0.40690679, 0.35494053, 0.10733642, 0.62903175,\n",
       "        0.13314606, 0.86004624, 0.10209848, 0.38319566, 0.12123266,\n",
       "        0.08955718, 0.82032387, 0.55865833, 0.39401851, 0.85109055,\n",
       "        0.11009583, 0.84522474, 0.80194979, 0.35135492, 0.19269664,\n",
       "        0.93448453, 0.128883  , 0.10735293, 0.13170853, 0.82891609,\n",
       "        0.23362524, 0.49358892, 0.31477199, 0.10361864, 0.55295771,\n",
       "        0.23137807, 0.35084269, 0.1301388 , 0.86232649, 0.35917053,\n",
       "        0.90586588, 0.36558164, 0.29606742, 0.10756775, 0.51615998,\n",
       "        0.10761731, 0.18451753, 0.09001984, 0.2411269 , 0.51858893,\n",
       "        0.57486784, 0.09841376, 0.10622935, 0.35778257, 0.10743555,\n",
       "        0.56006278, 0.13380701, 0.09560475, 0.80917053, 0.30102446,\n",
       "        0.1659782 , 0.76893594, 0.43223729, 0.29653008, 0.59175482,\n",
       "        0.14230007, 0.75477532, 0.7394415 , 0.85431265, 0.09535691,\n",
       "        0.10651024, 0.55608062, 0.52123264, 0.90442834, 0.34097818,\n",
       "        0.38721085, 0.40657635, 0.53060147, 0.70059482, 0.40679117,\n",
       "        0.34705881, 0.32333114, 0.64993392, 0.50935227, 0.16870456,\n",
       "        0.84339066, 0.92671848, 0.82377732, 0.71523468, 0.19211831,\n",
       "        0.6306841 , 0.43980502, 0.65801387, 0.13636816, 0.83891277,\n",
       "        0.66574689, 0.09724058, 0.15593193, 0.22918045, 0.16459021,\n",
       "        0.20000001, 0.66475542, 0.20432914, 0.19838071, 0.75670854,\n",
       "        0.50551886, 0.27995704, 0.27417383, 0.21467285, 0.44814939,\n",
       "        0.86589561, 0.42402513, 0.58231989, 0.34142435, 0.47584269,\n",
       "        0.10196629, 0.10925315, 0.93797095, 0.12144744, 0.10279247,\n",
       "        0.47538003, 0.27466952, 0.18326173, 0.12486781, 0.60637805,\n",
       "        0.13167547, 0.19948778, 0.38051882, 0.49077991, 0.11933244,\n",
       "        0.86092204, 0.33349306, 0.33453404, 0.10551883, 0.7857072 ,\n",
       "        0.37468604, 0.20565103, 0.57186056, 0.85750167, 0.73540977,\n",
       "        0.18306345, 0.46624256, 0.86490415, 0.32430601, 0.70399865,\n",
       "        0.18854923, 0.33605422, 0.16135162, 0.22108394, 0.99796765,\n",
       "        0.3441672 , 0.38464969, 0.41615994, 0.31637477, 0.2845671 ,\n",
       "        0.27402513, 0.2294448 , 0.10047919, 0.17944481, 0.12053867,\n",
       "        0.92319897, 0.16637475, 0.82901523, 0.16016193, 0.13992069,\n",
       "        0.09497687, 0.85394908, 0.15905486, 0.11173167, 0.13982155,\n",
       "        0.80019831, 0.3625909 , 0.19852942, 0.57209186, 0.45446132,\n",
       "        0.10839392, 0.56799407, 0.75389953, 0.57665235]),\n",
       " 'y_test': array([0.21956378, 0.57288501, 0.35472572, 0.3245704 , 0.14515863,\n",
       "        0.65419696, 0.23283212, 0.68659949, 0.34079645, 0.15277594,\n",
       "        0.15760079, 0.09965302, 0.23243557, 0.19884337, 0.42851951,\n",
       "        0.37729677, 0.85196635, 0.18446793, 0.21825844, 0.18522802,\n",
       "        0.12346331, 0.16478851, 0.18025447, 0.77124917, 0.88681427,\n",
       "        0.34269664, 0.1251322 , 0.18623596, 0.54682752, 0.76701916,\n",
       "        0.5512558 , 0.15773298, 0.13466622, 0.30685724, 0.17296762,\n",
       "        0.28876403, 0.35064442, 0.65755122, 0.10110707, 0.30806347,\n",
       "        0.23350959, 0.09091209, 0.73428619, 0.40847654, 0.96614344,\n",
       "        0.10629543, 0.11959683, 0.40180105, 0.66687047, 0.18116324,\n",
       "        0.97111702, 0.49251485, 0.15571712, 0.85067747, 0.82926302,\n",
       "        0.91135168, 0.39963648, 0.1090879 , 0.34937213, 0.36739924,\n",
       "        0.08357567, 0.83818573, 0.73339396, 0.39813284, 0.55287512,\n",
       "        0.84391943, 0.10328817, 0.2312624 , 0.11206213, 0.55581629,\n",
       "        0.19428288, 0.12708195, 0.67124923, 0.09167218, 0.52157967,\n",
       "        0.36763054, 0.72813951, 0.33235293, 0.91896897, 0.19555519,\n",
       "        0.10951753, 0.45941836, 0.09206873, 0.09562129, 0.16229345,\n",
       "        0.21462326, 0.36730005, 0.20870788, 0.18109716, 0.10038003,\n",
       "        0.33709516, 0.11425975, 0.12542961, 0.33379047, 0.71178124,\n",
       "        0.10370126, 0.77704892, 0.12151355, 0.35784863, 0.16345011,\n",
       "        0.79742238, 0.46168212, 0.08752478, 0.15432915, 0.39877728,\n",
       "        0.17072043, 0.53413746, 0.35358559, 0.10657633, 0.84061473,\n",
       "        0.8427793 , 0.61478848, 0.60120625, 0.96634171, 0.85049573,\n",
       "        0.37252147, 0.87559488, 0.18109716, 0.1751983 , 0.5714805 ,\n",
       "        0.13712822, 0.12091871, 0.92240588, 0.92377725, 0.18982155,\n",
       "        0.15984798, 0.70774951, 0.66753145, 0.19674488, 0.10551883,\n",
       "        0.39750494, 0.11571383, 0.24753802, 0.46364841, 0.11082287,\n",
       "        0.55427957, 0.33828485, 0.69770321, 0.35884004, 0.15335427,\n",
       "        0.34829807, 0.94631528, 0.34914078, 0.18364176, 0.64702578,\n",
       "        0.34862856, 0.67237282, 0.12658624, 0.54312623, 0.09031725,\n",
       "        0.27204229, 0.43093192, 0.65046269, 0.83864844, 0.55983148,\n",
       "        0.85029746, 0.1284534 , 0.35464312, 0.12554528, 0.80586584,\n",
       "        0.11789491, 0.10710509, 0.18630204, 0.10778256, 0.14309319,\n",
       "        0.65461004, 0.35715467, 0.87756117, 0.15902181, 0.10375083,\n",
       "        0.78878054, 0.57384334, 0.32495045, 0.31685392, 0.37792467,\n",
       "        0.21102115, 0.13498017, 0.56409451, 0.51349967, 0.60950101,\n",
       "        0.1054858 , 0.15859221, 0.38841703, 0.1949934 , 0.12775942,\n",
       "        0.56477198, 0.43179113, 0.16419366, 0.33486453, 0.17187706,\n",
       "        0.89424983, 0.16839061, 0.17797423, 0.17572703, 0.44309322,\n",
       "        0.67888306, 0.11928288, 0.09912425, 0.14348976, 0.14248183,\n",
       "        0.35631196, 0.30102446, 0.94168873, 0.36424325, 0.38094845,\n",
       "        0.6202578 , 0.60361866, 0.12338071, 0.83504623, 0.10624586,\n",
       "        0.20779908, 0.34684404, 0.16130206, 0.15606412, 0.88686389,\n",
       "        0.25181759, 0.10118969, 0.2771976 , 0.12546266, 0.19937211,\n",
       "        0.91151687, 0.35441177, 0.15651024]),\n",
       " 'test_df':                   open        high         low       close    adjclose  \\\n",
       " 2022-02-01  481.000000  485.709991  471.269989  484.359985  484.359985   \n",
       " 2021-03-31  387.140015  407.059998  384.730011  396.690002  396.690002   \n",
       " 2020-10-14  350.000000  355.410004  344.720001  346.890015  346.890015   \n",
       " 2020-07-16  258.160004  260.209991  255.669998  258.220001  258.220001   \n",
       " 2022-02-10  459.809998  486.500000  459.000000  470.420013  470.420013   \n",
       " ...                ...         ...         ...         ...         ...   \n",
       " 2020-03-02  224.020004  229.869995  217.990005  229.869995  229.869995   \n",
       " 2020-01-15  228.149994  231.210007  228.110001  228.710007  228.710007   \n",
       " 2021-10-21  630.520020  643.640015  625.539978  642.669983  642.669983   \n",
       " 2020-09-29  330.220001  332.739990  324.089996  325.559998  325.559998   \n",
       " 2019-11-05  177.639999  178.320007  175.529999  177.809998  177.809998   \n",
       " \n",
       "             volume ticker        sma7        ema7       sma20  ...    percc1  \\\n",
       " 2022-02-01  367400   EPAM  456.411429  469.122848  512.393500  ...  1.017264   \n",
       " 2021-03-31  446800   EPAM  379.481428  382.586163  369.774500  ...  1.034367   \n",
       " 2020-10-14  208000   EPAM  341.754290  341.803381  329.600003  ...  0.994638   \n",
       " 2020-07-16  264400   EPAM  257.620004  256.686193  252.610501  ...  0.992887   \n",
       " 2022-02-10  528300   EPAM  460.744289  464.667953  472.724002  ...  0.983093   \n",
       " ...            ...    ...         ...         ...         ...  ...       ...   \n",
       " 2020-03-02  402200   EPAM  224.307142  226.213805  231.340001  ...  1.029884   \n",
       " 2020-01-15  148600   EPAM  223.971429  224.426357  216.660000  ...  1.006779   \n",
       " 2021-10-21  160300   EPAM  623.417140  623.836037  597.014496  ...  1.023523   \n",
       " 2020-09-29  235900   EPAM  320.461430  322.710640  322.294002  ...  0.989664   \n",
       " 2019-11-05  186000   EPAM  177.272858  177.498103  181.223000  ...  1.002990   \n",
       " \n",
       "               YTDPer      ROC5     ROC10     ROC25       MDT         NC  \\\n",
       " 2022-02-01  1.305799  1.086228  0.957328  0.694921  0.789477   8.219971   \n",
       " 2021-03-31  2.136648  1.076470  1.061009  1.074284  1.135822  13.179993   \n",
       " 2020-10-14  1.826218  1.007464  1.073033  1.087702  1.274365   1.869995   \n",
       " 2020-07-16  1.300791  0.985196  1.020794  1.076814  1.168277   1.850006   \n",
       " 2022-02-10  1.197333  1.060651  1.085393  0.800306  0.776261   8.089996   \n",
       " ...              ...       ...       ...       ...       ...        ...   \n",
       " 2020-03-02  1.405503  1.016989  0.970940  1.010107  1.119677   6.669998   \n",
       " 2020-01-15  1.716398  1.033110  1.078007  1.095302  1.170901   1.540009   \n",
       " 2021-10-21  1.850155  1.052108  1.080317  1.024388  1.168814  14.769958   \n",
       " 2020-09-29  1.785652  1.011904  0.972053  1.038701  1.256360   3.399994   \n",
       " 2019-11-05  0.000000  1.006738  1.045941  0.979993  0.976088   0.529999   \n",
       " \n",
       "                  LIN     LIN25       date  \n",
       " 2022-02-01  2.023120  1.366052 2022-02-01  \n",
       " 2021-03-31  0.705995  1.847059 2021-03-31  \n",
       " 2020-10-14  0.483966  1.719307 2020-10-14  \n",
       " 2020-07-16  1.245942  5.318649 2020-07-16  \n",
       " 2022-02-10  0.888752  1.227907 2022-02-10  \n",
       " ...              ...       ...        ...  \n",
       " 2020-03-02  0.676912  2.319636 2020-03-02  \n",
       " 2020-01-15  0.048704  1.536772 2020-01-15  \n",
       " 2021-10-21  0.288085  4.116870 2021-10-21  \n",
       " 2020-09-29  0.383822  1.877017 2020-09-29  \n",
       " 2019-11-05  0.367925  1.020383 2019-11-05  \n",
       " \n",
       " [223 rows x 58 columns]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "125c132c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe to csv file\n",
    "data[\"df\"].to_csv(ticker_data_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60e414e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the model\n",
    "model = create_model(sequence_length=N_STEPS, n_features=len(FEATURE_COLUMNS), loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS,\n",
    "                    dropout=DROPOUT, optimizer=OPTIMIZER, bidirectional=BIDIRECTIONAL, activation=ACTIVATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "837d793e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create userful tensorflow callbacks\n",
    "checkpointer = ModelCheckpoint(os.path.join(\"results\", model_name + \".h5\"), save_weights_only=True, save_best_only=True, verbose=1)\n",
    "tensorboard = TensorBoard(log_dir=os.path.join(\"logs\", model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9d7ea15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2774 - mean_absolute_error: 0.2774\n",
      "Epoch 00001: val_loss improved from inf to 0.25316, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 11s 11s/step - loss: 0.2774 - mean_absolute_error: 0.2774 - val_loss: 0.2532 - val_mean_absolute_error: 0.2532\n",
      "Epoch 2/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2508 - mean_absolute_error: 0.2508\n",
      "Epoch 00002: val_loss improved from 0.25316 to 0.22977, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 0.2508 - mean_absolute_error: 0.2508 - val_loss: 0.2298 - val_mean_absolute_error: 0.2298\n",
      "Epoch 3/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2277 - mean_absolute_error: 0.2277\n",
      "Epoch 00003: val_loss improved from 0.22977 to 0.20945, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.2277 - mean_absolute_error: 0.2277 - val_loss: 0.2094 - val_mean_absolute_error: 0.2094\n",
      "Epoch 4/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2081 - mean_absolute_error: 0.2081\n",
      "Epoch 00004: val_loss improved from 0.20945 to 0.19727, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.2081 - mean_absolute_error: 0.2081 - val_loss: 0.1973 - val_mean_absolute_error: 0.1973\n",
      "Epoch 5/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1981 - mean_absolute_error: 0.1981\n",
      "Epoch 00005: val_loss improved from 0.19727 to 0.18689, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.1981 - mean_absolute_error: 0.1981 - val_loss: 0.1869 - val_mean_absolute_error: 0.1869\n",
      "Epoch 6/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1870 - mean_absolute_error: 0.1870\n",
      "Epoch 00006: val_loss improved from 0.18689 to 0.16925, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.1870 - mean_absolute_error: 0.1870 - val_loss: 0.1692 - val_mean_absolute_error: 0.1692\n",
      "Epoch 7/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1658 - mean_absolute_error: 0.1658\n",
      "Epoch 00007: val_loss improved from 0.16925 to 0.14391, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.1658 - mean_absolute_error: 0.1658 - val_loss: 0.1439 - val_mean_absolute_error: 0.1439\n",
      "Epoch 8/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1444 - mean_absolute_error: 0.1444\n",
      "Epoch 00008: val_loss improved from 0.14391 to 0.11826, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.1444 - mean_absolute_error: 0.1444 - val_loss: 0.1183 - val_mean_absolute_error: 0.1183\n",
      "Epoch 9/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1219 - mean_absolute_error: 0.1219\n",
      "Epoch 00009: val_loss improved from 0.11826 to 0.10419, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.1219 - mean_absolute_error: 0.1219 - val_loss: 0.1042 - val_mean_absolute_error: 0.1042\n",
      "Epoch 10/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1029 - mean_absolute_error: 0.1029\n",
      "Epoch 00010: val_loss improved from 0.10419 to 0.09010, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.1029 - mean_absolute_error: 0.1029 - val_loss: 0.0901 - val_mean_absolute_error: 0.0901\n",
      "Epoch 11/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0903 - mean_absolute_error: 0.0903\n",
      "Epoch 00011: val_loss improved from 0.09010 to 0.08974, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0903 - mean_absolute_error: 0.0903 - val_loss: 0.0897 - val_mean_absolute_error: 0.0897\n",
      "Epoch 12/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0784 - mean_absolute_error: 0.0784\n",
      "Epoch 00012: val_loss did not improve from 0.08974\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.0784 - mean_absolute_error: 0.0784 - val_loss: 0.1015 - val_mean_absolute_error: 0.1015\n",
      "Epoch 13/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0868 - mean_absolute_error: 0.0868\n",
      "Epoch 00013: val_loss did not improve from 0.08974\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0868 - mean_absolute_error: 0.0868 - val_loss: 0.1107 - val_mean_absolute_error: 0.1107\n",
      "Epoch 14/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0900 - mean_absolute_error: 0.0900\n",
      "Epoch 00014: val_loss did not improve from 0.08974\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.0900 - mean_absolute_error: 0.0900 - val_loss: 0.1189 - val_mean_absolute_error: 0.1189\n",
      "Epoch 15/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0937 - mean_absolute_error: 0.0937\n",
      "Epoch 00015: val_loss did not improve from 0.08974\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.0937 - mean_absolute_error: 0.0937 - val_loss: 0.1213 - val_mean_absolute_error: 0.1213\n",
      "Epoch 16/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0929 - mean_absolute_error: 0.0929\n",
      "Epoch 00016: val_loss did not improve from 0.08974\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0929 - mean_absolute_error: 0.0929 - val_loss: 0.1172 - val_mean_absolute_error: 0.1172\n",
      "Epoch 17/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0910 - mean_absolute_error: 0.0910\n",
      "Epoch 00017: val_loss did not improve from 0.08974\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0910 - mean_absolute_error: 0.0910 - val_loss: 0.1125 - val_mean_absolute_error: 0.1125\n",
      "Epoch 18/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0897 - mean_absolute_error: 0.0897\n",
      "Epoch 00018: val_loss did not improve from 0.08974\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0897 - mean_absolute_error: 0.0897 - val_loss: 0.1051 - val_mean_absolute_error: 0.1051\n",
      "Epoch 19/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0844 - mean_absolute_error: 0.0844\n",
      "Epoch 00019: val_loss did not improve from 0.08974\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.0844 - mean_absolute_error: 0.0844 - val_loss: 0.0960 - val_mean_absolute_error: 0.0960\n",
      "Epoch 20/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0768 - mean_absolute_error: 0.0768\n",
      "Epoch 00020: val_loss improved from 0.08974 to 0.08883, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0768 - mean_absolute_error: 0.0768 - val_loss: 0.0888 - val_mean_absolute_error: 0.0888\n",
      "Epoch 21/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0728 - mean_absolute_error: 0.0728\n",
      "Epoch 00021: val_loss improved from 0.08883 to 0.08021, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0728 - mean_absolute_error: 0.0728 - val_loss: 0.0802 - val_mean_absolute_error: 0.0802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0680 - mean_absolute_error: 0.0680\n",
      "Epoch 00022: val_loss improved from 0.08021 to 0.07758, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.0680 - mean_absolute_error: 0.0680 - val_loss: 0.0776 - val_mean_absolute_error: 0.0776\n",
      "Epoch 23/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0662 - mean_absolute_error: 0.0662\n",
      "Epoch 00023: val_loss did not improve from 0.07758\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.0662 - mean_absolute_error: 0.0662 - val_loss: 0.0800 - val_mean_absolute_error: 0.0800\n",
      "Epoch 24/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0715 - mean_absolute_error: 0.0715\n",
      "Epoch 00024: val_loss did not improve from 0.07758\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0715 - mean_absolute_error: 0.0715 - val_loss: 0.0816 - val_mean_absolute_error: 0.0816\n",
      "Epoch 25/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0728 - mean_absolute_error: 0.0728\n",
      "Epoch 00025: val_loss did not improve from 0.07758\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0728 - mean_absolute_error: 0.0728 - val_loss: 0.0829 - val_mean_absolute_error: 0.0829\n",
      "Epoch 26/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0745 - mean_absolute_error: 0.0745\n",
      "Epoch 00026: val_loss did not improve from 0.07758\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0745 - mean_absolute_error: 0.0745 - val_loss: 0.0837 - val_mean_absolute_error: 0.0837\n",
      "Epoch 27/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0737 - mean_absolute_error: 0.0737\n",
      "Epoch 00027: val_loss did not improve from 0.07758\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.0737 - mean_absolute_error: 0.0737 - val_loss: 0.0817 - val_mean_absolute_error: 0.0817\n",
      "Epoch 28/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0702 - mean_absolute_error: 0.0702\n",
      "Epoch 00028: val_loss did not improve from 0.07758\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0702 - mean_absolute_error: 0.0702 - val_loss: 0.0783 - val_mean_absolute_error: 0.0783\n",
      "Epoch 29/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0685 - mean_absolute_error: 0.0685\n",
      "Epoch 00029: val_loss did not improve from 0.07758\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.0685 - mean_absolute_error: 0.0685 - val_loss: 0.0798 - val_mean_absolute_error: 0.0798\n",
      "Epoch 30/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0683 - mean_absolute_error: 0.0683\n",
      "Epoch 00030: val_loss did not improve from 0.07758\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0683 - mean_absolute_error: 0.0683 - val_loss: 0.0811 - val_mean_absolute_error: 0.0811\n",
      "Epoch 31/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0664 - mean_absolute_error: 0.0664\n",
      "Epoch 00031: val_loss did not improve from 0.07758\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0664 - mean_absolute_error: 0.0664 - val_loss: 0.0838 - val_mean_absolute_error: 0.0838\n",
      "Epoch 32/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0683 - mean_absolute_error: 0.0683\n",
      "Epoch 00032: val_loss did not improve from 0.07758\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0683 - mean_absolute_error: 0.0683 - val_loss: 0.0870 - val_mean_absolute_error: 0.0870\n",
      "Epoch 33/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0678 - mean_absolute_error: 0.0678\n",
      "Epoch 00033: val_loss did not improve from 0.07758\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0678 - mean_absolute_error: 0.0678 - val_loss: 0.0861 - val_mean_absolute_error: 0.0861\n",
      "Epoch 34/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0674 - mean_absolute_error: 0.0674\n",
      "Epoch 00034: val_loss did not improve from 0.07758\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0674 - mean_absolute_error: 0.0674 - val_loss: 0.0857 - val_mean_absolute_error: 0.0857\n",
      "Epoch 35/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0691 - mean_absolute_error: 0.0691\n",
      "Epoch 00035: val_loss did not improve from 0.07758\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0691 - mean_absolute_error: 0.0691 - val_loss: 0.0845 - val_mean_absolute_error: 0.0845\n",
      "Epoch 36/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0672 - mean_absolute_error: 0.0672\n",
      "Epoch 00036: val_loss did not improve from 0.07758\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0672 - mean_absolute_error: 0.0672 - val_loss: 0.0829 - val_mean_absolute_error: 0.0829\n",
      "Epoch 37/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0659 - mean_absolute_error: 0.0659\n",
      "Epoch 00037: val_loss did not improve from 0.07758\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0659 - mean_absolute_error: 0.0659 - val_loss: 0.0821 - val_mean_absolute_error: 0.0821\n",
      "Epoch 38/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0669 - mean_absolute_error: 0.0669\n",
      "Epoch 00038: val_loss did not improve from 0.07758\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0669 - mean_absolute_error: 0.0669 - val_loss: 0.0808 - val_mean_absolute_error: 0.0808\n",
      "Epoch 39/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0661 - mean_absolute_error: 0.0661\n",
      "Epoch 00039: val_loss did not improve from 0.07758\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0661 - mean_absolute_error: 0.0661 - val_loss: 0.0778 - val_mean_absolute_error: 0.0778\n",
      "Epoch 40/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0643 - mean_absolute_error: 0.0643\n",
      "Epoch 00040: val_loss improved from 0.07758 to 0.07672, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0643 - mean_absolute_error: 0.0643 - val_loss: 0.0767 - val_mean_absolute_error: 0.0767\n",
      "Epoch 41/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0671 - mean_absolute_error: 0.0671\n",
      "Epoch 00041: val_loss did not improve from 0.07672\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0671 - mean_absolute_error: 0.0671 - val_loss: 0.0769 - val_mean_absolute_error: 0.0769\n",
      "Epoch 42/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0652 - mean_absolute_error: 0.0652\n",
      "Epoch 00042: val_loss improved from 0.07672 to 0.07599, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.0652 - mean_absolute_error: 0.0652 - val_loss: 0.0760 - val_mean_absolute_error: 0.0760\n",
      "Epoch 43/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0649 - mean_absolute_error: 0.0649\n",
      "Epoch 00043: val_loss did not improve from 0.07599\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0649 - mean_absolute_error: 0.0649 - val_loss: 0.0761 - val_mean_absolute_error: 0.0761\n",
      "Epoch 44/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0651 - mean_absolute_error: 0.0651\n",
      "Epoch 00044: val_loss did not improve from 0.07599\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0651 - mean_absolute_error: 0.0651 - val_loss: 0.0766 - val_mean_absolute_error: 0.0766\n",
      "Epoch 45/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0636 - mean_absolute_error: 0.0636\n",
      "Epoch 00045: val_loss did not improve from 0.07599\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0636 - mean_absolute_error: 0.0636 - val_loss: 0.0771 - val_mean_absolute_error: 0.0771\n",
      "Epoch 46/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0657 - mean_absolute_error: 0.0657\n",
      "Epoch 00046: val_loss did not improve from 0.07599\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0657 - mean_absolute_error: 0.0657 - val_loss: 0.0762 - val_mean_absolute_error: 0.0762\n",
      "Epoch 47/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0626 - mean_absolute_error: 0.0626\n",
      "Epoch 00047: val_loss did not improve from 0.07599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0626 - mean_absolute_error: 0.0626 - val_loss: 0.0778 - val_mean_absolute_error: 0.0778\n",
      "Epoch 48/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0640 - mean_absolute_error: 0.0640\n",
      "Epoch 00048: val_loss did not improve from 0.07599\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0640 - mean_absolute_error: 0.0640 - val_loss: 0.0778 - val_mean_absolute_error: 0.0778\n",
      "Epoch 49/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0655 - mean_absolute_error: 0.0655\n",
      "Epoch 00049: val_loss did not improve from 0.07599\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0655 - mean_absolute_error: 0.0655 - val_loss: 0.0774 - val_mean_absolute_error: 0.0774\n",
      "Epoch 50/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0648 - mean_absolute_error: 0.0648\n",
      "Epoch 00050: val_loss did not improve from 0.07599\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0648 - mean_absolute_error: 0.0648 - val_loss: 0.0790 - val_mean_absolute_error: 0.0790\n",
      "Epoch 51/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0637 - mean_absolute_error: 0.0637\n",
      "Epoch 00051: val_loss did not improve from 0.07599\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0637 - mean_absolute_error: 0.0637 - val_loss: 0.0774 - val_mean_absolute_error: 0.0774\n",
      "Epoch 52/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0603 - mean_absolute_error: 0.0603\n",
      "Epoch 00052: val_loss did not improve from 0.07599\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0603 - mean_absolute_error: 0.0603 - val_loss: 0.0767 - val_mean_absolute_error: 0.0767\n",
      "Epoch 53/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0606 - mean_absolute_error: 0.0606\n",
      "Epoch 00053: val_loss did not improve from 0.07599\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0606 - mean_absolute_error: 0.0606 - val_loss: 0.0772 - val_mean_absolute_error: 0.0772\n",
      "Epoch 54/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0647 - mean_absolute_error: 0.0647\n",
      "Epoch 00054: val_loss improved from 0.07599 to 0.07594, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0647 - mean_absolute_error: 0.0647 - val_loss: 0.0759 - val_mean_absolute_error: 0.0759\n",
      "Epoch 55/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0626 - mean_absolute_error: 0.0626\n",
      "Epoch 00055: val_loss improved from 0.07594 to 0.07503, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0626 - mean_absolute_error: 0.0626 - val_loss: 0.0750 - val_mean_absolute_error: 0.0750\n",
      "Epoch 56/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0621 - mean_absolute_error: 0.0621\n",
      "Epoch 00056: val_loss did not improve from 0.07503\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0621 - mean_absolute_error: 0.0621 - val_loss: 0.0761 - val_mean_absolute_error: 0.0761\n",
      "Epoch 57/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0642 - mean_absolute_error: 0.0642\n",
      "Epoch 00057: val_loss did not improve from 0.07503\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0642 - mean_absolute_error: 0.0642 - val_loss: 0.0753 - val_mean_absolute_error: 0.0753\n",
      "Epoch 58/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0619 - mean_absolute_error: 0.0619\n",
      "Epoch 00058: val_loss improved from 0.07503 to 0.07407, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0619 - mean_absolute_error: 0.0619 - val_loss: 0.0741 - val_mean_absolute_error: 0.0741\n",
      "Epoch 59/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0610 - mean_absolute_error: 0.0610\n",
      "Epoch 00059: val_loss did not improve from 0.07407\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0610 - mean_absolute_error: 0.0610 - val_loss: 0.0747 - val_mean_absolute_error: 0.0747\n",
      "Epoch 60/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0625 - mean_absolute_error: 0.0625\n",
      "Epoch 00060: val_loss did not improve from 0.07407\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0625 - mean_absolute_error: 0.0625 - val_loss: 0.0750 - val_mean_absolute_error: 0.0750\n",
      "Epoch 61/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0613 - mean_absolute_error: 0.0613\n",
      "Epoch 00061: val_loss improved from 0.07407 to 0.07392, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0613 - mean_absolute_error: 0.0613 - val_loss: 0.0739 - val_mean_absolute_error: 0.0739\n",
      "Epoch 62/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0603 - mean_absolute_error: 0.0603\n",
      "Epoch 00062: val_loss did not improve from 0.07392\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0603 - mean_absolute_error: 0.0603 - val_loss: 0.0759 - val_mean_absolute_error: 0.0759\n",
      "Epoch 63/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0623 - mean_absolute_error: 0.0623\n",
      "Epoch 00063: val_loss did not improve from 0.07392\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0623 - mean_absolute_error: 0.0623 - val_loss: 0.0766 - val_mean_absolute_error: 0.0766\n",
      "Epoch 64/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0634 - mean_absolute_error: 0.0634\n",
      "Epoch 00064: val_loss improved from 0.07392 to 0.07388, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0634 - mean_absolute_error: 0.0634 - val_loss: 0.0739 - val_mean_absolute_error: 0.0739\n",
      "Epoch 65/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0614 - mean_absolute_error: 0.0614\n",
      "Epoch 00065: val_loss did not improve from 0.07388\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0614 - mean_absolute_error: 0.0614 - val_loss: 0.0741 - val_mean_absolute_error: 0.0741\n",
      "Epoch 66/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0630 - mean_absolute_error: 0.0630\n",
      "Epoch 00066: val_loss did not improve from 0.07388\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0630 - mean_absolute_error: 0.0630 - val_loss: 0.0741 - val_mean_absolute_error: 0.0741\n",
      "Epoch 67/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0616 - mean_absolute_error: 0.0616\n",
      "Epoch 00067: val_loss improved from 0.07388 to 0.07318, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0616 - mean_absolute_error: 0.0616 - val_loss: 0.0732 - val_mean_absolute_error: 0.0732\n",
      "Epoch 68/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0598 - mean_absolute_error: 0.0598\n",
      "Epoch 00068: val_loss did not improve from 0.07318\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0598 - mean_absolute_error: 0.0598 - val_loss: 0.0740 - val_mean_absolute_error: 0.0740\n",
      "Epoch 69/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0614 - mean_absolute_error: 0.0614\n",
      "Epoch 00069: val_loss improved from 0.07318 to 0.07284, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0614 - mean_absolute_error: 0.0614 - val_loss: 0.0728 - val_mean_absolute_error: 0.0728\n",
      "Epoch 70/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0602 - mean_absolute_error: 0.0602\n",
      "Epoch 00070: val_loss improved from 0.07284 to 0.07178, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 0.0602 - mean_absolute_error: 0.0602 - val_loss: 0.0718 - val_mean_absolute_error: 0.0718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0615 - mean_absolute_error: 0.0615\n",
      "Epoch 00071: val_loss did not improve from 0.07178\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0615 - mean_absolute_error: 0.0615 - val_loss: 0.0725 - val_mean_absolute_error: 0.0725\n",
      "Epoch 72/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0615 - mean_absolute_error: 0.0615\n",
      "Epoch 00072: val_loss improved from 0.07178 to 0.07164, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0615 - mean_absolute_error: 0.0615 - val_loss: 0.0716 - val_mean_absolute_error: 0.0716\n",
      "Epoch 73/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0571 - mean_absolute_error: 0.0571\n",
      "Epoch 00073: val_loss improved from 0.07164 to 0.07071, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0571 - mean_absolute_error: 0.0571 - val_loss: 0.0707 - val_mean_absolute_error: 0.0707\n",
      "Epoch 74/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0593 - mean_absolute_error: 0.0593\n",
      "Epoch 00074: val_loss did not improve from 0.07071\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0593 - mean_absolute_error: 0.0593 - val_loss: 0.0713 - val_mean_absolute_error: 0.0713\n",
      "Epoch 75/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0577 - mean_absolute_error: 0.0577\n",
      "Epoch 00075: val_loss improved from 0.07071 to 0.07050, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.0577 - mean_absolute_error: 0.0577 - val_loss: 0.0705 - val_mean_absolute_error: 0.0705\n",
      "Epoch 76/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0577 - mean_absolute_error: 0.0577\n",
      "Epoch 00076: val_loss improved from 0.07050 to 0.07037, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0577 - mean_absolute_error: 0.0577 - val_loss: 0.0704 - val_mean_absolute_error: 0.0704\n",
      "Epoch 77/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0565 - mean_absolute_error: 0.0565\n",
      "Epoch 00077: val_loss improved from 0.07037 to 0.07007, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0565 - mean_absolute_error: 0.0565 - val_loss: 0.0701 - val_mean_absolute_error: 0.0701\n",
      "Epoch 78/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0572 - mean_absolute_error: 0.0572\n",
      "Epoch 00078: val_loss improved from 0.07007 to 0.06788, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 0.0572 - mean_absolute_error: 0.0572 - val_loss: 0.0679 - val_mean_absolute_error: 0.0679\n",
      "Epoch 79/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0576 - mean_absolute_error: 0.0576\n",
      "Epoch 00079: val_loss improved from 0.06788 to 0.06667, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0576 - mean_absolute_error: 0.0576 - val_loss: 0.0667 - val_mean_absolute_error: 0.0667\n",
      "Epoch 80/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0580 - mean_absolute_error: 0.0580\n",
      "Epoch 00080: val_loss improved from 0.06667 to 0.06610, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0580 - mean_absolute_error: 0.0580 - val_loss: 0.0661 - val_mean_absolute_error: 0.0661\n",
      "Epoch 81/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0569 - mean_absolute_error: 0.0569\n",
      "Epoch 00081: val_loss improved from 0.06610 to 0.06508, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0569 - mean_absolute_error: 0.0569 - val_loss: 0.0651 - val_mean_absolute_error: 0.0651\n",
      "Epoch 82/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0579 - mean_absolute_error: 0.0579\n",
      "Epoch 00082: val_loss improved from 0.06508 to 0.06449, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0579 - mean_absolute_error: 0.0579 - val_loss: 0.0645 - val_mean_absolute_error: 0.0645\n",
      "Epoch 83/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0546 - mean_absolute_error: 0.0546\n",
      "Epoch 00083: val_loss did not improve from 0.06449\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0546 - mean_absolute_error: 0.0546 - val_loss: 0.0645 - val_mean_absolute_error: 0.0645\n",
      "Epoch 84/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0554 - mean_absolute_error: 0.0554\n",
      "Epoch 00084: val_loss improved from 0.06449 to 0.06399, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0554 - mean_absolute_error: 0.0554 - val_loss: 0.0640 - val_mean_absolute_error: 0.0640\n",
      "Epoch 85/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0542 - mean_absolute_error: 0.0542\n",
      "Epoch 00085: val_loss improved from 0.06399 to 0.06318, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0542 - mean_absolute_error: 0.0542 - val_loss: 0.0632 - val_mean_absolute_error: 0.0632\n",
      "Epoch 86/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0517 - mean_absolute_error: 0.0517\n",
      "Epoch 00086: val_loss improved from 0.06318 to 0.06176, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 0.0517 - mean_absolute_error: 0.0517 - val_loss: 0.0618 - val_mean_absolute_error: 0.0618\n",
      "Epoch 87/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0533 - mean_absolute_error: 0.0533\n",
      "Epoch 00087: val_loss improved from 0.06176 to 0.05929, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0533 - mean_absolute_error: 0.0533 - val_loss: 0.0593 - val_mean_absolute_error: 0.0593\n",
      "Epoch 88/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0520 - mean_absolute_error: 0.0520\n",
      "Epoch 00088: val_loss improved from 0.05929 to 0.05617, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0520 - mean_absolute_error: 0.0520 - val_loss: 0.0562 - val_mean_absolute_error: 0.0562\n",
      "Epoch 89/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0525 - mean_absolute_error: 0.0525\n",
      "Epoch 00089: val_loss improved from 0.05617 to 0.05294, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0525 - mean_absolute_error: 0.0525 - val_loss: 0.0529 - val_mean_absolute_error: 0.0529\n",
      "Epoch 90/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0523 - mean_absolute_error: 0.0523\n",
      "Epoch 00090: val_loss improved from 0.05294 to 0.05108, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0523 - mean_absolute_error: 0.0523 - val_loss: 0.0511 - val_mean_absolute_error: 0.0511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0508 - mean_absolute_error: 0.0508\n",
      "Epoch 00091: val_loss improved from 0.05108 to 0.04823, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.0508 - mean_absolute_error: 0.0508 - val_loss: 0.0482 - val_mean_absolute_error: 0.0482\n",
      "Epoch 92/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0490 - mean_absolute_error: 0.0490\n",
      "Epoch 00092: val_loss improved from 0.04823 to 0.04752, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.0490 - mean_absolute_error: 0.0490 - val_loss: 0.0475 - val_mean_absolute_error: 0.0475\n",
      "Epoch 93/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0455 - mean_absolute_error: 0.0455\n",
      "Epoch 00093: val_loss improved from 0.04752 to 0.04453, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0455 - mean_absolute_error: 0.0455 - val_loss: 0.0445 - val_mean_absolute_error: 0.0445\n",
      "Epoch 94/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0444 - mean_absolute_error: 0.0444\n",
      "Epoch 00094: val_loss did not improve from 0.04453\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0444 - mean_absolute_error: 0.0444 - val_loss: 0.0446 - val_mean_absolute_error: 0.0446\n",
      "Epoch 95/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0442 - mean_absolute_error: 0.0442\n",
      "Epoch 00095: val_loss improved from 0.04453 to 0.04116, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 0.0442 - mean_absolute_error: 0.0442 - val_loss: 0.0412 - val_mean_absolute_error: 0.0412\n",
      "Epoch 96/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0456 - mean_absolute_error: 0.0456\n",
      "Epoch 00096: val_loss improved from 0.04116 to 0.04038, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0456 - mean_absolute_error: 0.0456 - val_loss: 0.0404 - val_mean_absolute_error: 0.0404\n",
      "Epoch 97/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0442 - mean_absolute_error: 0.0442\n",
      "Epoch 00097: val_loss did not improve from 0.04038\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0442 - mean_absolute_error: 0.0442 - val_loss: 0.0439 - val_mean_absolute_error: 0.0439\n",
      "Epoch 98/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0458 - mean_absolute_error: 0.0458\n",
      "Epoch 00098: val_loss did not improve from 0.04038\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0458 - mean_absolute_error: 0.0458 - val_loss: 0.0442 - val_mean_absolute_error: 0.0442\n",
      "Epoch 99/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0510 - mean_absolute_error: 0.0510\n",
      "Epoch 00099: val_loss did not improve from 0.04038\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0510 - mean_absolute_error: 0.0510 - val_loss: 0.0427 - val_mean_absolute_error: 0.0427\n",
      "Epoch 100/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0449 - mean_absolute_error: 0.0449\n",
      "Epoch 00100: val_loss did not improve from 0.04038\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0449 - mean_absolute_error: 0.0449 - val_loss: 0.0445 - val_mean_absolute_error: 0.0445\n",
      "Epoch 101/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0475 - mean_absolute_error: 0.0475\n",
      "Epoch 00101: val_loss did not improve from 0.04038\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0475 - mean_absolute_error: 0.0475 - val_loss: 0.0416 - val_mean_absolute_error: 0.0416\n",
      "Epoch 102/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0457 - mean_absolute_error: 0.0457\n",
      "Epoch 00102: val_loss improved from 0.04038 to 0.03877, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.0457 - mean_absolute_error: 0.0457 - val_loss: 0.0388 - val_mean_absolute_error: 0.0388\n",
      "Epoch 103/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0446 - mean_absolute_error: 0.0446\n",
      "Epoch 00103: val_loss did not improve from 0.03877\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0446 - mean_absolute_error: 0.0446 - val_loss: 0.0447 - val_mean_absolute_error: 0.0447\n",
      "Epoch 104/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0464 - mean_absolute_error: 0.0464\n",
      "Epoch 00104: val_loss did not improve from 0.03877\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.0464 - mean_absolute_error: 0.0464 - val_loss: 0.0397 - val_mean_absolute_error: 0.0397\n",
      "Epoch 105/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0428 - mean_absolute_error: 0.0428\n",
      "Epoch 00105: val_loss did not improve from 0.03877\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.0428 - mean_absolute_error: 0.0428 - val_loss: 0.0454 - val_mean_absolute_error: 0.0454\n",
      "Epoch 106/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0479 - mean_absolute_error: 0.0479\n",
      "Epoch 00106: val_loss did not improve from 0.03877\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0479 - mean_absolute_error: 0.0479 - val_loss: 0.0412 - val_mean_absolute_error: 0.0412\n",
      "Epoch 107/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0450 - mean_absolute_error: 0.0450\n",
      "Epoch 00107: val_loss did not improve from 0.03877\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0450 - mean_absolute_error: 0.0450 - val_loss: 0.0443 - val_mean_absolute_error: 0.0443\n",
      "Epoch 108/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0421 - mean_absolute_error: 0.0421\n",
      "Epoch 00108: val_loss did not improve from 0.03877\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0421 - mean_absolute_error: 0.0421 - val_loss: 0.0454 - val_mean_absolute_error: 0.0454\n",
      "Epoch 109/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0447 - mean_absolute_error: 0.0447\n",
      "Epoch 00109: val_loss did not improve from 0.03877\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0447 - mean_absolute_error: 0.0447 - val_loss: 0.0417 - val_mean_absolute_error: 0.0417\n",
      "Epoch 110/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0410 - mean_absolute_error: 0.0410\n",
      "Epoch 00110: val_loss did not improve from 0.03877\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0410 - mean_absolute_error: 0.0410 - val_loss: 0.0448 - val_mean_absolute_error: 0.0448\n",
      "Epoch 111/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0440 - mean_absolute_error: 0.0440\n",
      "Epoch 00111: val_loss did not improve from 0.03877\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0440 - mean_absolute_error: 0.0440 - val_loss: 0.0426 - val_mean_absolute_error: 0.0426\n",
      "Epoch 112/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0422 - mean_absolute_error: 0.0422\n",
      "Epoch 00112: val_loss did not improve from 0.03877\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0422 - mean_absolute_error: 0.0422 - val_loss: 0.0429 - val_mean_absolute_error: 0.0429\n",
      "Epoch 113/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0429 - mean_absolute_error: 0.0429\n",
      "Epoch 00113: val_loss did not improve from 0.03877\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0429 - mean_absolute_error: 0.0429 - val_loss: 0.0454 - val_mean_absolute_error: 0.0454\n",
      "Epoch 114/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0460 - mean_absolute_error: 0.0460\n",
      "Epoch 00114: val_loss did not improve from 0.03877\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0460 - mean_absolute_error: 0.0460 - val_loss: 0.0416 - val_mean_absolute_error: 0.0416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0428 - mean_absolute_error: 0.0428\n",
      "Epoch 00115: val_loss did not improve from 0.03877\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0428 - mean_absolute_error: 0.0428 - val_loss: 0.0443 - val_mean_absolute_error: 0.0443\n",
      "Epoch 116/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0445 - mean_absolute_error: 0.0445\n",
      "Epoch 00116: val_loss did not improve from 0.03877\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0445 - mean_absolute_error: 0.0445 - val_loss: 0.0428 - val_mean_absolute_error: 0.0428\n",
      "Epoch 117/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0421 - mean_absolute_error: 0.0421\n",
      "Epoch 00117: val_loss did not improve from 0.03877\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0421 - mean_absolute_error: 0.0421 - val_loss: 0.0433 - val_mean_absolute_error: 0.0433\n",
      "Epoch 118/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0430 - mean_absolute_error: 0.0430\n",
      "Epoch 00118: val_loss did not improve from 0.03877\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0430 - mean_absolute_error: 0.0430 - val_loss: 0.0433 - val_mean_absolute_error: 0.0433\n",
      "Epoch 119/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0442 - mean_absolute_error: 0.0442\n",
      "Epoch 00119: val_loss did not improve from 0.03877\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0442 - mean_absolute_error: 0.0442 - val_loss: 0.0394 - val_mean_absolute_error: 0.0394\n",
      "Epoch 120/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0416 - mean_absolute_error: 0.0416\n",
      "Epoch 00120: val_loss improved from 0.03877 to 0.03857, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0416 - mean_absolute_error: 0.0416 - val_loss: 0.0386 - val_mean_absolute_error: 0.0386\n",
      "Epoch 121/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0407 - mean_absolute_error: 0.0407\n",
      "Epoch 00121: val_loss did not improve from 0.03857\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0407 - mean_absolute_error: 0.0407 - val_loss: 0.0389 - val_mean_absolute_error: 0.0389\n",
      "Epoch 122/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0424 - mean_absolute_error: 0.0424\n",
      "Epoch 00122: val_loss improved from 0.03857 to 0.03833, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0424 - mean_absolute_error: 0.0424 - val_loss: 0.0383 - val_mean_absolute_error: 0.0383\n",
      "Epoch 123/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0399 - mean_absolute_error: 0.0399\n",
      "Epoch 00123: val_loss did not improve from 0.03833\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0399 - mean_absolute_error: 0.0399 - val_loss: 0.0418 - val_mean_absolute_error: 0.0418\n",
      "Epoch 124/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0435 - mean_absolute_error: 0.0435\n",
      "Epoch 00124: val_loss did not improve from 0.03833\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0435 - mean_absolute_error: 0.0435 - val_loss: 0.0404 - val_mean_absolute_error: 0.0404\n",
      "Epoch 125/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0420 - mean_absolute_error: 0.0420\n",
      "Epoch 00125: val_loss improved from 0.03833 to 0.03720, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0420 - mean_absolute_error: 0.0420 - val_loss: 0.0372 - val_mean_absolute_error: 0.0372\n",
      "Epoch 126/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0424 - mean_absolute_error: 0.0424\n",
      "Epoch 00126: val_loss did not improve from 0.03720\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0424 - mean_absolute_error: 0.0424 - val_loss: 0.0397 - val_mean_absolute_error: 0.0397\n",
      "Epoch 127/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0439 - mean_absolute_error: 0.0439\n",
      "Epoch 00127: val_loss improved from 0.03720 to 0.03680, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.0439 - mean_absolute_error: 0.0439 - val_loss: 0.0368 - val_mean_absolute_error: 0.0368\n",
      "Epoch 128/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0436 - mean_absolute_error: 0.0436\n",
      "Epoch 00128: val_loss improved from 0.03680 to 0.03646, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0436 - mean_absolute_error: 0.0436 - val_loss: 0.0365 - val_mean_absolute_error: 0.0365\n",
      "Epoch 129/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0417 - mean_absolute_error: 0.0417\n",
      "Epoch 00129: val_loss did not improve from 0.03646\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0417 - mean_absolute_error: 0.0417 - val_loss: 0.0380 - val_mean_absolute_error: 0.0380\n",
      "Epoch 130/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0408 - mean_absolute_error: 0.0408\n",
      "Epoch 00130: val_loss did not improve from 0.03646\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0408 - mean_absolute_error: 0.0408 - val_loss: 0.0393 - val_mean_absolute_error: 0.0393\n",
      "Epoch 131/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0423 - mean_absolute_error: 0.0423\n",
      "Epoch 00131: val_loss did not improve from 0.03646\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0423 - mean_absolute_error: 0.0423 - val_loss: 0.0414 - val_mean_absolute_error: 0.0414\n",
      "Epoch 132/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0417 - mean_absolute_error: 0.0417\n",
      "Epoch 00132: val_loss did not improve from 0.03646\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0417 - mean_absolute_error: 0.0417 - val_loss: 0.0404 - val_mean_absolute_error: 0.0404\n",
      "Epoch 133/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0409 - mean_absolute_error: 0.0409\n",
      "Epoch 00133: val_loss did not improve from 0.03646\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0409 - mean_absolute_error: 0.0409 - val_loss: 0.0380 - val_mean_absolute_error: 0.0380\n",
      "Epoch 134/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0392 - mean_absolute_error: 0.0392\n",
      "Epoch 00134: val_loss improved from 0.03646 to 0.03517, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0392 - mean_absolute_error: 0.0392 - val_loss: 0.0352 - val_mean_absolute_error: 0.0352\n",
      "Epoch 135/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0408 - mean_absolute_error: 0.0408\n",
      "Epoch 00135: val_loss improved from 0.03517 to 0.03488, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0408 - mean_absolute_error: 0.0408 - val_loss: 0.0349 - val_mean_absolute_error: 0.0349\n",
      "Epoch 136/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0414 - mean_absolute_error: 0.0414\n",
      "Epoch 00136: val_loss did not improve from 0.03488\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0414 - mean_absolute_error: 0.0414 - val_loss: 0.0354 - val_mean_absolute_error: 0.0354\n",
      "Epoch 137/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0407 - mean_absolute_error: 0.0407\n",
      "Epoch 00137: val_loss did not improve from 0.03488\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0407 - mean_absolute_error: 0.0407 - val_loss: 0.0367 - val_mean_absolute_error: 0.0367\n",
      "Epoch 138/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0391 - mean_absolute_error: 0.0391\n",
      "Epoch 00138: val_loss did not improve from 0.03488\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0391 - mean_absolute_error: 0.0391 - val_loss: 0.0395 - val_mean_absolute_error: 0.0395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0395 - mean_absolute_error: 0.0395\n",
      "Epoch 00139: val_loss did not improve from 0.03488\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0395 - mean_absolute_error: 0.0395 - val_loss: 0.0398 - val_mean_absolute_error: 0.0398\n",
      "Epoch 140/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0417 - mean_absolute_error: 0.0417\n",
      "Epoch 00140: val_loss did not improve from 0.03488\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0417 - mean_absolute_error: 0.0417 - val_loss: 0.0382 - val_mean_absolute_error: 0.0382\n",
      "Epoch 141/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0413 - mean_absolute_error: 0.0413\n",
      "Epoch 00141: val_loss did not improve from 0.03488\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0413 - mean_absolute_error: 0.0413 - val_loss: 0.0366 - val_mean_absolute_error: 0.0366\n",
      "Epoch 142/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0392 - mean_absolute_error: 0.0392\n",
      "Epoch 00142: val_loss improved from 0.03488 to 0.03487, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0392 - mean_absolute_error: 0.0392 - val_loss: 0.0349 - val_mean_absolute_error: 0.0349\n",
      "Epoch 143/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0397 - mean_absolute_error: 0.0397\n",
      "Epoch 00143: val_loss improved from 0.03487 to 0.03372, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0397 - mean_absolute_error: 0.0397 - val_loss: 0.0337 - val_mean_absolute_error: 0.0337\n",
      "Epoch 144/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0388 - mean_absolute_error: 0.0388\n",
      "Epoch 00144: val_loss did not improve from 0.03372\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0388 - mean_absolute_error: 0.0388 - val_loss: 0.0337 - val_mean_absolute_error: 0.0337\n",
      "Epoch 145/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0382 - mean_absolute_error: 0.0382\n",
      "Epoch 00145: val_loss did not improve from 0.03372\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0382 - mean_absolute_error: 0.0382 - val_loss: 0.0352 - val_mean_absolute_error: 0.0352\n",
      "Epoch 146/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0374 - mean_absolute_error: 0.0374\n",
      "Epoch 00146: val_loss did not improve from 0.03372\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0374 - mean_absolute_error: 0.0374 - val_loss: 0.0379 - val_mean_absolute_error: 0.0379\n",
      "Epoch 147/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0398 - mean_absolute_error: 0.0398\n",
      "Epoch 00147: val_loss did not improve from 0.03372\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0398 - mean_absolute_error: 0.0398 - val_loss: 0.0386 - val_mean_absolute_error: 0.0386\n",
      "Epoch 148/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0390 - mean_absolute_error: 0.0390\n",
      "Epoch 00148: val_loss did not improve from 0.03372\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0390 - mean_absolute_error: 0.0390 - val_loss: 0.0364 - val_mean_absolute_error: 0.0364\n",
      "Epoch 149/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0396 - mean_absolute_error: 0.0396\n",
      "Epoch 00149: val_loss did not improve from 0.03372\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0396 - mean_absolute_error: 0.0396 - val_loss: 0.0352 - val_mean_absolute_error: 0.0352\n",
      "Epoch 150/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0403 - mean_absolute_error: 0.0403\n",
      "Epoch 00150: val_loss did not improve from 0.03372\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0403 - mean_absolute_error: 0.0403 - val_loss: 0.0347 - val_mean_absolute_error: 0.0347\n",
      "Epoch 151/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0389 - mean_absolute_error: 0.0389\n",
      "Epoch 00151: val_loss did not improve from 0.03372\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0389 - mean_absolute_error: 0.0389 - val_loss: 0.0340 - val_mean_absolute_error: 0.0340\n",
      "Epoch 152/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0400 - mean_absolute_error: 0.0400\n",
      "Epoch 00152: val_loss did not improve from 0.03372\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0400 - mean_absolute_error: 0.0400 - val_loss: 0.0352 - val_mean_absolute_error: 0.0352\n",
      "Epoch 153/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0401 - mean_absolute_error: 0.0401\n",
      "Epoch 00153: val_loss did not improve from 0.03372\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0401 - mean_absolute_error: 0.0401 - val_loss: 0.0359 - val_mean_absolute_error: 0.0359\n",
      "Epoch 154/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0380 - mean_absolute_error: 0.0380\n",
      "Epoch 00154: val_loss did not improve from 0.03372\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0380 - mean_absolute_error: 0.0380 - val_loss: 0.0389 - val_mean_absolute_error: 0.0389\n",
      "Epoch 155/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0367 - mean_absolute_error: 0.0367\n",
      "Epoch 00155: val_loss did not improve from 0.03372\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0367 - mean_absolute_error: 0.0367 - val_loss: 0.0372 - val_mean_absolute_error: 0.0372\n",
      "Epoch 156/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0405 - mean_absolute_error: 0.0405\n",
      "Epoch 00156: val_loss did not improve from 0.03372\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0405 - mean_absolute_error: 0.0405 - val_loss: 0.0357 - val_mean_absolute_error: 0.0357\n",
      "Epoch 157/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0396 - mean_absolute_error: 0.0396\n",
      "Epoch 00157: val_loss did not improve from 0.03372\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0396 - mean_absolute_error: 0.0396 - val_loss: 0.0346 - val_mean_absolute_error: 0.0346\n",
      "Epoch 158/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0402 - mean_absolute_error: 0.0402\n",
      "Epoch 00158: val_loss did not improve from 0.03372\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0402 - mean_absolute_error: 0.0402 - val_loss: 0.0350 - val_mean_absolute_error: 0.0350\n",
      "Epoch 159/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0372 - mean_absolute_error: 0.0372\n",
      "Epoch 00159: val_loss did not improve from 0.03372\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0372 - mean_absolute_error: 0.0372 - val_loss: 0.0348 - val_mean_absolute_error: 0.0348\n",
      "Epoch 160/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0403 - mean_absolute_error: 0.0403\n",
      "Epoch 00160: val_loss improved from 0.03372 to 0.03292, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0403 - mean_absolute_error: 0.0403 - val_loss: 0.0329 - val_mean_absolute_error: 0.0329\n",
      "Epoch 161/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0383 - mean_absolute_error: 0.0383\n",
      "Epoch 00161: val_loss did not improve from 0.03292\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0383 - mean_absolute_error: 0.0383 - val_loss: 0.0338 - val_mean_absolute_error: 0.0338\n",
      "Epoch 162/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0399 - mean_absolute_error: 0.0399\n",
      "Epoch 00162: val_loss did not improve from 0.03292\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0399 - mean_absolute_error: 0.0399 - val_loss: 0.0341 - val_mean_absolute_error: 0.0341\n",
      "Epoch 163/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0384 - mean_absolute_error: 0.0384\n",
      "Epoch 00163: val_loss did not improve from 0.03292\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0384 - mean_absolute_error: 0.0384 - val_loss: 0.0375 - val_mean_absolute_error: 0.0375\n",
      "Epoch 164/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - ETA: 0s - loss: 0.0384 - mean_absolute_error: 0.0384\n",
      "Epoch 00164: val_loss did not improve from 0.03292\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0384 - mean_absolute_error: 0.0384 - val_loss: 0.0364 - val_mean_absolute_error: 0.0364\n",
      "Epoch 165/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0377 - mean_absolute_error: 0.0377\n",
      "Epoch 00165: val_loss did not improve from 0.03292\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0377 - mean_absolute_error: 0.0377 - val_loss: 0.0353 - val_mean_absolute_error: 0.0353\n",
      "Epoch 166/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0387 - mean_absolute_error: 0.0387\n",
      "Epoch 00166: val_loss did not improve from 0.03292\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.0387 - mean_absolute_error: 0.0387 - val_loss: 0.0332 - val_mean_absolute_error: 0.0332\n",
      "Epoch 167/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0367 - mean_absolute_error: 0.0367\n",
      "Epoch 00167: val_loss did not improve from 0.03292\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0367 - mean_absolute_error: 0.0367 - val_loss: 0.0331 - val_mean_absolute_error: 0.0331\n",
      "Epoch 168/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0387 - mean_absolute_error: 0.0387\n",
      "Epoch 00168: val_loss did not improve from 0.03292\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0387 - mean_absolute_error: 0.0387 - val_loss: 0.0349 - val_mean_absolute_error: 0.0349\n",
      "Epoch 169/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0385 - mean_absolute_error: 0.0385\n",
      "Epoch 00169: val_loss did not improve from 0.03292\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0385 - mean_absolute_error: 0.0385 - val_loss: 0.0353 - val_mean_absolute_error: 0.0353\n",
      "Epoch 170/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0375 - mean_absolute_error: 0.0375\n",
      "Epoch 00170: val_loss did not improve from 0.03292\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0375 - mean_absolute_error: 0.0375 - val_loss: 0.0355 - val_mean_absolute_error: 0.0355\n",
      "Epoch 171/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0371 - mean_absolute_error: 0.0371\n",
      "Epoch 00171: val_loss did not improve from 0.03292\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0371 - mean_absolute_error: 0.0371 - val_loss: 0.0351 - val_mean_absolute_error: 0.0351\n",
      "Epoch 172/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0377 - mean_absolute_error: 0.0377\n",
      "Epoch 00172: val_loss did not improve from 0.03292\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0377 - mean_absolute_error: 0.0377 - val_loss: 0.0362 - val_mean_absolute_error: 0.0362\n",
      "Epoch 173/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0396 - mean_absolute_error: 0.0396\n",
      "Epoch 00173: val_loss did not improve from 0.03292\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0396 - mean_absolute_error: 0.0396 - val_loss: 0.0366 - val_mean_absolute_error: 0.0366\n",
      "Epoch 174/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0389 - mean_absolute_error: 0.0389\n",
      "Epoch 00174: val_loss did not improve from 0.03292\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0389 - mean_absolute_error: 0.0389 - val_loss: 0.0354 - val_mean_absolute_error: 0.0354\n",
      "Epoch 175/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0371 - mean_absolute_error: 0.0371\n",
      "Epoch 00175: val_loss did not improve from 0.03292\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0371 - mean_absolute_error: 0.0371 - val_loss: 0.0355 - val_mean_absolute_error: 0.0355\n",
      "Epoch 176/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0403 - mean_absolute_error: 0.0403\n",
      "Epoch 00176: val_loss did not improve from 0.03292\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0403 - mean_absolute_error: 0.0403 - val_loss: 0.0361 - val_mean_absolute_error: 0.0361\n",
      "Epoch 177/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0385 - mean_absolute_error: 0.0385\n",
      "Epoch 00177: val_loss did not improve from 0.03292\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0385 - mean_absolute_error: 0.0385 - val_loss: 0.0344 - val_mean_absolute_error: 0.0344\n",
      "Epoch 178/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0392 - mean_absolute_error: 0.0392\n",
      "Epoch 00178: val_loss improved from 0.03292 to 0.03269, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0392 - mean_absolute_error: 0.0392 - val_loss: 0.0327 - val_mean_absolute_error: 0.0327\n",
      "Epoch 179/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0376 - mean_absolute_error: 0.0376\n",
      "Epoch 00179: val_loss did not improve from 0.03269\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0376 - mean_absolute_error: 0.0376 - val_loss: 0.0333 - val_mean_absolute_error: 0.0333\n",
      "Epoch 180/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0393 - mean_absolute_error: 0.0393\n",
      "Epoch 00180: val_loss did not improve from 0.03269\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0393 - mean_absolute_error: 0.0393 - val_loss: 0.0345 - val_mean_absolute_error: 0.0345\n",
      "Epoch 181/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0368 - mean_absolute_error: 0.0368\n",
      "Epoch 00181: val_loss did not improve from 0.03269\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0368 - mean_absolute_error: 0.0368 - val_loss: 0.0365 - val_mean_absolute_error: 0.0365\n",
      "Epoch 182/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0378 - mean_absolute_error: 0.0378\n",
      "Epoch 00182: val_loss did not improve from 0.03269\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0378 - mean_absolute_error: 0.0378 - val_loss: 0.0334 - val_mean_absolute_error: 0.0334\n",
      "Epoch 183/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0384 - mean_absolute_error: 0.0384\n",
      "Epoch 00183: val_loss improved from 0.03269 to 0.03248, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0384 - mean_absolute_error: 0.0384 - val_loss: 0.0325 - val_mean_absolute_error: 0.0325\n",
      "Epoch 184/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0371 - mean_absolute_error: 0.0371\n",
      "Epoch 00184: val_loss improved from 0.03248 to 0.03055, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0371 - mean_absolute_error: 0.0371 - val_loss: 0.0305 - val_mean_absolute_error: 0.0305\n",
      "Epoch 185/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0384 - mean_absolute_error: 0.0384\n",
      "Epoch 00185: val_loss did not improve from 0.03055\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0384 - mean_absolute_error: 0.0384 - val_loss: 0.0316 - val_mean_absolute_error: 0.0316\n",
      "Epoch 186/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0380 - mean_absolute_error: 0.0380\n",
      "Epoch 00186: val_loss did not improve from 0.03055\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0380 - mean_absolute_error: 0.0380 - val_loss: 0.0318 - val_mean_absolute_error: 0.0318\n",
      "Epoch 187/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0369 - mean_absolute_error: 0.0369\n",
      "Epoch 00187: val_loss did not improve from 0.03055\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0369 - mean_absolute_error: 0.0369 - val_loss: 0.0347 - val_mean_absolute_error: 0.0347\n",
      "Epoch 188/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0395 - mean_absolute_error: 0.0395\n",
      "Epoch 00188: val_loss did not improve from 0.03055\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0395 - mean_absolute_error: 0.0395 - val_loss: 0.0341 - val_mean_absolute_error: 0.0341\n",
      "Epoch 189/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - ETA: 0s - loss: 0.0390 - mean_absolute_error: 0.0390\n",
      "Epoch 00189: val_loss did not improve from 0.03055\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0390 - mean_absolute_error: 0.0390 - val_loss: 0.0352 - val_mean_absolute_error: 0.0352\n",
      "Epoch 190/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0395 - mean_absolute_error: 0.0395\n",
      "Epoch 00190: val_loss did not improve from 0.03055\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0395 - mean_absolute_error: 0.0395 - val_loss: 0.0330 - val_mean_absolute_error: 0.0330\n",
      "Epoch 191/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0374 - mean_absolute_error: 0.0374\n",
      "Epoch 00191: val_loss did not improve from 0.03055\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0374 - mean_absolute_error: 0.0374 - val_loss: 0.0316 - val_mean_absolute_error: 0.0316\n",
      "Epoch 192/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0382 - mean_absolute_error: 0.0382\n",
      "Epoch 00192: val_loss did not improve from 0.03055\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0382 - mean_absolute_error: 0.0382 - val_loss: 0.0328 - val_mean_absolute_error: 0.0328\n",
      "Epoch 193/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0406 - mean_absolute_error: 0.0406\n",
      "Epoch 00193: val_loss did not improve from 0.03055\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0406 - mean_absolute_error: 0.0406 - val_loss: 0.0325 - val_mean_absolute_error: 0.0325\n",
      "Epoch 194/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0361 - mean_absolute_error: 0.0361\n",
      "Epoch 00194: val_loss did not improve from 0.03055\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0361 - mean_absolute_error: 0.0361 - val_loss: 0.0350 - val_mean_absolute_error: 0.0350\n",
      "Epoch 195/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0374 - mean_absolute_error: 0.0374\n",
      "Epoch 00195: val_loss did not improve from 0.03055\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0374 - mean_absolute_error: 0.0374 - val_loss: 0.0332 - val_mean_absolute_error: 0.0332\n",
      "Epoch 196/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0375 - mean_absolute_error: 0.0375\n",
      "Epoch 00196: val_loss did not improve from 0.03055\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0375 - mean_absolute_error: 0.0375 - val_loss: 0.0340 - val_mean_absolute_error: 0.0340\n",
      "Epoch 197/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0393 - mean_absolute_error: 0.0393\n",
      "Epoch 00197: val_loss did not improve from 0.03055\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0393 - mean_absolute_error: 0.0393 - val_loss: 0.0314 - val_mean_absolute_error: 0.0314\n",
      "Epoch 198/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0371 - mean_absolute_error: 0.0371\n",
      "Epoch 00198: val_loss did not improve from 0.03055\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0371 - mean_absolute_error: 0.0371 - val_loss: 0.0328 - val_mean_absolute_error: 0.0328\n",
      "Epoch 199/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0392 - mean_absolute_error: 0.0392\n",
      "Epoch 00199: val_loss did not improve from 0.03055\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.0392 - mean_absolute_error: 0.0392 - val_loss: 0.0314 - val_mean_absolute_error: 0.0314\n",
      "Epoch 200/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0378 - mean_absolute_error: 0.0378\n",
      "Epoch 00200: val_loss did not improve from 0.03055\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0378 - mean_absolute_error: 0.0378 - val_loss: 0.0330 - val_mean_absolute_error: 0.0330\n",
      "Epoch 201/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0384 - mean_absolute_error: 0.0384\n",
      "Epoch 00201: val_loss did not improve from 0.03055\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0384 - mean_absolute_error: 0.0384 - val_loss: 0.0323 - val_mean_absolute_error: 0.0323\n",
      "Epoch 202/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0361 - mean_absolute_error: 0.0361\n",
      "Epoch 00202: val_loss did not improve from 0.03055\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0361 - mean_absolute_error: 0.0361 - val_loss: 0.0338 - val_mean_absolute_error: 0.0338\n",
      "Epoch 203/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0359 - mean_absolute_error: 0.0359\n",
      "Epoch 00203: val_loss did not improve from 0.03055\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0359 - mean_absolute_error: 0.0359 - val_loss: 0.0328 - val_mean_absolute_error: 0.0328\n",
      "Epoch 204/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0375 - mean_absolute_error: 0.0375\n",
      "Epoch 00204: val_loss did not improve from 0.03055\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0375 - mean_absolute_error: 0.0375 - val_loss: 0.0322 - val_mean_absolute_error: 0.0322\n",
      "Epoch 205/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0358 - mean_absolute_error: 0.0358\n",
      "Epoch 00205: val_loss did not improve from 0.03055\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0358 - mean_absolute_error: 0.0358 - val_loss: 0.0310 - val_mean_absolute_error: 0.0310\n",
      "Epoch 206/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0367 - mean_absolute_error: 0.0367\n",
      "Epoch 00206: val_loss did not improve from 0.03055\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0367 - mean_absolute_error: 0.0367 - val_loss: 0.0317 - val_mean_absolute_error: 0.0317\n",
      "Epoch 207/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0356 - mean_absolute_error: 0.0356\n",
      "Epoch 00207: val_loss did not improve from 0.03055\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0356 - mean_absolute_error: 0.0356 - val_loss: 0.0329 - val_mean_absolute_error: 0.0329\n",
      "Epoch 208/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0367 - mean_absolute_error: 0.0367\n",
      "Epoch 00208: val_loss did not improve from 0.03055\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0367 - mean_absolute_error: 0.0367 - val_loss: 0.0324 - val_mean_absolute_error: 0.0324\n",
      "Epoch 209/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0352 - mean_absolute_error: 0.0352\n",
      "Epoch 00209: val_loss did not improve from 0.03055\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0352 - mean_absolute_error: 0.0352 - val_loss: 0.0321 - val_mean_absolute_error: 0.0321\n",
      "Epoch 210/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0368 - mean_absolute_error: 0.0368\n",
      "Epoch 00210: val_loss did not improve from 0.03055\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0368 - mean_absolute_error: 0.0368 - val_loss: 0.0316 - val_mean_absolute_error: 0.0316\n",
      "Epoch 211/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0372 - mean_absolute_error: 0.0372\n",
      "Epoch 00211: val_loss did not improve from 0.03055\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0372 - mean_absolute_error: 0.0372 - val_loss: 0.0310 - val_mean_absolute_error: 0.0310\n",
      "Epoch 212/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0376 - mean_absolute_error: 0.0376\n",
      "Epoch 00212: val_loss did not improve from 0.03055\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0376 - mean_absolute_error: 0.0376 - val_loss: 0.0316 - val_mean_absolute_error: 0.0316\n",
      "Epoch 213/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0352 - mean_absolute_error: 0.0352\n",
      "Epoch 00213: val_loss did not improve from 0.03055\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0352 - mean_absolute_error: 0.0352 - val_loss: 0.0323 - val_mean_absolute_error: 0.0323\n",
      "Epoch 214/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0359 - mean_absolute_error: 0.0359\n",
      "Epoch 00214: val_loss did not improve from 0.03055\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0359 - mean_absolute_error: 0.0359 - val_loss: 0.0326 - val_mean_absolute_error: 0.0326\n",
      "Epoch 215/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0361 - mean_absolute_error: 0.0361\n",
      "Epoch 00215: val_loss did not improve from 0.03055\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0361 - mean_absolute_error: 0.0361 - val_loss: 0.0321 - val_mean_absolute_error: 0.0321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 216/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0354 - mean_absolute_error: 0.0354\n",
      "Epoch 00216: val_loss did not improve from 0.03055\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0354 - mean_absolute_error: 0.0354 - val_loss: 0.0306 - val_mean_absolute_error: 0.0306\n",
      "Epoch 217/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0365 - mean_absolute_error: 0.0365\n",
      "Epoch 00217: val_loss improved from 0.03055 to 0.02950, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0365 - mean_absolute_error: 0.0365 - val_loss: 0.0295 - val_mean_absolute_error: 0.0295\n",
      "Epoch 218/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0378 - mean_absolute_error: 0.0378\n",
      "Epoch 00218: val_loss did not improve from 0.02950\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0378 - mean_absolute_error: 0.0378 - val_loss: 0.0299 - val_mean_absolute_error: 0.0299\n",
      "Epoch 219/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0358 - mean_absolute_error: 0.0358\n",
      "Epoch 00219: val_loss did not improve from 0.02950\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0358 - mean_absolute_error: 0.0358 - val_loss: 0.0317 - val_mean_absolute_error: 0.0317\n",
      "Epoch 220/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0370 - mean_absolute_error: 0.0370\n",
      "Epoch 00220: val_loss did not improve from 0.02950\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0370 - mean_absolute_error: 0.0370 - val_loss: 0.0337 - val_mean_absolute_error: 0.0337\n",
      "Epoch 221/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0376 - mean_absolute_error: 0.0376\n",
      "Epoch 00221: val_loss did not improve from 0.02950\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0376 - mean_absolute_error: 0.0376 - val_loss: 0.0309 - val_mean_absolute_error: 0.0309\n",
      "Epoch 222/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0364 - mean_absolute_error: 0.0364\n",
      "Epoch 00222: val_loss did not improve from 0.02950\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0364 - mean_absolute_error: 0.0364 - val_loss: 0.0320 - val_mean_absolute_error: 0.0320\n",
      "Epoch 223/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0372 - mean_absolute_error: 0.0372\n",
      "Epoch 00223: val_loss did not improve from 0.02950\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0372 - mean_absolute_error: 0.0372 - val_loss: 0.0302 - val_mean_absolute_error: 0.0302\n",
      "Epoch 224/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0356 - mean_absolute_error: 0.0356\n",
      "Epoch 00224: val_loss did not improve from 0.02950\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0356 - mean_absolute_error: 0.0356 - val_loss: 0.0332 - val_mean_absolute_error: 0.0332\n",
      "Epoch 225/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0376 - mean_absolute_error: 0.0376\n",
      "Epoch 00225: val_loss did not improve from 0.02950\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0376 - mean_absolute_error: 0.0376 - val_loss: 0.0316 - val_mean_absolute_error: 0.0316\n",
      "Epoch 226/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0355 - mean_absolute_error: 0.0355\n",
      "Epoch 00226: val_loss did not improve from 0.02950\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0355 - mean_absolute_error: 0.0355 - val_loss: 0.0330 - val_mean_absolute_error: 0.0330\n",
      "Epoch 227/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0374 - mean_absolute_error: 0.0374\n",
      "Epoch 00227: val_loss did not improve from 0.02950\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0374 - mean_absolute_error: 0.0374 - val_loss: 0.0313 - val_mean_absolute_error: 0.0313\n",
      "Epoch 228/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0345 - mean_absolute_error: 0.0345\n",
      "Epoch 00228: val_loss did not improve from 0.02950\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0345 - mean_absolute_error: 0.0345 - val_loss: 0.0324 - val_mean_absolute_error: 0.0324\n",
      "Epoch 229/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0357 - mean_absolute_error: 0.0357\n",
      "Epoch 00229: val_loss did not improve from 0.02950\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0357 - mean_absolute_error: 0.0357 - val_loss: 0.0323 - val_mean_absolute_error: 0.0323\n",
      "Epoch 230/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0342 - mean_absolute_error: 0.0342\n",
      "Epoch 00230: val_loss did not improve from 0.02950\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0342 - mean_absolute_error: 0.0342 - val_loss: 0.0342 - val_mean_absolute_error: 0.0342\n",
      "Epoch 231/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0365 - mean_absolute_error: 0.0365\n",
      "Epoch 00231: val_loss did not improve from 0.02950\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0365 - mean_absolute_error: 0.0365 - val_loss: 0.0340 - val_mean_absolute_error: 0.0340\n",
      "Epoch 232/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0381 - mean_absolute_error: 0.0381\n",
      "Epoch 00232: val_loss did not improve from 0.02950\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0381 - mean_absolute_error: 0.0381 - val_loss: 0.0318 - val_mean_absolute_error: 0.0318\n",
      "Epoch 233/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0368 - mean_absolute_error: 0.0368\n",
      "Epoch 00233: val_loss improved from 0.02950 to 0.02886, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0368 - mean_absolute_error: 0.0368 - val_loss: 0.0289 - val_mean_absolute_error: 0.0289\n",
      "Epoch 234/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0356 - mean_absolute_error: 0.0356\n",
      "Epoch 00234: val_loss did not improve from 0.02886\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0356 - mean_absolute_error: 0.0356 - val_loss: 0.0307 - val_mean_absolute_error: 0.0307\n",
      "Epoch 235/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0345 - mean_absolute_error: 0.0345\n",
      "Epoch 00235: val_loss did not improve from 0.02886\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.0345 - mean_absolute_error: 0.0345 - val_loss: 0.0302 - val_mean_absolute_error: 0.0302\n",
      "Epoch 236/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0347 - mean_absolute_error: 0.0347\n",
      "Epoch 00236: val_loss did not improve from 0.02886\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0347 - mean_absolute_error: 0.0347 - val_loss: 0.0337 - val_mean_absolute_error: 0.0337\n",
      "Epoch 237/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0369 - mean_absolute_error: 0.0369\n",
      "Epoch 00237: val_loss did not improve from 0.02886\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0369 - mean_absolute_error: 0.0369 - val_loss: 0.0331 - val_mean_absolute_error: 0.0331\n",
      "Epoch 238/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0346 - mean_absolute_error: 0.0346\n",
      "Epoch 00238: val_loss did not improve from 0.02886\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0346 - mean_absolute_error: 0.0346 - val_loss: 0.0314 - val_mean_absolute_error: 0.0314\n",
      "Epoch 239/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0365 - mean_absolute_error: 0.0365\n",
      "Epoch 00239: val_loss did not improve from 0.02886\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0365 - mean_absolute_error: 0.0365 - val_loss: 0.0298 - val_mean_absolute_error: 0.0298\n",
      "Epoch 240/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0372 - mean_absolute_error: 0.0372\n",
      "Epoch 00240: val_loss did not improve from 0.02886\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0372 - mean_absolute_error: 0.0372 - val_loss: 0.0297 - val_mean_absolute_error: 0.0297\n",
      "Epoch 241/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0344 - mean_absolute_error: 0.0344\n",
      "Epoch 00241: val_loss did not improve from 0.02886\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0344 - mean_absolute_error: 0.0344 - val_loss: 0.0317 - val_mean_absolute_error: 0.0317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 242/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0350 - mean_absolute_error: 0.0350\n",
      "Epoch 00242: val_loss did not improve from 0.02886\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0350 - mean_absolute_error: 0.0350 - val_loss: 0.0324 - val_mean_absolute_error: 0.0324\n",
      "Epoch 243/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0364 - mean_absolute_error: 0.0364\n",
      "Epoch 00243: val_loss did not improve from 0.02886\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0364 - mean_absolute_error: 0.0364 - val_loss: 0.0321 - val_mean_absolute_error: 0.0321\n",
      "Epoch 244/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0347 - mean_absolute_error: 0.0347\n",
      "Epoch 00244: val_loss did not improve from 0.02886\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0347 - mean_absolute_error: 0.0347 - val_loss: 0.0318 - val_mean_absolute_error: 0.0318\n",
      "Epoch 245/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0341 - mean_absolute_error: 0.0341\n",
      "Epoch 00245: val_loss did not improve from 0.02886\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0341 - mean_absolute_error: 0.0341 - val_loss: 0.0299 - val_mean_absolute_error: 0.0299\n",
      "Epoch 246/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0357 - mean_absolute_error: 0.0357\n",
      "Epoch 00246: val_loss did not improve from 0.02886\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0357 - mean_absolute_error: 0.0357 - val_loss: 0.0291 - val_mean_absolute_error: 0.0291\n",
      "Epoch 247/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0380 - mean_absolute_error: 0.0380\n",
      "Epoch 00247: val_loss did not improve from 0.02886\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0380 - mean_absolute_error: 0.0380 - val_loss: 0.0289 - val_mean_absolute_error: 0.0289\n",
      "Epoch 248/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0350 - mean_absolute_error: 0.0350\n",
      "Epoch 00248: val_loss did not improve from 0.02886\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0350 - mean_absolute_error: 0.0350 - val_loss: 0.0298 - val_mean_absolute_error: 0.0298\n",
      "Epoch 249/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0337 - mean_absolute_error: 0.0337\n",
      "Epoch 00249: val_loss did not improve from 0.02886\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0337 - mean_absolute_error: 0.0337 - val_loss: 0.0298 - val_mean_absolute_error: 0.0298\n",
      "Epoch 250/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0344 - mean_absolute_error: 0.0344\n",
      "Epoch 00250: val_loss did not improve from 0.02886\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0344 - mean_absolute_error: 0.0344 - val_loss: 0.0293 - val_mean_absolute_error: 0.0293\n",
      "Epoch 251/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0330 - mean_absolute_error: 0.0330\n",
      "Epoch 00251: val_loss did not improve from 0.02886\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0330 - mean_absolute_error: 0.0330 - val_loss: 0.0296 - val_mean_absolute_error: 0.0296\n",
      "Epoch 252/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0351 - mean_absolute_error: 0.0351\n",
      "Epoch 00252: val_loss improved from 0.02886 to 0.02847, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0351 - mean_absolute_error: 0.0351 - val_loss: 0.0285 - val_mean_absolute_error: 0.0285\n",
      "Epoch 253/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0338 - mean_absolute_error: 0.0338\n",
      "Epoch 00253: val_loss did not improve from 0.02847\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0338 - mean_absolute_error: 0.0338 - val_loss: 0.0290 - val_mean_absolute_error: 0.0290\n",
      "Epoch 254/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0345 - mean_absolute_error: 0.0345\n",
      "Epoch 00254: val_loss did not improve from 0.02847\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0345 - mean_absolute_error: 0.0345 - val_loss: 0.0285 - val_mean_absolute_error: 0.0285\n",
      "Epoch 255/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0342 - mean_absolute_error: 0.0342\n",
      "Epoch 00255: val_loss did not improve from 0.02847\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0342 - mean_absolute_error: 0.0342 - val_loss: 0.0286 - val_mean_absolute_error: 0.0286\n",
      "Epoch 256/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0337 - mean_absolute_error: 0.0337\n",
      "Epoch 00256: val_loss did not improve from 0.02847\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0337 - mean_absolute_error: 0.0337 - val_loss: 0.0297 - val_mean_absolute_error: 0.0297\n",
      "Epoch 257/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0353 - mean_absolute_error: 0.0353\n",
      "Epoch 00257: val_loss did not improve from 0.02847\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0353 - mean_absolute_error: 0.0353 - val_loss: 0.0286 - val_mean_absolute_error: 0.0286\n",
      "Epoch 258/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0344 - mean_absolute_error: 0.0344\n",
      "Epoch 00258: val_loss improved from 0.02847 to 0.02835, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0344 - mean_absolute_error: 0.0344 - val_loss: 0.0284 - val_mean_absolute_error: 0.0284\n",
      "Epoch 259/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0362 - mean_absolute_error: 0.0362\n",
      "Epoch 00259: val_loss did not improve from 0.02835\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0362 - mean_absolute_error: 0.0362 - val_loss: 0.0284 - val_mean_absolute_error: 0.0284\n",
      "Epoch 260/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0337 - mean_absolute_error: 0.0337\n",
      "Epoch 00260: val_loss did not improve from 0.02835\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0337 - mean_absolute_error: 0.0337 - val_loss: 0.0302 - val_mean_absolute_error: 0.0302\n",
      "Epoch 261/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0352 - mean_absolute_error: 0.0352\n",
      "Epoch 00261: val_loss did not improve from 0.02835\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0352 - mean_absolute_error: 0.0352 - val_loss: 0.0295 - val_mean_absolute_error: 0.0295\n",
      "Epoch 262/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0338 - mean_absolute_error: 0.0338\n",
      "Epoch 00262: val_loss improved from 0.02835 to 0.02818, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0338 - mean_absolute_error: 0.0338 - val_loss: 0.0282 - val_mean_absolute_error: 0.0282\n",
      "Epoch 263/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0340 - mean_absolute_error: 0.0340\n",
      "Epoch 00263: val_loss improved from 0.02818 to 0.02789, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0340 - mean_absolute_error: 0.0340 - val_loss: 0.0279 - val_mean_absolute_error: 0.0279\n",
      "Epoch 264/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0348 - mean_absolute_error: 0.0348\n",
      "Epoch 00264: val_loss improved from 0.02789 to 0.02760, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0348 - mean_absolute_error: 0.0348 - val_loss: 0.0276 - val_mean_absolute_error: 0.0276\n",
      "Epoch 265/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0344 - mean_absolute_error: 0.0344\n",
      "Epoch 00265: val_loss did not improve from 0.02760\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0344 - mean_absolute_error: 0.0344 - val_loss: 0.0282 - val_mean_absolute_error: 0.0282\n",
      "Epoch 266/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0356 - mean_absolute_error: 0.0356\n",
      "Epoch 00266: val_loss did not improve from 0.02760\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0356 - mean_absolute_error: 0.0356 - val_loss: 0.0278 - val_mean_absolute_error: 0.0278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 267/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0346 - mean_absolute_error: 0.0346\n",
      "Epoch 00267: val_loss did not improve from 0.02760\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0346 - mean_absolute_error: 0.0346 - val_loss: 0.0297 - val_mean_absolute_error: 0.0297\n",
      "Epoch 268/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0342 - mean_absolute_error: 0.0342\n",
      "Epoch 00268: val_loss did not improve from 0.02760\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0342 - mean_absolute_error: 0.0342 - val_loss: 0.0294 - val_mean_absolute_error: 0.0294\n",
      "Epoch 269/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0332 - mean_absolute_error: 0.0332\n",
      "Epoch 00269: val_loss did not improve from 0.02760\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0332 - mean_absolute_error: 0.0332 - val_loss: 0.0294 - val_mean_absolute_error: 0.0294\n",
      "Epoch 270/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0318 - mean_absolute_error: 0.0318\n",
      "Epoch 00270: val_loss did not improve from 0.02760\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0318 - mean_absolute_error: 0.0318 - val_loss: 0.0304 - val_mean_absolute_error: 0.0304\n",
      "Epoch 271/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0339 - mean_absolute_error: 0.0339\n",
      "Epoch 00271: val_loss did not improve from 0.02760\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0339 - mean_absolute_error: 0.0339 - val_loss: 0.0295 - val_mean_absolute_error: 0.0295\n",
      "Epoch 272/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0330 - mean_absolute_error: 0.0330\n",
      "Epoch 00272: val_loss improved from 0.02760 to 0.02742, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.0330 - mean_absolute_error: 0.0330 - val_loss: 0.0274 - val_mean_absolute_error: 0.0274\n",
      "Epoch 273/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0343 - mean_absolute_error: 0.0343\n",
      "Epoch 00273: val_loss did not improve from 0.02742\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0343 - mean_absolute_error: 0.0343 - val_loss: 0.0275 - val_mean_absolute_error: 0.0275\n",
      "Epoch 274/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0344 - mean_absolute_error: 0.0344\n",
      "Epoch 00274: val_loss did not improve from 0.02742\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0344 - mean_absolute_error: 0.0344 - val_loss: 0.0275 - val_mean_absolute_error: 0.0275\n",
      "Epoch 275/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0338 - mean_absolute_error: 0.0338\n",
      "Epoch 00275: val_loss did not improve from 0.02742\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0338 - mean_absolute_error: 0.0338 - val_loss: 0.0283 - val_mean_absolute_error: 0.0283\n",
      "Epoch 276/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0340 - mean_absolute_error: 0.0340\n",
      "Epoch 00276: val_loss improved from 0.02742 to 0.02647, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0340 - mean_absolute_error: 0.0340 - val_loss: 0.0265 - val_mean_absolute_error: 0.0265\n",
      "Epoch 277/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0325 - mean_absolute_error: 0.0325\n",
      "Epoch 00277: val_loss improved from 0.02647 to 0.02518, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0325 - mean_absolute_error: 0.0325 - val_loss: 0.0252 - val_mean_absolute_error: 0.0252\n",
      "Epoch 278/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0346 - mean_absolute_error: 0.0346\n",
      "Epoch 00278: val_loss did not improve from 0.02518\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0346 - mean_absolute_error: 0.0346 - val_loss: 0.0290 - val_mean_absolute_error: 0.0290\n",
      "Epoch 279/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0359 - mean_absolute_error: 0.0359\n",
      "Epoch 00279: val_loss did not improve from 0.02518\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0359 - mean_absolute_error: 0.0359 - val_loss: 0.0304 - val_mean_absolute_error: 0.0304\n",
      "Epoch 280/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0343 - mean_absolute_error: 0.0343\n",
      "Epoch 00280: val_loss did not improve from 0.02518\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.0343 - mean_absolute_error: 0.0343 - val_loss: 0.0267 - val_mean_absolute_error: 0.0267\n",
      "Epoch 281/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0350 - mean_absolute_error: 0.0350\n",
      "Epoch 00281: val_loss did not improve from 0.02518\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0350 - mean_absolute_error: 0.0350 - val_loss: 0.0261 - val_mean_absolute_error: 0.0261\n",
      "Epoch 282/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0352 - mean_absolute_error: 0.0352\n",
      "Epoch 00282: val_loss did not improve from 0.02518\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0352 - mean_absolute_error: 0.0352 - val_loss: 0.0275 - val_mean_absolute_error: 0.0275\n",
      "Epoch 283/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0325 - mean_absolute_error: 0.0325\n",
      "Epoch 00283: val_loss did not improve from 0.02518\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0325 - mean_absolute_error: 0.0325 - val_loss: 0.0307 - val_mean_absolute_error: 0.0307\n",
      "Epoch 284/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0346 - mean_absolute_error: 0.0346\n",
      "Epoch 00284: val_loss did not improve from 0.02518\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0346 - mean_absolute_error: 0.0346 - val_loss: 0.0293 - val_mean_absolute_error: 0.0293\n",
      "Epoch 285/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0337 - mean_absolute_error: 0.0337\n",
      "Epoch 00285: val_loss did not improve from 0.02518\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0337 - mean_absolute_error: 0.0337 - val_loss: 0.0268 - val_mean_absolute_error: 0.0268\n",
      "Epoch 286/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0331 - mean_absolute_error: 0.0331\n",
      "Epoch 00286: val_loss did not improve from 0.02518\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0331 - mean_absolute_error: 0.0331 - val_loss: 0.0268 - val_mean_absolute_error: 0.0268\n",
      "Epoch 287/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0345 - mean_absolute_error: 0.0345\n",
      "Epoch 00287: val_loss did not improve from 0.02518\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0345 - mean_absolute_error: 0.0345 - val_loss: 0.0289 - val_mean_absolute_error: 0.0289\n",
      "Epoch 288/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0323 - mean_absolute_error: 0.0323\n",
      "Epoch 00288: val_loss did not improve from 0.02518\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0323 - mean_absolute_error: 0.0323 - val_loss: 0.0293 - val_mean_absolute_error: 0.0293\n",
      "Epoch 289/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0340 - mean_absolute_error: 0.0340\n",
      "Epoch 00289: val_loss did not improve from 0.02518\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0340 - mean_absolute_error: 0.0340 - val_loss: 0.0271 - val_mean_absolute_error: 0.0271\n",
      "Epoch 290/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0330 - mean_absolute_error: 0.0330\n",
      "Epoch 00290: val_loss did not improve from 0.02518\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0330 - mean_absolute_error: 0.0330 - val_loss: 0.0272 - val_mean_absolute_error: 0.0272\n",
      "Epoch 291/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0349 - mean_absolute_error: 0.0349\n",
      "Epoch 00291: val_loss did not improve from 0.02518\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0349 - mean_absolute_error: 0.0349 - val_loss: 0.0265 - val_mean_absolute_error: 0.0265\n",
      "Epoch 292/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - ETA: 0s - loss: 0.0355 - mean_absolute_error: 0.0355\n",
      "Epoch 00292: val_loss did not improve from 0.02518\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0355 - mean_absolute_error: 0.0355 - val_loss: 0.0274 - val_mean_absolute_error: 0.0274\n",
      "Epoch 293/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0322 - mean_absolute_error: 0.0322\n",
      "Epoch 00293: val_loss did not improve from 0.02518\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.0322 - mean_absolute_error: 0.0322 - val_loss: 0.0275 - val_mean_absolute_error: 0.0275\n",
      "Epoch 294/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0340 - mean_absolute_error: 0.0340\n",
      "Epoch 00294: val_loss did not improve from 0.02518\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0340 - mean_absolute_error: 0.0340 - val_loss: 0.0264 - val_mean_absolute_error: 0.0264\n",
      "Epoch 295/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0327 - mean_absolute_error: 0.0327\n",
      "Epoch 00295: val_loss did not improve from 0.02518\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0327 - mean_absolute_error: 0.0327 - val_loss: 0.0269 - val_mean_absolute_error: 0.0269\n",
      "Epoch 296/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0312 - mean_absolute_error: 0.0312\n",
      "Epoch 00296: val_loss did not improve from 0.02518\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0312 - mean_absolute_error: 0.0312 - val_loss: 0.0321 - val_mean_absolute_error: 0.0321\n",
      "Epoch 297/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0368 - mean_absolute_error: 0.0368\n",
      "Epoch 00297: val_loss did not improve from 0.02518\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0368 - mean_absolute_error: 0.0368 - val_loss: 0.0308 - val_mean_absolute_error: 0.0308\n",
      "Epoch 298/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0326 - mean_absolute_error: 0.0326\n",
      "Epoch 00298: val_loss did not improve from 0.02518\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0326 - mean_absolute_error: 0.0326 - val_loss: 0.0325 - val_mean_absolute_error: 0.0325\n",
      "Epoch 299/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0351 - mean_absolute_error: 0.0351\n",
      "Epoch 00299: val_loss did not improve from 0.02518\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0351 - mean_absolute_error: 0.0351 - val_loss: 0.0295 - val_mean_absolute_error: 0.0295\n",
      "Epoch 300/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0331 - mean_absolute_error: 0.0331\n",
      "Epoch 00300: val_loss did not improve from 0.02518\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0331 - mean_absolute_error: 0.0331 - val_loss: 0.0311 - val_mean_absolute_error: 0.0311\n",
      "Epoch 301/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0359 - mean_absolute_error: 0.0359\n",
      "Epoch 00301: val_loss did not improve from 0.02518\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0359 - mean_absolute_error: 0.0359 - val_loss: 0.0267 - val_mean_absolute_error: 0.0267\n",
      "Epoch 302/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0327 - mean_absolute_error: 0.0327\n",
      "Epoch 00302: val_loss did not improve from 0.02518\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0327 - mean_absolute_error: 0.0327 - val_loss: 0.0332 - val_mean_absolute_error: 0.0332\n",
      "Epoch 303/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0376 - mean_absolute_error: 0.0376\n",
      "Epoch 00303: val_loss did not improve from 0.02518\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0376 - mean_absolute_error: 0.0376 - val_loss: 0.0297 - val_mean_absolute_error: 0.0297\n",
      "Epoch 304/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0332 - mean_absolute_error: 0.0332\n",
      "Epoch 00304: val_loss did not improve from 0.02518\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0332 - mean_absolute_error: 0.0332 - val_loss: 0.0327 - val_mean_absolute_error: 0.0327\n",
      "Epoch 305/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0365 - mean_absolute_error: 0.0365\n",
      "Epoch 00305: val_loss did not improve from 0.02518\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.0365 - mean_absolute_error: 0.0365 - val_loss: 0.0267 - val_mean_absolute_error: 0.0267\n",
      "Epoch 306/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0317 - mean_absolute_error: 0.0317\n",
      "Epoch 00306: val_loss did not improve from 0.02518\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0317 - mean_absolute_error: 0.0317 - val_loss: 0.0340 - val_mean_absolute_error: 0.0340\n",
      "Epoch 307/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0385 - mean_absolute_error: 0.0385\n",
      "Epoch 00307: val_loss did not improve from 0.02518\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0385 - mean_absolute_error: 0.0385 - val_loss: 0.0263 - val_mean_absolute_error: 0.0263\n",
      "Epoch 308/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0340 - mean_absolute_error: 0.0340\n",
      "Epoch 00308: val_loss did not improve from 0.02518\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0340 - mean_absolute_error: 0.0340 - val_loss: 0.0324 - val_mean_absolute_error: 0.0324\n",
      "Epoch 309/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0339 - mean_absolute_error: 0.0339\n",
      "Epoch 00309: val_loss did not improve from 0.02518\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0339 - mean_absolute_error: 0.0339 - val_loss: 0.0351 - val_mean_absolute_error: 0.0351\n",
      "Epoch 310/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0357 - mean_absolute_error: 0.0357\n",
      "Epoch 00310: val_loss did not improve from 0.02518\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0357 - mean_absolute_error: 0.0357 - val_loss: 0.0313 - val_mean_absolute_error: 0.0313\n",
      "Epoch 311/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0324 - mean_absolute_error: 0.0324\n",
      "Epoch 00311: val_loss did not improve from 0.02518\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0324 - mean_absolute_error: 0.0324 - val_loss: 0.0330 - val_mean_absolute_error: 0.0330\n",
      "Epoch 312/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0351 - mean_absolute_error: 0.0351\n",
      "Epoch 00312: val_loss did not improve from 0.02518\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0351 - mean_absolute_error: 0.0351 - val_loss: 0.0280 - val_mean_absolute_error: 0.0280\n",
      "Epoch 313/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0333 - mean_absolute_error: 0.0333\n",
      "Epoch 00313: val_loss did not improve from 0.02518\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0333 - mean_absolute_error: 0.0333 - val_loss: 0.0300 - val_mean_absolute_error: 0.0300\n",
      "Epoch 314/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0358 - mean_absolute_error: 0.0358\n",
      "Epoch 00314: val_loss did not improve from 0.02518\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0358 - mean_absolute_error: 0.0358 - val_loss: 0.0316 - val_mean_absolute_error: 0.0316\n",
      "Epoch 315/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0343 - mean_absolute_error: 0.0343\n",
      "Epoch 00315: val_loss did not improve from 0.02518\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0343 - mean_absolute_error: 0.0343 - val_loss: 0.0345 - val_mean_absolute_error: 0.0345\n",
      "Epoch 316/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0355 - mean_absolute_error: 0.0355\n",
      "Epoch 00316: val_loss did not improve from 0.02518\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.0355 - mean_absolute_error: 0.0355 - val_loss: 0.0326 - val_mean_absolute_error: 0.0326\n",
      "Epoch 317/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0351 - mean_absolute_error: 0.0351\n",
      "Epoch 00317: val_loss did not improve from 0.02518\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0351 - mean_absolute_error: 0.0351 - val_loss: 0.0261 - val_mean_absolute_error: 0.0261\n",
      "Epoch 318/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0353 - mean_absolute_error: 0.0353\n",
      "Epoch 00318: val_loss did not improve from 0.02518\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0353 - mean_absolute_error: 0.0353 - val_loss: 0.0265 - val_mean_absolute_error: 0.0265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 319/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0339 - mean_absolute_error: 0.0339\n",
      "Epoch 00319: val_loss did not improve from 0.02518\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0339 - mean_absolute_error: 0.0339 - val_loss: 0.0263 - val_mean_absolute_error: 0.0263\n",
      "Epoch 320/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0355 - mean_absolute_error: 0.0355\n",
      "Epoch 00320: val_loss did not improve from 0.02518\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0355 - mean_absolute_error: 0.0355 - val_loss: 0.0293 - val_mean_absolute_error: 0.0293\n",
      "Epoch 321/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0333 - mean_absolute_error: 0.0333\n",
      "Epoch 00321: val_loss did not improve from 0.02518\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0333 - mean_absolute_error: 0.0333 - val_loss: 0.0312 - val_mean_absolute_error: 0.0312\n",
      "Epoch 322/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0341 - mean_absolute_error: 0.0341\n",
      "Epoch 00322: val_loss did not improve from 0.02518\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0341 - mean_absolute_error: 0.0341 - val_loss: 0.0280 - val_mean_absolute_error: 0.0280\n",
      "Epoch 323/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0301 - mean_absolute_error: 0.0301\n",
      "Epoch 00323: val_loss did not improve from 0.02518\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.0301 - mean_absolute_error: 0.0301 - val_loss: 0.0263 - val_mean_absolute_error: 0.0263\n",
      "Epoch 324/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0314 - mean_absolute_error: 0.0314\n",
      "Epoch 00324: val_loss did not improve from 0.02518\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0314 - mean_absolute_error: 0.0314 - val_loss: 0.0259 - val_mean_absolute_error: 0.0259\n",
      "Epoch 325/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0344 - mean_absolute_error: 0.0344\n",
      "Epoch 00325: val_loss did not improve from 0.02518\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0344 - mean_absolute_error: 0.0344 - val_loss: 0.0256 - val_mean_absolute_error: 0.0256\n",
      "Epoch 326/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0327 - mean_absolute_error: 0.0327\n",
      "Epoch 00326: val_loss did not improve from 0.02518\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0327 - mean_absolute_error: 0.0327 - val_loss: 0.0281 - val_mean_absolute_error: 0.0281\n",
      "Epoch 327/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0308 - mean_absolute_error: 0.0308\n",
      "Epoch 00327: val_loss did not improve from 0.02518\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0308 - mean_absolute_error: 0.0308 - val_loss: 0.0280 - val_mean_absolute_error: 0.0280\n",
      "Epoch 328/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0316 - mean_absolute_error: 0.0316\n",
      "Epoch 00328: val_loss did not improve from 0.02518\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0316 - mean_absolute_error: 0.0316 - val_loss: 0.0258 - val_mean_absolute_error: 0.0258\n",
      "Epoch 329/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0315 - mean_absolute_error: 0.0315\n",
      "Epoch 00329: val_loss did not improve from 0.02518\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0315 - mean_absolute_error: 0.0315 - val_loss: 0.0255 - val_mean_absolute_error: 0.0255\n",
      "Epoch 330/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0335 - mean_absolute_error: 0.0335\n",
      "Epoch 00330: val_loss did not improve from 0.02518\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0335 - mean_absolute_error: 0.0335 - val_loss: 0.0256 - val_mean_absolute_error: 0.0256\n",
      "Epoch 331/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0309 - mean_absolute_error: 0.0309\n",
      "Epoch 00331: val_loss did not improve from 0.02518\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0309 - mean_absolute_error: 0.0309 - val_loss: 0.0278 - val_mean_absolute_error: 0.0278\n",
      "Epoch 332/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0323 - mean_absolute_error: 0.0323\n",
      "Epoch 00332: val_loss did not improve from 0.02518\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.0323 - mean_absolute_error: 0.0323 - val_loss: 0.0290 - val_mean_absolute_error: 0.0290\n",
      "Epoch 333/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0330 - mean_absolute_error: 0.0330\n",
      "Epoch 00333: val_loss did not improve from 0.02518\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 0.0330 - mean_absolute_error: 0.0330 - val_loss: 0.0278 - val_mean_absolute_error: 0.0278\n",
      "Epoch 334/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0319 - mean_absolute_error: 0.0319\n",
      "Epoch 00334: val_loss improved from 0.02518 to 0.02488, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.0319 - mean_absolute_error: 0.0319 - val_loss: 0.0249 - val_mean_absolute_error: 0.0249\n",
      "Epoch 335/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0332 - mean_absolute_error: 0.0332\n",
      "Epoch 00335: val_loss did not improve from 0.02488\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0332 - mean_absolute_error: 0.0332 - val_loss: 0.0249 - val_mean_absolute_error: 0.0249\n",
      "Epoch 336/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0320 - mean_absolute_error: 0.0320\n",
      "Epoch 00336: val_loss did not improve from 0.02488\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0320 - mean_absolute_error: 0.0320 - val_loss: 0.0250 - val_mean_absolute_error: 0.0250\n",
      "Epoch 337/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0324 - mean_absolute_error: 0.0324\n",
      "Epoch 00337: val_loss did not improve from 0.02488\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0324 - mean_absolute_error: 0.0324 - val_loss: 0.0265 - val_mean_absolute_error: 0.0265\n",
      "Epoch 338/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0319 - mean_absolute_error: 0.0319\n",
      "Epoch 00338: val_loss did not improve from 0.02488\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0319 - mean_absolute_error: 0.0319 - val_loss: 0.0275 - val_mean_absolute_error: 0.0275\n",
      "Epoch 339/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0316 - mean_absolute_error: 0.0316\n",
      "Epoch 00339: val_loss improved from 0.02488 to 0.02479, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0316 - mean_absolute_error: 0.0316 - val_loss: 0.0248 - val_mean_absolute_error: 0.0248\n",
      "Epoch 340/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0306 - mean_absolute_error: 0.0306\n",
      "Epoch 00340: val_loss did not improve from 0.02479\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0306 - mean_absolute_error: 0.0306 - val_loss: 0.0265 - val_mean_absolute_error: 0.0265\n",
      "Epoch 341/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0335 - mean_absolute_error: 0.0335\n",
      "Epoch 00341: val_loss did not improve from 0.02479\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0335 - mean_absolute_error: 0.0335 - val_loss: 0.0259 - val_mean_absolute_error: 0.0259\n",
      "Epoch 342/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0302 - mean_absolute_error: 0.0302\n",
      "Epoch 00342: val_loss did not improve from 0.02479\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.0302 - mean_absolute_error: 0.0302 - val_loss: 0.0308 - val_mean_absolute_error: 0.0308\n",
      "Epoch 343/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0331 - mean_absolute_error: 0.0331\n",
      "Epoch 00343: val_loss did not improve from 0.02479\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.0331 - mean_absolute_error: 0.0331 - val_loss: 0.0300 - val_mean_absolute_error: 0.0300\n",
      "Epoch 344/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0319 - mean_absolute_error: 0.0319\n",
      "Epoch 00344: val_loss did not improve from 0.02479\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.0319 - mean_absolute_error: 0.0319 - val_loss: 0.0252 - val_mean_absolute_error: 0.0252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 345/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0308 - mean_absolute_error: 0.0308\n",
      "Epoch 00345: val_loss did not improve from 0.02479\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0308 - mean_absolute_error: 0.0308 - val_loss: 0.0267 - val_mean_absolute_error: 0.0267\n",
      "Epoch 346/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0321 - mean_absolute_error: 0.0321\n",
      "Epoch 00346: val_loss improved from 0.02479 to 0.02386, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.0321 - mean_absolute_error: 0.0321 - val_loss: 0.0239 - val_mean_absolute_error: 0.0239\n",
      "Epoch 347/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0323 - mean_absolute_error: 0.0323\n",
      "Epoch 00347: val_loss did not improve from 0.02386\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0323 - mean_absolute_error: 0.0323 - val_loss: 0.0260 - val_mean_absolute_error: 0.0260\n",
      "Epoch 348/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0317 - mean_absolute_error: 0.0317\n",
      "Epoch 00348: val_loss did not improve from 0.02386\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0317 - mean_absolute_error: 0.0317 - val_loss: 0.0271 - val_mean_absolute_error: 0.0271\n",
      "Epoch 349/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0308 - mean_absolute_error: 0.0308\n",
      "Epoch 00349: val_loss did not improve from 0.02386\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0308 - mean_absolute_error: 0.0308 - val_loss: 0.0263 - val_mean_absolute_error: 0.0263\n",
      "Epoch 350/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0303 - mean_absolute_error: 0.0303\n",
      "Epoch 00350: val_loss did not improve from 0.02386\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0303 - mean_absolute_error: 0.0303 - val_loss: 0.0239 - val_mean_absolute_error: 0.0239\n",
      "Epoch 351/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0308 - mean_absolute_error: 0.0308\n",
      "Epoch 00351: val_loss did not improve from 0.02386\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0308 - mean_absolute_error: 0.0308 - val_loss: 0.0244 - val_mean_absolute_error: 0.0244\n",
      "Epoch 352/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0325 - mean_absolute_error: 0.0325\n",
      "Epoch 00352: val_loss did not improve from 0.02386\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0325 - mean_absolute_error: 0.0325 - val_loss: 0.0260 - val_mean_absolute_error: 0.0260\n",
      "Epoch 353/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0314 - mean_absolute_error: 0.0314\n",
      "Epoch 00353: val_loss did not improve from 0.02386\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0314 - mean_absolute_error: 0.0314 - val_loss: 0.0297 - val_mean_absolute_error: 0.0297\n",
      "Epoch 354/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0305 - mean_absolute_error: 0.0305\n",
      "Epoch 00354: val_loss did not improve from 0.02386\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0305 - mean_absolute_error: 0.0305 - val_loss: 0.0293 - val_mean_absolute_error: 0.0293\n",
      "Epoch 355/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0322 - mean_absolute_error: 0.0322\n",
      "Epoch 00355: val_loss did not improve from 0.02386\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0322 - mean_absolute_error: 0.0322 - val_loss: 0.0260 - val_mean_absolute_error: 0.0260\n",
      "Epoch 356/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0315 - mean_absolute_error: 0.0315\n",
      "Epoch 00356: val_loss did not improve from 0.02386\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0315 - mean_absolute_error: 0.0315 - val_loss: 0.0268 - val_mean_absolute_error: 0.0268\n",
      "Epoch 357/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0313 - mean_absolute_error: 0.0313\n",
      "Epoch 00357: val_loss did not improve from 0.02386\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0313 - mean_absolute_error: 0.0313 - val_loss: 0.0240 - val_mean_absolute_error: 0.0240\n",
      "Epoch 358/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0299 - mean_absolute_error: 0.0299\n",
      "Epoch 00358: val_loss did not improve from 0.02386\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0299 - mean_absolute_error: 0.0299 - val_loss: 0.0276 - val_mean_absolute_error: 0.0276\n",
      "Epoch 359/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0333 - mean_absolute_error: 0.0333\n",
      "Epoch 00359: val_loss did not improve from 0.02386\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0333 - mean_absolute_error: 0.0333 - val_loss: 0.0276 - val_mean_absolute_error: 0.0276\n",
      "Epoch 360/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0313 - mean_absolute_error: 0.0313\n",
      "Epoch 00360: val_loss did not improve from 0.02386\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0313 - mean_absolute_error: 0.0313 - val_loss: 0.0260 - val_mean_absolute_error: 0.0260\n",
      "Epoch 361/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0297 - mean_absolute_error: 0.0297\n",
      "Epoch 00361: val_loss improved from 0.02386 to 0.02359, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0297 - mean_absolute_error: 0.0297 - val_loss: 0.0236 - val_mean_absolute_error: 0.0236\n",
      "Epoch 362/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0325 - mean_absolute_error: 0.0325\n",
      "Epoch 00362: val_loss did not improve from 0.02359\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.0325 - mean_absolute_error: 0.0325 - val_loss: 0.0243 - val_mean_absolute_error: 0.0243\n",
      "Epoch 363/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0299 - mean_absolute_error: 0.0299\n",
      "Epoch 00363: val_loss did not improve from 0.02359\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0299 - mean_absolute_error: 0.0299 - val_loss: 0.0250 - val_mean_absolute_error: 0.0250\n",
      "Epoch 364/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0306 - mean_absolute_error: 0.0306\n",
      "Epoch 00364: val_loss did not improve from 0.02359\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0306 - mean_absolute_error: 0.0306 - val_loss: 0.0260 - val_mean_absolute_error: 0.0260\n",
      "Epoch 365/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0322 - mean_absolute_error: 0.0322\n",
      "Epoch 00365: val_loss did not improve from 0.02359\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0322 - mean_absolute_error: 0.0322 - val_loss: 0.0265 - val_mean_absolute_error: 0.0265\n",
      "Epoch 366/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0317 - mean_absolute_error: 0.0317\n",
      "Epoch 00366: val_loss did not improve from 0.02359\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0317 - mean_absolute_error: 0.0317 - val_loss: 0.0247 - val_mean_absolute_error: 0.0247\n",
      "Epoch 367/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0309 - mean_absolute_error: 0.0309\n",
      "Epoch 00367: val_loss did not improve from 0.02359\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0309 - mean_absolute_error: 0.0309 - val_loss: 0.0250 - val_mean_absolute_error: 0.0250\n",
      "Epoch 368/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0312 - mean_absolute_error: 0.0312\n",
      "Epoch 00368: val_loss did not improve from 0.02359\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0312 - mean_absolute_error: 0.0312 - val_loss: 0.0245 - val_mean_absolute_error: 0.0245\n",
      "Epoch 369/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0298 - mean_absolute_error: 0.0298\n",
      "Epoch 00369: val_loss did not improve from 0.02359\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0298 - mean_absolute_error: 0.0298 - val_loss: 0.0265 - val_mean_absolute_error: 0.0265\n",
      "Epoch 370/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0299 - mean_absolute_error: 0.0299\n",
      "Epoch 00370: val_loss did not improve from 0.02359\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0299 - mean_absolute_error: 0.0299 - val_loss: 0.0272 - val_mean_absolute_error: 0.0272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 371/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0313 - mean_absolute_error: 0.0313\n",
      "Epoch 00371: val_loss did not improve from 0.02359\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0313 - mean_absolute_error: 0.0313 - val_loss: 0.0284 - val_mean_absolute_error: 0.0284\n",
      "Epoch 372/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0315 - mean_absolute_error: 0.0315\n",
      "Epoch 00372: val_loss did not improve from 0.02359\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0315 - mean_absolute_error: 0.0315 - val_loss: 0.0256 - val_mean_absolute_error: 0.0256\n",
      "Epoch 373/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0307 - mean_absolute_error: 0.0307\n",
      "Epoch 00373: val_loss did not improve from 0.02359\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0307 - mean_absolute_error: 0.0307 - val_loss: 0.0245 - val_mean_absolute_error: 0.0245\n",
      "Epoch 374/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0302 - mean_absolute_error: 0.0302\n",
      "Epoch 00374: val_loss did not improve from 0.02359\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0302 - mean_absolute_error: 0.0302 - val_loss: 0.0247 - val_mean_absolute_error: 0.0247\n",
      "Epoch 375/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0306 - mean_absolute_error: 0.0306\n",
      "Epoch 00375: val_loss did not improve from 0.02359\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0306 - mean_absolute_error: 0.0306 - val_loss: 0.0255 - val_mean_absolute_error: 0.0255\n",
      "Epoch 376/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0305 - mean_absolute_error: 0.0305\n",
      "Epoch 00376: val_loss did not improve from 0.02359\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0305 - mean_absolute_error: 0.0305 - val_loss: 0.0243 - val_mean_absolute_error: 0.0243\n",
      "Epoch 377/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0295 - mean_absolute_error: 0.0295\n",
      "Epoch 00377: val_loss improved from 0.02359 to 0.02333, saving model to results\\2022-03_EPAM-sh-1-sc-1-sbd-0-custom_loss-adam-LSTM-seq-50-step-20-layers-4-units-32-activation-sigmoid-b.h5\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.0295 - mean_absolute_error: 0.0295 - val_loss: 0.0233 - val_mean_absolute_error: 0.0233\n",
      "Epoch 378/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0321 - mean_absolute_error: 0.0321\n",
      "Epoch 00378: val_loss did not improve from 0.02333\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0321 - mean_absolute_error: 0.0321 - val_loss: 0.0238 - val_mean_absolute_error: 0.0238\n",
      "Epoch 379/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0309 - mean_absolute_error: 0.0309\n",
      "Epoch 00379: val_loss did not improve from 0.02333\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0309 - mean_absolute_error: 0.0309 - val_loss: 0.0250 - val_mean_absolute_error: 0.0250\n",
      "Epoch 380/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0305 - mean_absolute_error: 0.0305\n",
      "Epoch 00380: val_loss did not improve from 0.02333\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0305 - mean_absolute_error: 0.0305 - val_loss: 0.0275 - val_mean_absolute_error: 0.0275\n",
      "Epoch 381/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0309 - mean_absolute_error: 0.0309\n",
      "Epoch 00381: val_loss did not improve from 0.02333\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0309 - mean_absolute_error: 0.0309 - val_loss: 0.0255 - val_mean_absolute_error: 0.0255\n",
      "Epoch 382/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0304 - mean_absolute_error: 0.0304\n",
      "Epoch 00382: val_loss did not improve from 0.02333\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0304 - mean_absolute_error: 0.0304 - val_loss: 0.0247 - val_mean_absolute_error: 0.0247\n",
      "Epoch 383/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0299 - mean_absolute_error: 0.0299\n",
      "Epoch 00383: val_loss did not improve from 0.02333\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0299 - mean_absolute_error: 0.0299 - val_loss: 0.0241 - val_mean_absolute_error: 0.0241\n",
      "Epoch 384/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0295 - mean_absolute_error: 0.0295\n",
      "Epoch 00384: val_loss did not improve from 0.02333\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0295 - mean_absolute_error: 0.0295 - val_loss: 0.0260 - val_mean_absolute_error: 0.0260\n",
      "Epoch 385/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0313 - mean_absolute_error: 0.0313\n",
      "Epoch 00385: val_loss did not improve from 0.02333\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0313 - mean_absolute_error: 0.0313 - val_loss: 0.0249 - val_mean_absolute_error: 0.0249\n",
      "Epoch 386/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0311 - mean_absolute_error: 0.0311\n",
      "Epoch 00386: val_loss did not improve from 0.02333\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0311 - mean_absolute_error: 0.0311 - val_loss: 0.0250 - val_mean_absolute_error: 0.0250\n",
      "Epoch 387/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0298 - mean_absolute_error: 0.0298\n",
      "Epoch 00387: val_loss did not improve from 0.02333\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0298 - mean_absolute_error: 0.0298 - val_loss: 0.0246 - val_mean_absolute_error: 0.0246\n",
      "Epoch 388/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0312 - mean_absolute_error: 0.0312\n",
      "Epoch 00388: val_loss did not improve from 0.02333\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0312 - mean_absolute_error: 0.0312 - val_loss: 0.0247 - val_mean_absolute_error: 0.0247\n",
      "Epoch 389/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0310 - mean_absolute_error: 0.0310\n",
      "Epoch 00389: val_loss did not improve from 0.02333\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0310 - mean_absolute_error: 0.0310 - val_loss: 0.0233 - val_mean_absolute_error: 0.0233\n",
      "Epoch 390/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0294 - mean_absolute_error: 0.0294\n",
      "Epoch 00390: val_loss did not improve from 0.02333\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0294 - mean_absolute_error: 0.0294 - val_loss: 0.0235 - val_mean_absolute_error: 0.0235\n",
      "Epoch 391/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0300 - mean_absolute_error: 0.0300\n",
      "Epoch 00391: val_loss did not improve from 0.02333\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0300 - mean_absolute_error: 0.0300 - val_loss: 0.0248 - val_mean_absolute_error: 0.0248\n",
      "Epoch 392/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0290 - mean_absolute_error: 0.0290\n",
      "Epoch 00392: val_loss did not improve from 0.02333\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0290 - mean_absolute_error: 0.0290 - val_loss: 0.0264 - val_mean_absolute_error: 0.0264\n",
      "Epoch 393/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0297 - mean_absolute_error: 0.0297\n",
      "Epoch 00393: val_loss did not improve from 0.02333\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0297 - mean_absolute_error: 0.0297 - val_loss: 0.0238 - val_mean_absolute_error: 0.0238\n",
      "Epoch 394/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0294 - mean_absolute_error: 0.0294\n",
      "Epoch 00394: val_loss did not improve from 0.02333\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0294 - mean_absolute_error: 0.0294 - val_loss: 0.0239 - val_mean_absolute_error: 0.0239\n",
      "Epoch 395/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0299 - mean_absolute_error: 0.0299\n",
      "Epoch 00395: val_loss did not improve from 0.02333\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0299 - mean_absolute_error: 0.0299 - val_loss: 0.0240 - val_mean_absolute_error: 0.0240\n",
      "Epoch 396/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0308 - mean_absolute_error: 0.0308\n",
      "Epoch 00396: val_loss did not improve from 0.02333\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0308 - mean_absolute_error: 0.0308 - val_loss: 0.0248 - val_mean_absolute_error: 0.0248\n",
      "Epoch 397/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - ETA: 0s - loss: 0.0307 - mean_absolute_error: 0.0307\n",
      "Epoch 00397: val_loss did not improve from 0.02333\n",
      "1/1 [==============================] - 1s 611ms/step - loss: 0.0307 - mean_absolute_error: 0.0307 - val_loss: 0.0255 - val_mean_absolute_error: 0.0255\n",
      "Epoch 398/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0315 - mean_absolute_error: 0.0315\n",
      "Epoch 00398: val_loss did not improve from 0.02333\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0315 - mean_absolute_error: 0.0315 - val_loss: 0.0238 - val_mean_absolute_error: 0.0238\n",
      "Epoch 399/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0310 - mean_absolute_error: 0.0310\n",
      "Epoch 00399: val_loss did not improve from 0.02333\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0310 - mean_absolute_error: 0.0310 - val_loss: 0.0237 - val_mean_absolute_error: 0.0237\n",
      "Epoch 400/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0291 - mean_absolute_error: 0.0291\n",
      "Epoch 00400: val_loss did not improve from 0.02333\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0291 - mean_absolute_error: 0.0291 - val_loss: 0.0235 - val_mean_absolute_error: 0.0235\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "history = model.fit(data[\"X_train\"], data[\"y_train\"],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(data[\"X_test\"], data[\"y_test\"]),\n",
    "    callbacks=[checkpointer, tensorboard],\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4383d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "model.save(os.path.join(\"results\", model_name) + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ca690ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model weights\n",
    "model_path = os.path.join(\"results\", model_name) + \".h5\"\n",
    "model.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7b3f364",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stock_prediction import np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba746c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data):\n",
    "    # retrieve the last sequence from data\n",
    "    last_sequence = data[\"last_sequence\"][-N_STEPS:]\n",
    "    # expand dimension\n",
    "    last_sequence = np.expand_dims(last_sequence, axis=0)\n",
    "    # get the prediction (scaled from 0 to 1)\n",
    "    prediction = model.predict(last_sequence)\n",
    "    # get the price (by inverting the scaling)\n",
    "    if SCALE:\n",
    "        predicted_price = data[\"column_scaler\"][\"adjclose\"].inverse_transform(prediction)[0][0]\n",
    "    else:\n",
    "        predicted_price = prediction[0][0]\n",
    "    return predicted_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aff80c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_gap(model, data, df2, indexVals):\n",
    "    predicted_prices = []\n",
    "    for X in range(LOOKUP_STEP):\n",
    "        # retrieve the last sequence from data\n",
    "        last_sequence = data[\"last_sequence\"][-N_STEPS-X:]\n",
    "        last_sequence = last_sequence[:N_STEPS]\n",
    "        # expand dimension\n",
    "        last_sequence = np.expand_dims(last_sequence, axis=0)\n",
    "        # get the prediction (scaled from 0 to 1)\n",
    "        prediction = model.predict(last_sequence)\n",
    "        # get the price (by inverting the scaling)\n",
    "        if SCALE:\n",
    "            predicted_price = data[\"column_scaler\"][\"adjclose\"].inverse_transform(prediction)[0][0]\n",
    "        else:\n",
    "            predicted_price = prediction[0][0]   \n",
    "\n",
    "        df2.loc[indexVals[-X],'forecast'] = predicted_price\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d61bd530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph2(test_df, df2):\n",
    "    \"\"\"\n",
    "    This function plots true close price along with predicted close price\n",
    "    with blue and red colors respectively\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(18,8))\n",
    "    plt.title(TICKER+\" Stock Price Forecast \"+ f\"{LOOKUP_STEP}\" +\" days out\", fontsize=16)\n",
    "    plt.plot(test_df[f'true_adjclose_{LOOKUP_STEP}'].tail(N_STEPS), c='b')\n",
    "    plt.plot(test_df[f'adjclose_{LOOKUP_STEP}'].tail(N_STEPS), c='r')\n",
    "    plt.plot(df2['forecast'].tail(LOOKUP_STEP-1), c='r')\n",
    "    plt.xlabel(\"Days\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.legend([\"Actual Price\", \"Predicted Price\"])\n",
    "    forecast_folder = \"forecasts\"\n",
    "    if not os.path.isdir(forecast_folder):\n",
    "        os.mkdir(forecast_folder)\n",
    "    filename = os.path.join(forecast_folder, TICKER.lower() + \"_\" + f\"{LOOKUP_STEP}\" + \"_forecast.png\")\n",
    "    plt.savefig(filename)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb6b40e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_df(model, data):\n",
    "    \"\"\"\n",
    "    This function takes the `model` and `data` dict to \n",
    "    construct a final dataframe that includes the features along \n",
    "    with true and predicted prices of the testing dataset\n",
    "    \"\"\"\n",
    "    # if predicted future price is higher than the current, \n",
    "    # then calculate the true future price minus the current price, to get the buy profit\n",
    "    buy_profit  = lambda current, true_future, pred_future: true_future - current if pred_future > current else 0\n",
    "    # if the predicted future price is lower than the current price,\n",
    "    # then subtract the true future price from the current price\n",
    "    sell_profit = lambda current, true_future, pred_future: current - true_future if pred_future < current else 0\n",
    "    X_test = data[\"X_test\"]\n",
    "    y_test = data[\"y_test\"]\n",
    "    # perform prediction and get prices\n",
    "    y_pred = model.predict(X_test)\n",
    "    if SCALE:\n",
    "        y_test = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(np.expand_dims(y_test, axis=0)))\n",
    "        y_pred = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(y_pred))\n",
    "    test_df = data[\"test_df\"]\n",
    "    # add predicted future prices to the dataframe\n",
    "    test_df[f\"adjclose_{LOOKUP_STEP}\"] = y_pred\n",
    "    # add true future prices to the dataframe\n",
    "    test_df[f\"true_adjclose_{LOOKUP_STEP}\"] = y_test\n",
    "    # sort the dataframe by date\n",
    "    test_df.sort_index(inplace=True)\n",
    "    final_df = test_df\n",
    "    # add the buy profit column\n",
    "    final_df[\"buy_profit\"] = list(map(buy_profit, \n",
    "                                    final_df[\"adjclose\"], \n",
    "                                    final_df[f\"adjclose_{LOOKUP_STEP}\"], \n",
    "                                    final_df[f\"true_adjclose_{LOOKUP_STEP}\"])\n",
    "                                    # since we don't have profit for last sequence, add 0's\n",
    "                                    )\n",
    "    # add the sell profit column\n",
    "    final_df[\"sell_profit\"] = list(map(sell_profit, \n",
    "                                    final_df[\"adjclose\"], \n",
    "                                    final_df[f\"adjclose_{LOOKUP_STEP}\"], \n",
    "                                    final_df[f\"true_adjclose_{LOOKUP_STEP}\"])\n",
    "                                    # since we don't have profit for last sequence, add 0's\n",
    "                                    )\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7002737",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = get_final_df(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "840bac61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shutt\\AppData\\Local\\Temp/ipykernel_24340/3675880277.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['forecast'] = 0\n"
     ]
    }
   ],
   "source": [
    "df2 = data['df'].tail(LOOKUP_STEP)\n",
    "df2['forecast'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea96afd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1817: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n",
      "C:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1817: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n",
      "C:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1817: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n",
      "C:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1817: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n",
      "C:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1817: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n",
      "C:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1817: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n",
      "C:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1817: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n",
      "C:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1817: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n",
      "C:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1817: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n",
      "C:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1817: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n",
      "C:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1817: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n",
      "C:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1817: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n",
      "C:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1817: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n",
      "C:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1817: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n",
      "C:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1817: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n",
      "C:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1817: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n",
      "C:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1817: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n",
      "C:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1817: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n",
      "C:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1817: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n",
      "C:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1817: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n"
     ]
    }
   ],
   "source": [
    "indexVals = []\n",
    "for index in df2.index:\n",
    "    indexVals.append(index)    \n",
    "future_prices = predict_gap(model, data, df2, indexVals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0108c638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot true/pred prices graph\n",
    "plot_graph2(final_df, df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73b375ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Future $ price after 20 days is 256.59\n"
     ]
    }
   ],
   "source": [
    "future_price = predict(model, data)\n",
    "print(f\"Future $ price after {LOOKUP_STEP} days is {future_price:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b535eff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
